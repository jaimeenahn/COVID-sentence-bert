{
  "005aea80a403da18f95fcb9944236a976d83580e": {
    "title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information",
    "abstract": "This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f/spl isin/C/sup N/ and a randomly chosen set of frequencies /spl Omega/. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set /spl Omega/? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)=/spl sigma//sub /spl tau//spl isin/T/f(/spl tau/)/spl delta/(t-/spl tau/) obeying |T|/spl les/C/sub M//spl middot/(log N)/sup -1/ /spl middot/ |/spl Omega/| for some constant C/sub M/>0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N/sup -M/), f can be reconstructed exactly as the solution to the /spl lscr//sub 1/ minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C/sub M/ which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T|/spl middot/logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N/sup -M/) would in general require a number of frequency samples at least proportional to |T|/spl middot/logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.",
    "paper_id": "005aea80a403da18f95fcb9944236a976d83580e"
  },
  "009fba8df6bbca155d9e070a9bd8d0959bc693c2": {
    "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
    "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
    "paper_id": "009fba8df6bbca155d9e070a9bd8d0959bc693c2"
  },
  "00bbba51721dee6e0b1cd2a5b614ab46f33abab6": {
    "title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy",
    "abstract": "1. Please explain how this manuscript advances this field of research and/or contributes something new to the literature. They tackle the problem of semantic similarity based on the information content the concepts share. They combine the taxonomic structure with the empirical problem estimates which provides better way of adapting knowledge to multiple context concepts. They also try to find a solution to the multiple inheritance problems.",
    "paper_id": "00bbba51721dee6e0b1cd2a5b614ab46f33abab6"
  },
  "00f7b192212078fc8afcbe504cc8caf57d8f73b5": {
    "title": "The Part-Time Parliament",
    "abstract": "Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament's protocol provides a new way of implementing the state machine approach to the design of distributed systems.",
    "paper_id": "00f7b192212078fc8afcbe504cc8caf57d8f73b5"
  },
  "0122e063ca5f0f9fb9d144d44d41421503252010": {
    "title": "Large Scale Distributed Deep Networks",
    "abstract": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestlysized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",
    "paper_id": "0122e063ca5f0f9fb9d144d44d41421503252010"
  },
  "0161e4348a7079e9c37434c5af47f6372d4b412d": {
    "title": "Class segmentation and object localization with superpixel neighborhoods",
    "abstract": "We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.",
    "paper_id": "0161e4348a7079e9c37434c5af47f6372d4b412d"
  },
  "01efec88d36070dc3bc49f341a77476f74d373bc": {
    "title": "Generation and Comprehension of Unambiguous Object Descriptions",
    "abstract": "We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google_Refexp_toolbox.",
    "paper_id": "01efec88d36070dc3bc49f341a77476f74d373bc"
  },
  "027205a9e2b6e0e1b7f46a028b9be8a0de5fdafe": {
    "title": "Image coding using wavelet transform",
    "abstract": "A scheme for image compression that takes into account psychovisual features both in the space and frequency domains is proposed. This method involves two steps. First, a wavelet transform used in order to obtain a set of biorthogonal subclasses of images: the original image is decomposed at different scales using a pyramidal algorithm architecture. The decomposition is along the vertical and horizontal directions and maintains constant the number of pixels required to describe the image. Second, according to Shannon's rate distortion theory, the wavelet coefficients are vector quantized using a multiresolution codebook. To encode the wavelet coefficients, a noise shaping bit allocation procedure which assumes that details at high resolution are less visible to the human eye is proposed. In order to allow the receiver to recognize a picture as quickly as possible at minimum cost, a progressive transmission scheme is presented. It is shown that the wavelet transform is particularly well adapted to progressive transmission.",
    "paper_id": "027205a9e2b6e0e1b7f46a028b9be8a0de5fdafe"
  },
  "02e85d62fbd8249a046d00ac10e39546511b2a51": {
    "title": "Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation",
    "abstract": "We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.",
    "paper_id": "02e85d62fbd8249a046d00ac10e39546511b2a51"
  },
  "03a00248b7d5e2d89f5337e62c39fad277c66102": {
    "title": "Introduction to Algorithms",
    "abstract": "problems To understand the class of polynomial-time solvable proble ms, we must first have a formal notion of what a \u201cproblem\u201d is. We define anbstract problemQ to be a binary relation on a set I of probleminstancesand a setS of problemsolutions. For example, an instance for SHORTEST-PATH is a triple consi sting of a graph and two vertices. A solution is a sequence of vertices in the g raph, with perhaps the empty sequence denoting that no path exists. The problem SHORTEST-PATH itself is the relation that associates each instance of a gra ph and two vertices with a shortest path in the graph that connects the two vertices. S ince shortest paths are not necessarily unique, a given problem instance may have mo r than one solution. This formulation of an abstract problem is more general than is required for our purposes. As we saw above, the theory of NP-completeness res tricts attention to decision problems : those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instan ce setI to the solution set {0, 1}. For example, a decision problem related to SHORTEST-PATH i s the problem PATH that we saw earlier. If i = \u3008G,u, v,k\u3009 is an instance of the decision problem PATH, then PATH(i ) = 1 (yes) if a shortest path fromu to v has at mostk edges, and PATH (i ) = 0 (no) otherwise. Many abstract problems are not decision problems, but rather optimization problems , in which some value must be minimized or maximized. As we saw above, however, it is usual ly a simple matter to recast an optimization problem as a decision problem that is no harder. 1See Hopcroft and Ullman [156] or Lewis and Papadimitriou [20 4] for a thorough treatment of the Turing-machine model. 34.1 Polynomial time 973",
    "paper_id": "03a00248b7d5e2d89f5337e62c39fad277c66102"
  },
  "03adaf0497fdd9fb32ef0ef925db9bc7da4f2b4f": {
    "title": "Learning a Classification Model for Segmentation",
    "abstract": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is oversegmented into superpixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.",
    "paper_id": "03adaf0497fdd9fb32ef0ef925db9bc7da4f2b4f"
  },
  "0419bd22665a85aa57e2d6442c255f516181ec22": {
    "title": "Empirical analysis of an evolving social network.",
    "abstract": "Social networks evolve over time, driven by the shared activities and affiliations of their members, by similarity of individuals' attributes, and by the closure of short network cycles. We analyzed a dynamic social network comprising 43,553 students, faculty, and staff at a large university, in which interactions between individuals are inferred from time-stamped e-mail headers recorded over one academic year and are matched with affiliations and attributes. We found that network evolution is dominated by a combination of effects arising from network topology itself and the organizational structure in which the network is embedded. In the absence of global perturbations, average network properties appear to approach an equilibrium state, whereas individual properties are unstable.",
    "paper_id": "0419bd22665a85aa57e2d6442c255f516181ec22"
  },
  "04483e2c56695b19f6912b061769eb8c175a5a7a": {
    "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    "abstract": "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
    "paper_id": "04483e2c56695b19f6912b061769eb8c175a5a7a"
  },
  "0480c9b7bfb4a6ec7dc36e8113c5f37d721cbb1a": {
    "title": "How to search a social network",
    "abstract": "We address the question of how participants in a small world experiment are able to find short paths in a social network using only local information about their immediate contacts. We simulate such experiments on a network of actual email contacts within an organization as well as on a student social networking website. On the email network we find that small world search strategies using a contact\u2019s position in physical space or in an organizational hierarchy relative to the target can effectively be used to locate most individuals. However, we find that in the online student network, where the data is incomplete and hierarchical structures are not well defined, local search strategies are less effective. We compare our findings to recent theoretical hypotheses about underlying social structure that would enable these simple search strategies to succeed and discuss the implications to social",
    "paper_id": "0480c9b7bfb4a6ec7dc36e8113c5f37d721cbb1a"
  },
  "049504df22c77010e5bb62a2088f70fabc5ecb6d": {
    "title": "Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?",
    "abstract": "Suppose we are given a vector f in a class FsubeRopf<sup>N </sup>, e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision epsi in the Euclidean (lscr<sub>2</sub>) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the nth largest entry of the vector |f| (or of its coefficients in a fixed basis) obeys |f|<sub>(n)</sub>lesRmiddotn<sup>-1</sup>p/, where R>0 and p>0. Suppose that we take measurements y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang,k=1,...,K, where the X<sub>k</sub> are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0<p<1 and with overwhelming probability, our reconstruction f<sup>t</sup>, defined as the solution to the constraints y<sub>k</sub>=langf<sup># </sup>,X<sub>k</sub>rang with minimal lscr<sub>1</sub> norm, obeys parf-f<sup>#</sup>par<sub>lscr2</sub>lesC<sub>p </sub>middotRmiddot(K/logN)<sup>-r</sup>, r=1/p-1/2. There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed",
    "paper_id": "049504df22c77010e5bb62a2088f70fabc5ecb6d"
  },
  "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac": {
    "title": "Boosting the margin: A new explanation for the effectiveness of voting methods",
    "abstract": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik\u2019s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.",
    "paper_id": "04a20cd0199d0a24fea8e6bf0e0cc61b26c1f3ac"
  },
  "04b3200915d72344059015c6c32c11a6eeb6a622": {
    "title": "Shape Quantization and Recognition with Randomized Trees",
    "abstract": "We explore a new approach to shape recognition based on a virtually infinite family of binary features (queries) of the image data, designed to accommodate prior information about shape invariance and regularity. Each query corresponds to a spatial arrangement of several local topographic codes (or tags), which are in themselves too primitive and common to be informative about shape. All the discriminating power derives from relative angles and distances among the tags. The important attributes of the queries are a natural partial ordering corresponding to increasing structure and complexity; semi-invariance, meaning that most shapes of a given class will answer the same way to two queries that are successive in the ordering; and stability, since the queries are not based on distinguished points and substructures. No classifier based on the full feature set can be evaluated, and it is impossible to determine a priori which arrangements are informative. Our approach is to select informative features and build tree classifiers at the same time by inductive learning. In effect, each tree provides an approximation to the full posterior where the features chosen depend on the branch that is traversed. Due to the number and nature of the queries, standard decision tree construction based on a fixed-length feature vector is not feasible. Instead we entertain only a small random sample of queries at each node, constrain their complexity to increase with tree depth, and grow multiple trees. The terminal nodes are labeled by estimates of the corresponding posterior distribution over shape classes. An image is classified by sending it down every tree and aggregating the resulting distributions. The method is applied to classifying handwritten digits and synthetic linear and nonlinear deformations of three hundred symbols. State-of-the-art error rates are achieved on the National Institute of Standards and Technology database of digits. The principal goal of the experiments on symbols is to analyze invariance, generalization error and related issues, and a comparison with artificial neural networks methods is presented in this context. Figure 1: LATEX Symbol",
    "paper_id": "04b3200915d72344059015c6c32c11a6eeb6a622"
  },
  "052373d7fd12145c41377f9a05513596d32e409c": {
    "title": "Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data",
    "abstract": "While recent deep neural network models have achieved promising results on the image captioning task, they rely largely on the availability of corpora with paired image and sentence captions to describe objects in context. In this work, we propose the Deep Compositional Captioner (DCC) to address the task of generating descriptions of novel objects which are not present in paired imagesentence datasets. Our method achieves this by leveraging large object recognition datasets and external text corpora and by transferring knowledge between semantically similar concepts. Current deep caption models can only describe objects contained in paired image-sentence corpora, despite the fact that they are pre-trained with large object recognition datasets, namely ImageNet. In contrast, our model can compose sentences that describe novel objects and their interactions with other objects. We demonstrate our model's ability to describe novel concepts by empirically evaluating its performance on MSCOCO and show qualitative results on ImageNet images of objects for which no paired image-sentence data exist. Further, we extend our approach to generate descriptions of objects in video clips. Our results show that DCC has distinct advantages over existing image and video captioning approaches for generating descriptions of new objects in context.",
    "paper_id": "052373d7fd12145c41377f9a05513596d32e409c"
  },
  "052b1d8ce63b07fec3de9dbb583772d860b7c769": {
    "title": "Learning representations by back-propagating errors",
    "abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.",
    "paper_id": "052b1d8ce63b07fec3de9dbb583772d860b7c769"
  },
  "053912e76e50c9f923a1fc1c173f1365776060cc": {
    "title": "On optimization methods for deep learning",
    "abstract": "The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.",
    "paper_id": "053912e76e50c9f923a1fc1c173f1365776060cc"
  },
  "057158cd15c2716411bcf7c5a28ef403a1e23009": {
    "title": "Learning structured prediction models: a large margin approach",
    "abstract": "We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.",
    "paper_id": "057158cd15c2716411bcf7c5a28ef403a1e23009"
  },
  "05aba481e8a221df5d8775a3bb749001e7f2525e": {
    "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
    "abstract": "We present a new family of subgradient methods that dynamica lly incorporate knowledge of the geometry of the data observed in earlier iterations to perfo rm more informative gradient-based learning. Metaphorically, the adaptation allows us to find n eedles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems fro m recent advances in stochastic optimization and online learning which employ proximal funct ions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adap tively modifying the proximal function, which significantly simplifies setting a learning rate nd results in regret guarantees that are provably as good as the best proximal function that can be cho sen in hindsight. We give several efficient algorithms for empirical risk minimization probl ems with common and important regularization functions and domain constraints. We experimen tally study our theoretical analysis and show that adaptive subgradient methods outperform state-o f-the-art, yet non-adaptive, subgradient algorithms.",
    "paper_id": "05aba481e8a221df5d8775a3bb749001e7f2525e"
  },
  "061356704ec86334dbbc073985375fe13cd39088": {
    "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
    "abstract": "In this work we investigate the effect of the convolutional n etwork depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth, which shows that a significant improvement on the prior-art configurations can be achi eved by pushing the depth to 16\u201319 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first a nd he second places in the localisation and classification tracks respec tively. We also show that our representations generalise well to other datasets, whe re t y achieve the stateof-the-art results. Importantly, we have made our two bestp rforming ConvNet models publicly available to facilitate further research o n the use of deep visual representations in computer vision.",
    "paper_id": "061356704ec86334dbbc073985375fe13cd39088"
  },
  "0626908dd710b91aece1a81f4ca0635f23fc47f3": {
    "title": "Rethinking the Inception Architecture for Computer Vision",
    "abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.",
    "paper_id": "0626908dd710b91aece1a81f4ca0635f23fc47f3"
  },
  "064905a3eeddf6cf17538ac7bca40483a54bb9cc": {
    "title": "Boosting in the Limit: Maximizing the Margin of Learned Ensembles",
    "abstract": "The \u201cminimum margin\u201d of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it give s to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generaliza tion error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new \u201cLPboosting\u201d algorithms that achieve better minimum margins than Adaboost. However, these algorithms do n talways yield better generalization performance. In fact, more often the opposite is tru e. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open. Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit\u2014eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.",
    "paper_id": "064905a3eeddf6cf17538ac7bca40483a54bb9cc"
  },
  "0678c73ff855c93f238538735fcecc1ee6bc8379": {
    "title": "Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization",
    "abstract": "This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.",
    "paper_id": "0678c73ff855c93f238538735fcecc1ee6bc8379"
  },
  "0690ba31424310a90028533218d0afd25a829c8d": {
    "title": "Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs",
    "abstract": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called \u201dsemantic image segmentation\u201d). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our \u201cDeepLab\u201d system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the \u2019hole\u2019 algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.",
    "paper_id": "0690ba31424310a90028533218d0afd25a829c8d"
  },
  "069c40a8ca5305c9a0734c1f6134eb19a678f4ab": {
    "title": "LabelMe: A Database and Web-Based Tool for Image Annotation",
    "abstract": "We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.",
    "paper_id": "069c40a8ca5305c9a0734c1f6134eb19a678f4ab"
  },
  "06a23ffbd9752ce204197df59812b2ebd1a097ff": {
    "title": "Feedforward semantic segmentation with zoom-out features",
    "abstract": "We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by \u201czooming out\u201d from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves 69.6% average accuracy on the PASCAL VOC 2012 test set.",
    "paper_id": "06a23ffbd9752ce204197df59812b2ebd1a097ff"
  },
  "06bae254319f8d39e80c7254c841787b45baf820": {
    "title": "Supervised sequence labelling with recurrent neural networks",
    "abstract": "Recurrent neural networks are powerful sequence learners. They are able to incorporate context information in a flexible way, and are robust to localised distortions of the input data. These properties make them well suited to sequence labelling, where input sequences are transcribed with streams of labels. Long short-term memory is an especially promising recurrent architecture, able to bridge long time delays between relevant input and output events, and thereby access long range context. The aim of this thesis is to advance the state-of-the-art in supervised sequence labelling with recurrent networks in general, and long short-term memory in particular. Its two main contributions are (1) a new type of output layer that allows recurrent networks to be trained directly for sequence labelling tasks where the alignment between the inputs and the labels is unknown, and (2) an extension of long short-term memory to multidimensional data, such as images and video sequences. Experimental results are presented on speech recognition, online and offline handwriting recognition, keyword spotting, image segmentation and image classification, demonstrating the advantages of advanced recurrent networks over other sequential algorithms, such as hidden Markov Models.",
    "paper_id": "06bae254319f8d39e80c7254c841787b45baf820"
  },
  "071b16f25117fb6133480c6259227d54fc2a5ea0": {
    "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
    "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\u2013decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\u2013decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
    "paper_id": "071b16f25117fb6133480c6259227d54fc2a5ea0"
  },
  "072fd0b8d471f183da0ca9880379b3bb29031b6a": {
    "title": "Image-to-Image Translation with Conditional Adversarial Networks",
    "abstract": "We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",
    "paper_id": "072fd0b8d471f183da0ca9880379b3bb29031b6a"
  },
  "079495c84b68623d9b1d4d48809cc24c1eee0a7e": {
    "title": "Natural Language Object Retrieval",
    "abstract": "In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",
    "paper_id": "079495c84b68623d9b1d4d48809cc24c1eee0a7e"
  },
  "07c6db589e18192a2c6c23ab9fa7c51e432cc09a": {
    "title": "Recovering Surface Layout from an Image",
    "abstract": "Humans have an amazing ability to instantly grasp the overall 3D structure of a scene\u2014ground orientation, relative positions of major landmarks, etc.\u2014even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this \u201csurface layout\u201d of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis. In this paper, we take the first step towards constructing the surface layout, a labeling of the image intogeometric classes. Our main insight is to learn appearance-based models of these geometric classes, which coarsely describe the 3D scene orientation of each image region. Our multiple segmentation framework provides robust spatial support, allowing a wide variety of cues (e.g., color, texture, and perspective) to contribute to the confidence in each geometric label. In experiments on a large set of outdoor images, we evaluate the impact of the individual cues and design choices in our algorithm. We further demonstrate the applicability of our method to indoor images, describe potential applications, and discuss extensions to a more complete notion of surface layout.",
    "paper_id": "07c6db589e18192a2c6c23ab9fa7c51e432cc09a"
  },
  "080aebd2cc1019f17e78496354c37195560b0697": {
    "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems",
    "abstract": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",
    "paper_id": "080aebd2cc1019f17e78496354c37195560b0697"
  },
  "0825788b9b5a18e3dfea5b0af123b5e939a4f564": {
    "title": "Glove: Global Vectors for Word Representation",
    "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
    "paper_id": "0825788b9b5a18e3dfea5b0af123b5e939a4f564"
  },
  "0841ac8b72c7a493dd7cdf42e6d54fc5b90ae343": {
    "title": "Tendermint : Consensus without Mining",
    "abstract": "Cryptocurrencies such as Bitcoin enable users to submit payment transactions without going through a centralized trusted organization. Bitcoin relies on proof-of-work mining to secure consensus which is problematic; mining requires a massive expenditure of energy, confirmation of transactions is slow, and security is difficult to quantify. We propose a solution to the blockchain consensus problem that does not require mining by adapting an existing solution to the Byzantine Generals Problem.",
    "paper_id": "0841ac8b72c7a493dd7cdf42e6d54fc5b90ae343"
  },
  "08b67692bc037eada8d3d7ce76cc70994e7c8116": {
    "title": "Information Theory and Statistical Mechanics",
    "abstract": "Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum.entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncom-mittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting \"subjective statistical mechanics,\" the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether",
    "paper_id": "08b67692bc037eada8d3d7ce76cc70994e7c8116"
  },
  "08fcc764a9967fd6dace5637691d49bd92887fff": {
    "title": "A Survey and Taxonomy of Location Systems for Ubiquitous Computing",
    "abstract": "Emerging mobile computing applications often need to know where things are physically located. To meet this need, many di erent location systems and technologies have been developed. In this paper we present a the basic techniques used for location-sensing, describe a taxonomy of location system properties, present a survey of research and commercial location systems that de ne the eld, show how the taxonomy can be used to evaluate location-sensing systems, and o er suggestions for future research. It is our hope that this paper is a useful reference for researchers and location-aware application builders alike for understanding and evaluating the many options in this domain.",
    "paper_id": "08fcc764a9967fd6dace5637691d49bd92887fff"
  },
  "09031aa6d6743bebebc695955cd77c032cd9192f": {
    "title": "Group formation in large social networks: membership, growth, and evolution",
    "abstract": "The processes by which communities come together, attract new members, and develop over time is a central research issue in the social sciences - political movements, professional organizations, and religious denominations all provide fundamental examples of such communities. In the digital domain, on-line groups are becoming increasingly prominent due to the growth of community and social networking sites such as MySpace and LiveJournal. However, the challenge of collecting and analyzing large-scale time-resolved data on social groups and communities has left most basic questions about the evolution of such groups largely unresolved: what are the structural features that influence whether individuals will join communities, which communities will grow rapidly, and how do the overlaps among pairs of communities change over time.Here we address these questions using two large sources of data: friendship links and community membership on LiveJournal, and co-authorship and conference publications in DBLP. Both of these datasets provide explicit user-defined communities, where conferences serve as proxies for communities in DBLP. We study how the evolution of these communities relates to properties such as the structure of the underlying social networks. We find that the propensity of individuals to join communities, and of communities to grow rapidly, depends in subtle ways on the underlying network structure. For example, the tendency of an individual to join a community is influenced not just by the number of friends he or she has within the community, but also crucially by how those friends are connected to one another. We use decision-tree techniques to identify the most significant structural determinants of these properties. We also develop a novel methodology for measuring movement of individuals between communities, and show how such movements are closely aligned with changes in the topics of interest within the communities.",
    "paper_id": "09031aa6d6743bebebc695955cd77c032cd9192f"
  },
  "09b2ddbba0bf89d0f5172e8dff5090714d279d3c": {
    "title": "Recycling ambient microwave energy with broad-band rectenna arrays",
    "abstract": "This paper presents a study of reception and rectification of broad-band statistically time-varying low-power-density microwave radiation. The applications are in wireless powering of industrial sensors and recycling of ambient RF energy. A 64-element dual-circularly-polarized spiral rectenna array is designed and characterized over a frequency range of 2-18 GHz with single-tone and multitone incident waves. The integrated design of the antenna and rectifier, using a combination of full-wave electromagnetic field analysis and harmonic balance nonlinear circuit analysis, eliminates matching and filtering circuits, allowing for a compact element design. The rectified dc power and efficiency is characterized as a function of dc load and dc circuit topology, RF frequency, polarization, and incidence angle for power densities between 10/sup -5/-10/sup -1/ mW/cm/sup 2/. In addition, the increase in rectenna efficiency for multitone input waves is presented.",
    "paper_id": "09b2ddbba0bf89d0f5172e8dff5090714d279d3c"
  },
  "0a10d64beb0931efdc24a28edaa91d539194b2e2": {
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
    "paper_id": "0a10d64beb0931efdc24a28edaa91d539194b2e2"
  },
  "0a48a04d88ba6772a08f29e3c9b9879a18d2dd67": {
    "title": "Overview of Knowledge Sharing and Reuse Components: Ontologies and Problem-Solving Methods",
    "abstract": "Ontologies and problem-solving methods are promising candidates for reuse in Knowledge Engineering. Ontologies define domain knowledge at a generic level, while problem-solving methods specify generic reasoning knowledge. Both type of components can be viewed as complementary entities that can be used to configure new knowledge systems from existing, reusable components. In this paper, we give an overview of approaches for ontologies and problem-solving methods.",
    "paper_id": "0a48a04d88ba6772a08f29e3c9b9879a18d2dd67"
  },
  "0ab7f239a4784492779437ac54ae0917b5dd6730": {
    "title": "The BSD Packet Filter: A New Architecture for User-level Packet Capture",
    "abstract": "Many versions of Unix provide facilities for user-level packet capture, making possible the use of general purpose workstations for network monitoring. Because network monitors run as user-level processes, packets must be copied across the kernel/user-space protection boundary. This copying can be minimized by deploying a kernel agent called a packet filter , which discards unwanted packets as early as possible. The original Unix packet filter was designed around a stack-based filter evaluator that performs sub-optimally on current RISC CPUs. The BSD Packet Filter (BPF) uses a new, registerbased filter evaluator that is up to 20 times faster than the original design. BPF also uses a straightforward buffering strategy that makes its overall performance up to 100 times faster than Sun\u2019s NIT running on the same hardware.",
    "paper_id": "0ab7f239a4784492779437ac54ae0917b5dd6730"
  },
  "0b3cfbf79d50dae4a16584533227bb728e3522aa": {
    "title": "Long Short-Term Memory",
    "abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",
    "paper_id": "0b3cfbf79d50dae4a16584533227bb728e3522aa"
  },
  "0b44fcbeea9415d400c5f5789d6b892b6f98daff": {
    "title": "Building a Large Annotated Corpus of English: The Penn Treebank",
    "abstract": "In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-93-87. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/237 Building A Large Annotated Corpus of English: The Penn Treebank MS-CIS-93-87 LINC LAB 260 Mitchell P. Marcus Beatrice Santorini Mary Ann Marcinkiewicz University of Pennsylvania School of Engineering and Applied Science Computer and Information Science Department Philadelphia, PA 19104-6389",
    "paper_id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff"
  },
  "0b6e98a6a8cf8283fd76fe1100b23f11f4cfa711": {
    "title": "Matching pursuits with time-frequency dictionaries",
    "abstract": "We introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. We derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. We compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser.",
    "paper_id": "0b6e98a6a8cf8283fd76fe1100b23f11f4cfa711"
  },
  "0c04909ed933469246defcf9aca2b71ae8e3f623": {
    "title": "Information Retrieval",
    "abstract": "The major change in the second edition of this book is the addition of a new chapter on probabilistic retrieval. This chapter has been included because I think this is one of the most interesting and active areas of research in information retrieval. There are still many problems to be solved so I hope that this particular chapter will be of some help to those who want to advance the state of knowledge in this area. All the other chapters have been updated by including some of the more recent work on the topics covered. In preparing this new edition I have benefited from discussions with Bruce Croft, The material of this book is aimed at advanced undergraduate information (or computer) science students, postgraduate library science students, and research workers in the field of IR. Some of the chapters, particularly Chapter 6 * , make simple use of a little advanced mathematics. However, the necessary mathematical tools can be easily mastered from numerous mathematical texts that now exist and, in any case, references have been given where the mathematics occur. I had to face the problem of balancing clarity of exposition with density of references. I was tempted to give large numbers of references but was afraid they would have destroyed the continuity of the text. I have tried to steer a middle course and not compete with the Annual Review of Information Science and Technology. Normally one is encouraged to cite only works that have been published in some readily accessible form, such as a book or periodical. Unfortunately, much of the interesting work in IR is contained in technical reports and Ph.D. theses. For example, most the work done on the SMART system at Cornell is available only in reports. Luckily many of these are now available through the National Technical Information Service (U.S.) and University Microfilms (U.K.). I have not avoided using these sources although if the same material is accessible more readily in some other form I have given it preference. I should like to acknowledge my considerable debt to many people and institutions that have helped me. Let me say first that they are responsible for many of the ideas in this book but that only I wish to be held responsible. My greatest debt is to Karen Sparck Jones who taught me to research information retrieval as an experimental science. Nick Jardine and Robin \u2026",
    "paper_id": "0c04909ed933469246defcf9aca2b71ae8e3f623"
  },
  "0c35a65a99af8202fe966c5e7bee00dea7cfcbf8": {
    "title": "Experiences with an Interactive Museum Tour-Guide Robot",
    "abstract": "This article describes the software architecture of an auto nomous, interactive tour-guide robot. It presents a modular and distributed software archi te ture, which integrates localization, mapping, collision avoidance, planning, and vari ous modules concerned with user interaction and Web-based telepresence. At its heart, the s oftware approach relies on probabilistic computation, on-line learning, and any-time alg orithms. It enables robots to operate safely, reliably, and at high speeds in highly dynamic environments, and does not require any modifications of the environment to aid the robot \u2019s peration. Special emphasis is placed on the design of interactive capabilities that appeal to people\u2019s intuition. The interface provides new means for human-robot interaction w ith crowds of people in public places, and it also provides people all around the world with the ability to establish a \u201cvirtual telepresence\u201d using the Web. To illustrate our approac h, results are reported obtained in mid-1997, when our robot \u201cRHINO\u201d was deployed for a period of six days in a densely populated museum. The empirical results demonstrate relia bl operation in public environments. The robot successfully raised the museum\u2019s atten dance by more than 50%. In addition, thousands of people all over the world controlled the robot through the Web. We conjecture that these innovations transcend to a much large r range of application domains for service robots.",
    "paper_id": "0c35a65a99af8202fe966c5e7bee00dea7cfcbf8"
  },
  "0c739b915d633cc3c162e4ef1e57b796c2dc2217": {
    "title": "VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations",
    "abstract": "Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.",
    "paper_id": "0c739b915d633cc3c162e4ef1e57b796c2dc2217"
  },
  "0c7f52c753a65ceaf3755e20b906ffd0c05c994a": {
    "title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data",
    "abstract": "We presentconditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.",
    "paper_id": "0c7f52c753a65ceaf3755e20b906ffd0c05c994a"
  },
  "0cbc08adb7a85770d53ac3ce9b2dce2f8f4871ef": {
    "title": "The Inside-Outside Recursive Neural Network model for Dependency Parsing",
    "abstract": "We propose the first implementation of an infinite-order generative dependency model. The model is based on a new recursive neural network architecture, the Inside-Outside Recursive Neural Network. This architecture allows information to flow not only bottom-up, as in traditional recursive neural networks, but also topdown. This is achieved by computing content as well as context representations for any constituent, and letting these representations interact. Experimental results on the English section of the Universal Dependency Treebank show that the infinite-order model achieves a perplexity seven times lower than the traditional third-order model using counting, and tends to choose more accurate parses in k-best lists. In addition, reranking with this model achieves state-of-the-art unlabelled attachment scores and unlabelled exact match scores.",
    "paper_id": "0cbc08adb7a85770d53ac3ce9b2dce2f8f4871ef"
  },
  "0d1bfcdfc90e662defd26b8b0deae6ef6e661b23": {
    "title": "TextonBoost for Image Understanding: Multi-Class Object Recognition and Segmentation by Jointly Modeling Texture, Layout, and Context",
    "abstract": "This paper details a new approach for learning a discriminative model of object classes, incorporating texture, layout, and context information efficiently. The learned model is used for automatic visual understanding and semantic segmentation of photographs. Our discriminative model exploits texture-layout filters, novel features based on textons, which jointly model patterns of texture and their spatial layout. Unary classification and feature selection is achieved using shared boosting to give an efficient classifier which can be applied to a large number of classes. Accurate image segmentation is achieved by incorporating the unary classifier in a conditional random field, which (i) captures the spatial interactions between class labels of neighboring pixels, and (ii) improves the segmentation of specific object instances. Efficient training of the model on large datasets is achieved by exploiting both random feature selection and piecewise training methods. High classification and segmentation accuracy is demonstrated on four varied databases: (i) the MSRC 21-class database containing photographs of real objects viewed under general lighting conditions, poses and viewpoints, (ii) the 7-class Corel subset and (iii) the 7-class Sowerby database used in He et\u00a0al. (Proceeding of IEEE Conference on Computer Vision and Pattern Recognition, vol.\u00a02, pp.\u00a0695\u2013702, June 2004), and (iv) a set of video sequences of television shows. The proposed algorithm gives competitive and visually pleasing results for objects that are highly textured (grass, trees, etc.), highly structured (cars, faces, bicycles, airplanes, etc.), and even articulated (body, cow, etc.).",
    "paper_id": "0d1bfcdfc90e662defd26b8b0deae6ef6e661b23"
  },
  "0d386c11c8ca4aa76f8803c18734b0863078e33f": {
    "title": "The Packet Filter: An Efficient Mechanism for User-level Network Code",
    "abstract": "Code to implement network protocols can be either inside the kernel of an operating system or in user-level processes. Kernel-resident code is hard to develop, debug, and maintain, but user-level implementations typically incur significant overhead and perform poorly.\nThe performance of user-level network code depends on the mechanism used to demultiplex received packets. Demultiplexing in a user-level process increases the rate of context switches and system calls, resulting in poor performance. Demultiplexing in the kernel eliminates unnecessary overhead.\nThis paper describes the packet filter, a kernel-resident, protocol-independent packet demultiplexer. Individual user processes have great flexibility in selecting which packets they will receive. Protocol implementations using the packet filter perform quite well, and have been in production use for several years.",
    "paper_id": "0d386c11c8ca4aa76f8803c18734b0863078e33f"
  },
  "0d5676d90f20215d08dfe7e71fb55303f23604f7": {
    "title": "The Cricket location-support system",
    "abstract": "This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.",
    "paper_id": "0d5676d90f20215d08dfe7e71fb55303f23604f7"
  },
  "0da75bded3ae15e255f5bd376960cfeffa173b4e": {
    "title": "The Role of Context for Object Detection and Semantic Segmentation in the Wild",
    "abstract": "In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.",
    "paper_id": "0da75bded3ae15e255f5bd376960cfeffa173b4e"
  },
  "0e0900b88c33b671be5dd2ded9885b6526d6b429": {
    "title": "From captions to visual concepts and back",
    "abstract": "This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",
    "paper_id": "0e0900b88c33b671be5dd2ded9885b6526d6b429"
  },
  "0ea38a5ba0c8739d1196da5d20efb13406bb6550": {
    "title": "Relative attributes",
    "abstract": "Human-nameable visual \u201cattributes\u201d can benefit various recognition tasks. However, existing techniques restrict these properties to categorical labels (for example, a person is \u2018smiling\u2019 or not, a scene is \u2018dry\u2019 or not), and thus fail to capture more general semantic relationships. We propose to model relative attributes. Given training data stating how object/scene categories relate according to different attributes, we learn a ranking function per attribute. The learned ranking functions predict the relative strength of each property in novel images. We then build a generative model over the joint space of attribute ranking outputs, and propose a novel form of zero-shot learning in which the supervisor relates the unseen object category to previously seen objects via attributes (for example, \u2018bears are furrier than giraffes\u2019). We further show how the proposed relative attributes enable richer textual descriptions for new images, which in practice are more precise for human interpretation. We demonstrate the approach on datasets of faces and natural scenes, and show its clear advantages over traditional binary attribute prediction for these new tasks.",
    "paper_id": "0ea38a5ba0c8739d1196da5d20efb13406bb6550"
  },
  "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a": {
    "title": "The Pascal Visual Object Classes (VOC) Challenge",
    "abstract": "The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension.",
    "paper_id": "0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a"
  },
  "0f390c9474d09726ccfd6df7469ee54fe23da280": {
    "title": "Collective Generation of Natural Image Descriptions",
    "abstract": "We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.",
    "paper_id": "0f390c9474d09726ccfd6df7469ee54fe23da280"
  },
  "0f6a8e9139cdd15fd0a8a292204aed21a8b51c5a": {
    "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
    "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.",
    "paper_id": "0f6a8e9139cdd15fd0a8a292204aed21a8b51c5a"
  },
  "0f8724d14607e17473d428bd232e9dd12361bf31": {
    "title": "Logistic Regression in Rare Events Data",
    "abstract": "We study rare events data, binary dependent variables with dozens to thousands of times fewer ones (events, such as wars, vetoes, cases of political activism, or epidemiological infections) than zeros (\u201cnonevents\u201d). In many literatures, these variables have proven difficult to explain and predict, a problem that seems to have at least two sources. First, popular statistical procedures, such as logistic regression, can sharply underestimate the probability of rare events. We recommend corrections that outperform existing methods and change the estimates of absolute and relative risks by as much as some estimated effects reported in the literature. Second, commonly used data collection strategies are grossly inefficient for rare events data. The fear of collecting data with too few events has led to data collections with huge numbers of observations but relatively few, and poorly measured, explanatory variables, such as in international conflict data with more than a quarter-million dyads, only a few of which are at war. As it turns out, more efficient sampling designs exist for making valid inferences, such as sampling all available events (e.g., wars) and a tiny fraction of nonevents (peace). This enables scholars to save as much as 99% of their (nonfixed) data collection costs or to collect much more meaningful explanatory variables. We provide methods that link these two results, enabling both types of corrections to work simultaneously, and software that implements the methods developed.",
    "paper_id": "0f8724d14607e17473d428bd232e9dd12361bf31"
  },
  "10105a6e45dfb4d6b34a538e663149a05360eff8": {
    "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks",
    "abstract": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper, we argue that the improved accuracy achieved by the DNNs is the result of their ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. We show that these representations become increasingly insensitive to small perturbations in the input with increasing network depth, which leads to better speech recognition performance with deeper networks. We also show that DNNs cannot extrapolate to test samples that are substantially different from the training examples. If the training data are sufficiently representative, however, internal features learned by the DNN are relatively stable with respect to speaker differences, bandwidth differences, and environment distortion. This enables DNN-based recognizers to perform as well or better than state-of-the-art systems based on GMMs or shallow networks without the need for explicit model adaptation or feature normalization.",
    "paper_id": "10105a6e45dfb4d6b34a538e663149a05360eff8"
  },
  "10767cd60ac9e33188ae7e35d1e84a6614387da2": {
    "title": "Maximum likelihood linear transformations for HMM-based speech recognition",
    "abstract": "This paper examines the application of linear transformations for speaker and environmental adaptation in an HMM-based speech recognition system. In particular, transformations that are trained in a maximum likelihood sense on adaptation data are investigated. Other than in the form of a simple bias, strict linear feature-space transformations are inappropriate in this case. Hence, only model-based linear transforms are considered. The paper compares the two possible forms of model-based transforms: (i) unconstrained, where any combination of mean and variance transform may be used, and (ii) constrained, which requires the variance transform to have the same form as the mean transform (sometimes referred to as feature-space transforms). Re-estimation formulae for all appropriate cases of transform are given. This includes a new and e cient \\full\" variance transform and the extension of the constrained model-space transform from the simple diagonal case to the full or block-diagonal case. The constrained and unconstrained transforms are evaluated in terms of computational cost, recognition time e ciency, and use for speaker adaptive training. The recognition performance of the two model-space transforms on a large vocabulary speech recognition task using incremental adaptation is investigated. In addition, initial experiments using the constrained model-space transform for speaker adaptive training are detailed.",
    "paper_id": "10767cd60ac9e33188ae7e35d1e84a6614387da2"
  },
  "10a4db59e81d26b2e0e896d3186ef81b4458b93f": {
    "title": "Named Entity Recognition with Bidirectional LSTM-CNNs",
    "abstract": "Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.",
    "paper_id": "10a4db59e81d26b2e0e896d3186ef81b4458b93f"
  },
  "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5": {
    "title": "Histograms of oriented gradients for human detection",
    "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.",
    "paper_id": "10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5"
  },
  "10e70e16e5a68d52fa2c9d0a452db9ed2f9403aa": {
    "title": "A generalized uncertainty principle and sparse representation in pairs of bases",
    "abstract": "An elementary proof of a basic uncertainty principle concerning pairs of representations of RN vectors in different orthonormal bases is provided. The result, slightly stronger than stated before, has a direct impact on the uniqueness property of the sparse representation of such vectors using pairs of orthonormal bases as overcomplete dictionaries. The main contribution in this paper is the improvement of an important result due to Donoho and Huo concerning the replacement of the l0 optimization problem by a linear programming minimization when searching for the unique sparse representation.",
    "paper_id": "10e70e16e5a68d52fa2c9d0a452db9ed2f9403aa"
  },
  "10eb7bfa7687f498268bdf74b2f60020a151bdc6": {
    "title": "Visualizing Data using t-SNE",
    "abstract": "We present a new technique called \u201ct-SNE\u201d that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large datasets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of datasets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the datasets.",
    "paper_id": "10eb7bfa7687f498268bdf74b2f60020a151bdc6"
  },
  "1109b663453e78a59e4f66446d71720ac58cec25": {
    "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks",
    "abstract": "We present an integrated framework for using Convolutional Networks for classification , localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.",
    "paper_id": "1109b663453e78a59e4f66446d71720ac58cec25"
  },
  "117a50fbdfd473e43e550c6103733e6cb4aecb4c": {
    "title": "Maximum margin planning",
    "abstract": "Imitation learning of sequential, goal-directed behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A* and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task.",
    "paper_id": "117a50fbdfd473e43e550c6103733e6cb4aecb4c"
  },
  "11b6bdfe36c48b11367b27187da11d95892f0361": {
    "title": "Maximum Entropy Inverse Reinforcement Learning",
    "abstract": "Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling realworld navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.",
    "paper_id": "11b6bdfe36c48b11367b27187da11d95892f0361"
  },
  "11da2d589485685f792a8ac79d4c2e589e5f77bd": {
    "title": "Show and tell: A neural image caption generator",
    "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
    "paper_id": "11da2d589485685f792a8ac79d4c2e589e5f77bd"
  },
  "11f4d997de8e35a1daf8b115439345d9994cfb69": {
    "title": "Fast Marginal Likelihood Maximisation for Sparse Bayesian Models",
    "abstract": "The \u2018sparse Bayesian\u2019 modelling approach, as exemplified by the \u2018relevance vector machine\u2019, enables sparse classification and regression functions to be obtained by linearly-weighting a small number of fixed basis functions from a large dictionary of potential candidates. Such a model conveys a number of advantages over the related and very popular \u2018support vector machine\u2019, but the necessary \u2018training\u2019 procedure \u2014 optimisation of the marginal likelihood function \u2014 is typically much slower. We describe a new and highly accelerated algorithm which exploits recently-elucidated properties of the marginal likelihood function to enable maximisation via a principled and efficient sequential addition and deletion of candidate basis functions.",
    "paper_id": "11f4d997de8e35a1daf8b115439345d9994cfb69"
  },
  "120106ee391ee380ba09b862268e914d9616aa80": {
    "title": "Deliberate practice and performance in music, games, sports, education, and professions: a meta-analysis.",
    "abstract": "More than 20 years ago, researchers proposed that individual differences in performance in such domains as music, sports, and games largely reflect individual differences in amount of deliberate practice, which was defined as engagement in structured activities created specifically to improve performance in a domain. This view is a frequent topic of popular-science writing-but is it supported by empirical evidence? To answer this question, we conducted a meta-analysis covering all major domains in which deliberate practice has been investigated. We found that deliberate practice explained 26% of the variance in performance for games, 21% for music, 18% for sports, 4% for education, and less than 1% for professions. We conclude that deliberate practice is important, but not as important as has been argued.",
    "paper_id": "120106ee391ee380ba09b862268e914d9616aa80"
  },
  "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0": {
    "title": "Machine learning in automated text categorization",
    "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",
    "paper_id": "13082af1fd6bb9bfe63e73cf007de1655b7f9ae0"
  },
  "130ce1bcd496a7b9192f5f53dd8d7ef626e40675": {
    "title": "Asynchronous Consensus and Broadcast Protocols",
    "abstract": "A consensus protocol enables a system of n asynchronous processes, some of which are faulty, to reach agreement. There are two kinds of faulty processes: fail-stop processes that can only die and malicious processes that can also send false messages. The class of asynchronous systems with fair schedulers is defined, and consensus protocols that terminate with probability 1 for these systems are investigated. With fail-stop processes, it is shown that \u2308(n + 1)/2\u2309 correct processes are necessary and sufficient to reach agreement. In the malicious case, it is shown that \u2308(2n + 1)/3\u2309 correct processes are necessary and sufficient to reach agreement. This is contrasted with an earlier result, stating that there is no consensus protocol for the fail-stop case that always terminates within a bounded number of steps, even if only one process can fail. The possibility of reliable broadcast (Byzantine Agreement) in asynchronous systems is also investigated. Asynchronous Byzantine Agreement is defined, and it is shown that \u2308(2n + 1)/3\u2309 correct processes are necessary and sufficient to achieve it.",
    "paper_id": "130ce1bcd496a7b9192f5f53dd8d7ef626e40675"
  },
  "131f78dbfaf740ceec4c9a233f1e8e28386a2124": {
    "title": "ISLES 2015 - A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI",
    "abstract": "Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained data enables a critical evaluation of the current state-of-the-art, recommendations for further developments, and the identification of remaining challenges. The segmentation of acute perfusion lesions addressed in SPES was found to be feasible. However, algorithms applied to sub-acute lesion segmentation in SISS still lack accuracy. Overall, no algorithmic characteristic of any method was found to perform superior to the others. Instead, the characteristics of stroke lesion appearances, their evolution, and the observed challenges should be studied in detail. The annotated ISLES image datasets continue to be publicly available through an online evaluation system to serve as an ongoing benchmarking resource (www.isles-challenge.org).",
    "paper_id": "131f78dbfaf740ceec4c9a233f1e8e28386a2124"
  },
  "1395f0561db13cad21a519e18be111cbe1e6d818": {
    "title": "Semantic segmentation using regions and parts",
    "abstract": "We address the problem of segmenting and recognizing objects in real world images, focusing on challenging articulated categories such as humans and other animals. For this purpose, we propose a novel design for region-based object detectors that integrates efficiently top-down information from scanning-windows part models and global appearance cues. Our detectors produce class-specific scores for bottom-up regions, and then aggregate the votes of multiple overlapping candidates through pixel classification. We evaluate our approach on the PASCAL segmentation challenge, and report competitive performance with respect to current leading techniques. On VOC2010, our method obtains the best results in 6/20 categories and the highest performance on articulated objects.",
    "paper_id": "1395f0561db13cad21a519e18be111cbe1e6d818"
  },
  "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986": {
    "title": "Random Forests",
    "abstract": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
    "paper_id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986"
  },
  "13dd25c5e7df2b23ec9a168a233598702c2afc97": {
    "title": "Efficient Graph-Based Image Segmentation",
    "abstract": "This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.",
    "paper_id": "13dd25c5e7df2b23ec9a168a233598702c2afc97"
  },
  "13e415ed39f406f1a7c687cb55b6129b1c40ddd5": {
    "title": "Hierarchical models of object recognition in cortex",
    "abstract": "Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.",
    "paper_id": "13e415ed39f406f1a7c687cb55b6129b1c40ddd5"
  },
  "14318685b5959b51d0f1e3db34643eb2855dc6d9": {
    "title": "Going deeper with convolutions",
    "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
    "paper_id": "14318685b5959b51d0f1e3db34643eb2855dc6d9"
  },
  "146f6f6ed688c905fb6e346ad02332efd5464616": {
    "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
    "abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.",
    "paper_id": "146f6f6ed688c905fb6e346ad02332efd5464616"
  },
  "14761b89152aa1fc280a33ea4d77b723df4e3864": {
    "title": "Zero-Shot Learning via Visual Abstraction",
    "abstract": "One of the main challenges in learning fine-grained visual categories is gathering training images. Recent work in Zero-Shot Learning (ZSL) circumvents this challenge by describing categories via attributes or text. However, not all visual concepts, e.g ., two people dancing, are easily amenable to such descriptions. In this paper, we propose a new modality for ZSL using visual abstraction to learn difficult-to-describe concepts. Specifically, we explore concepts related to people and their interactions with others. Our proposed modality allows one to provide training data by manipulating abstract visualizations, e.g ., one can illustrate interactions between two clipart people by manipulating each person\u2019s pose, expression, gaze, and gender. The feasibility of our approach is shown on a human pose dataset and a new dataset containing complex interactions between two people, where we outperform several baselines. To better match across the two domains, we learn an explicit mapping between the abstract and real worlds.",
    "paper_id": "14761b89152aa1fc280a33ea4d77b723df4e3864"
  },
  "14c2321851fb5ae580a19726dd2753a525d6ad76": {
    "title": "Grounding of Textual Phrases in Images by Reconstruction",
    "abstract": "[Charikar 02] Charikar et al. Finding frequent items in data streams. Automata, languages and programming \u201802. [Donahue 15] Donahue et al. Long-term recurrent convolutional networks for visual recognition and description. CVPR\u201915 [Fukui 16] Fukui et al. Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding. EMNLP\u201916 [Guadarrama 14] Guadarrama et al. Open-vocabulary object retrieval. In Robotics: science and systems\u201914 [Hu 16] Hu et al. Natural language object retrieval. CVPR\u201916 [Karpathy 14] Karpathy et al. Deep fragment embeddings for bidirectional image sentence mapping. NIPS\u201814 [Kazemzadeh 14] Kazemzadeh et al. Referit game: Referring to objects in photographs of natural scenes. EMNLP\u201914 [Plummer 15] Plummer et al. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to sentence models. ICCV\u201915 [Pham and Pagh 13] Pham and Pagh. Fast and scalable polynomial kernels via explicit feature maps. KDD\u201913 [Wang 16] Wang et al. Learning Deep Structure-Preserving Image-Text Embeddings. CVPR\u201916 Semi-supervised better than supervised",
    "paper_id": "14c2321851fb5ae580a19726dd2753a525d6ad76"
  },
  "14ce7635ff18318e7094417d0f92acbec6669f1c": {
    "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
    "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.",
    "paper_id": "14ce7635ff18318e7094417d0f92acbec6669f1c"
  },
  "151c9a0e8e31ee17a43bdd66091f49324f36dbdc": {
    "title": "Client-Side Defense Against Web-Based Identity Theft",
    "abstract": "Web spoofing is a significant problem involving fraudulent email and web sites that trick unsuspecting users into revealing private information. We discuss some aspects of common attacks and propose a framework for client-side defense: a browser plug-in that examines web pages and warns the user when requests for data may be part of a spoof attack. While the plugin, SpoofGuard, has been tested using actual sites obtained through government agencies concerned about the problem, we expect that web spoofing and other forms of identity theft will be continuing problems in",
    "paper_id": "151c9a0e8e31ee17a43bdd66091f49324f36dbdc"
  },
  "1521e801e8e08ecec3b0baabb07f9a6ce0a67a85": {
    "title": "Making Byzantine Fault Tolerant Systems Tolerate Byzantine Faults",
    "abstract": "This paper argues for a new approach to building Byzantine fault tolerant systems. We observe that although recently developed BFT state machine replication protocols are quite fast, they don\u2019t actually tolerate Byzantine faults very well: a single faulty client or server is capable of rendering PBFT, Q/U, HQ, and Zyzzyva virtually unusable. In this paper, we (1) demonstrate that existing protocols are dangerously fragile, (2) define a set of principles for constructing BFT services that remain useful even when Byzantine faults occur, and (3) apply these new principles to construct a new protocol, Aardvark, which can achieve peak performance within 25% of that of the best existing protocol in our tests and which provides a significant fraction of that performance when the network is well behaved and up to f servers and any number of clients are faulty. We observe useful throughputs between 11706 and 38667 for a broad range of injected faults.",
    "paper_id": "1521e801e8e08ecec3b0baabb07f9a6ce0a67a85"
  },
  "153dc4d5f2fbd233fec32b8e102f9a7128feed53": {
    "title": "k-Anonymity: A Model for Protecting Privacy",
    "abstract": "Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release ca nnot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to kanonymity unlessaccompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world systems known as Datafly,\u03bc-Argus andk-Similar provide guarantees of privacy protection.",
    "paper_id": "153dc4d5f2fbd233fec32b8e102f9a7128feed53"
  },
  "154898f34460e95aef932bec5615bbd995824cad": {
    "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
    "abstract": "Stereo matching is one of the most active research areas in computer vision. While a large number of algorithms for stereo correspondence have been developed, relatively little work has been done on characterizing their performance. In this paper, we present a taxonomy of dense, two-frame stereo methods. Our taxonomy is designed to assess the different components and design decisions made in individual stereo algorithms. Using this taxonomy, we compare existing stereo methods and present experiments evaluating the performance of many different variants. In order to establish a common software platform and a collection of data sets for easy evaluation, we have designed a stand-alone, flexible C++ implementation that enables the evaluation of individual components and that can easily be extended to include new algorithms. We have also produced several new multi-frame stereo data sets with ground truth and are making both the code and data sets available on the Web. Finally, we include a comparative evaluation of a large set of today's best-performing stereo algorithms.",
    "paper_id": "154898f34460e95aef932bec5615bbd995824cad"
  },
  "16fc1065c296840cb0f8ca62601aa17b7f0a02bf": {
    "title": "Object Detection with Discriminatively Trained Part-Based Models",
    "abstract": "\u2022 Lighting. When an object is in bright light, it looks brighter than when it\u2019s in shadow, so a program can\u2019t just look at image intensity values. \u2022 Within-class variation. Different instances of the same kind of object can look quite different to one another. For example, a green station wagon and a red convertible are both cars, so a program can\u2019t simply compare a picture to one example. \u2022 Aspect. The same object can look very different when viewed at from different directions\u2014pick up a book and compare its cover and its spine to see this effect. Again, this means that a program might need to have many examples of each type of object. \u2022 Deformation. Many objects can change their appearance significantly without their identity changing. For example, you can move your limbs around, change clothes, paint your face, or have your hair cut. You will look very different indeed, but you will still be a person.",
    "paper_id": "16fc1065c296840cb0f8ca62601aa17b7f0a02bf"
  },
  "179661c78320848eb6c0389067b7f12fbbb63a35": {
    "title": "Personalized privacy preservation",
    "abstract": "We study generalization for preserving privacy in publication of sensitive data. The existing methods focus on a universal approach that exerts the same amount of preservation for all persons, with-out catering for their concrete needs. The consequence is that we may be offering insufficient protection to a subset of people, while applying excessive privacy control to another subset. Motivated by this, we present a new generalization framework based on the concept of personalized anonymity. Our technique performs the minimum generalization for satisfying everybody's requirements, and thus, retains the largest amount of information from the microdata. We carry out a careful theoretical study that leads to valuable insight into the behavior of alternative solutions. In particular, our analysis mathematically reveals the circumstances where the previous work fails to protect privacy, and establishes the superiority of the proposed solutions. The theoretical findings are verified with extensive experiments.",
    "paper_id": "179661c78320848eb6c0389067b7f12fbbb63a35"
  },
  "1827de6fa9c9c1b3d647a9d707042e89cf94abf0": {
    "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
    "abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.",
    "paper_id": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0"
  },
  "1835227a28b84b8e7c93a7232437b54c31c52a02": {
    "title": "Training Products of Experts by Minimizing Contrastive Divergence",
    "abstract": "It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.",
    "paper_id": "1835227a28b84b8e7c93a7232437b54c31c52a02"
  },
  "187bbfd80449a88a7c6a5c0fe8ec2dd947cd761c": {
    "title": "Summarization Evaluation Methods: Experiments and Analysis",
    "abstract": "Two methods are used for evaluation of summarization systems: an evaluation of generated summaries against an \"ideal\" summary and evaluation of how well summaries help a person perform in a task such as informa. tion retrieval. We carried out two large experiments to study the two evaluation methods. Our results show that different parameters of an experiment can (h-amatically affect how well a system scores. For example, summary length was found to affect both types of evaluations. For the \"ideal\" summary based evaluation, accuracy decreases as summary length increases, while for task based evaluations summary length and accuracy on an information retrieval task appear to correlate randomly. In this paper, we show how this parameter and others can affect evaluation results and describe how parameters can be controlled to produce a sound evaluation.",
    "paper_id": "187bbfd80449a88a7c6a5c0fe8ec2dd947cd761c"
  },
  "18ae7c9a4bbc832b8b14bc4122070d7939f5e00e": {
    "title": "Overview of the face recognition grand challenge",
    "abstract": "Over the last couple of years, face recognition researchers have been developing new techniques. These developments are being fueled by advances in computer vision techniques, computer design, sensor design, and interest in fielding face recognition systems. Such advances hold the promise of reducing the error rate in face recognition systems by an order of magnitude over Face Recognition Vendor Test (FRVT) 2002 results. The face recognition grand challenge (FRGC) is designed to achieve this performance goal by presenting to researchers a six-experiment challenge problem along with data corpus of 50,000 images. The data consists of 3D scans and high resolution still imagery taken under controlled and uncontrolled conditions. This paper describes the challenge problem, data corpus, and presents baseline performance and preliminary results on natural statistics of facial imagery.",
    "paper_id": "18ae7c9a4bbc832b8b14bc4122070d7939f5e00e"
  },
  "18e93fa7d408e9596992f3d63155cb92827839a4": {
    "title": "Adapting boosting for information retrieval measures",
    "abstract": "We present a new ranking algorithm that combines the strengths of two previous methods: boosted tree classification, and LambdaRank, which has been shown to be empirically optimal for a widely used information retrieval measure. Our algorithm is based on boosted regression trees, although the ideas apply to any weak learners, and it is significantly faster in both train and test phases than the state of the art, for comparable accuracy. We also show how to find the optimal linear combination for any two rankers, and we use this method to solve the line search problem exactly during boosting. In addition, we show that starting with a previously trained model, and boosting using its residuals, furnishes an effective technique for model adaptation, and we give significantly improved results for a particularly pressing problem in web search\u2014training rankers for markets for which only small amounts of labeled data are available, given a ranker trained on much more data from a larger market.",
    "paper_id": "18e93fa7d408e9596992f3d63155cb92827839a4"
  },
  "18f1143c64e6557c933b206fb8b2a7bd1f389afd": {
    "title": "Rich Image Captioning in the Wild",
    "abstract": "We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include generating high quality caption with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out-of-domain datasets. We also make the system publicly accessible as a part of the Microsoft Cognitive Services.",
    "paper_id": "18f1143c64e6557c933b206fb8b2a7bd1f389afd"
  },
  "195155a3b4affc58ec1246d5267632408d37c233": {
    "title": "Factored conditional restricted Boltzmann Machines for modeling motion style",
    "abstract": "The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.",
    "paper_id": "195155a3b4affc58ec1246d5267632408d37c233"
  },
  "19d3b02185ad36fb0b792f2a15a027c58ac91e8e": {
    "title": "Im2Text: Describing Images Using 1 Million Captioned Photographs",
    "abstract": "We develop and demonstrate automatic image description methods using a large captioned photo collection. One contribution is our technique for the automatic collection of this new dataset \u2013 performing a huge number of Flickr queries and then filtering the noisy results down to 1 million images with associated visually relevant captions. Such a collection allows us to approach the extremely challenging problem of description generation using relatively simple non-parametric methods and produces surprisingly effective results. We also develop methods incorporating many state of the art, but fairly noisy, estimates of image content to produce even more pleasing results. Finally we introduce a new objective performance measure for image captioning.",
    "paper_id": "19d3b02185ad36fb0b792f2a15a027c58ac91e8e"
  },
  "19e8aaa1021f829c8ff0378158d9b69699ea4f83": {
    "title": "Temporal Summaries of News Topics",
    "abstract": "We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.",
    "paper_id": "19e8aaa1021f829c8ff0378158d9b69699ea4f83"
  },
  "1a090df137014acab572aa5dc23449b270db64b4": {
    "title": "LIBSVM: a library for support vector machines",
    "abstract": null,
    "paper_id": "1a090df137014acab572aa5dc23449b270db64b4"
  },
  "1a2f1af7ad62e79ca4f3d8f6b24239eec485602b": {
    "title": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization",
    "abstract": "We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.",
    "paper_id": "1a2f1af7ad62e79ca4f3d8f6b24239eec485602b"
  },
  "1b7ffae8feb4d5ec9f2bf396f355ca0fdbfb64c6": {
    "title": "Data Compression and Harmonic Analysis",
    "abstract": "In this article we review some recent interactions between harmonic analysis and data compression. The story goes back of course to Shannon\u2019s R(D) theory in the case of Gaussian stationary processes, which says that transforming into a Fourier basis followed by block coding gives an optimal lossy compression technique; practical developments like transformbased image compression (JPEG) have been inspired by this result. In this article we also discuss connections perhaps less familiar to the Information Theory community, growing out of the field of harmonic analysis. Recent harmonic analysis constructions, such as wavelet transforms and Gabor transforms, are essentially optimal transforms for transform coding in certain settings. Some of these transforms are under consideration for future compression standards, like JPEG-2000. We discuss some of the lessons of harmonic analysis in this century. Typically, the problems and achievements of this field have involved goals that were not obviously related to practical data compression, and have used a language not immediately accessible to outsiders. Nevertheless, through a extensive generalization of what Shannon called the \u2018sampling theorem\u2019, harmonic analysis has succeeded in developing new forms of functional representation which turn out to have significant data compression interpretations. We explain why harmonic analysis has interacted with data compression, and we describe some interesting recent ideas in the field that may affect data compression in the future.",
    "paper_id": "1b7ffae8feb4d5ec9f2bf396f355ca0fdbfb64c6"
  },
  "1bad3e9f15df77f06ae449bba17f9e85a3bb9187": {
    "title": "Centroid-based summarization of multiple documents: sentence extraction utility-based evaluation, and user studies",
    "abstract": "We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",
    "paper_id": "1bad3e9f15df77f06ae449bba17f9e85a3bb9187"
  },
  "1c1911d551553aa17c048ec1fecd1cd0f66edac3": {
    "title": "A Normative Agent System to Prevent Cyberbullying",
    "abstract": "Automated approaches to prevent the negative effects of cyber bullying mainly focus on affective agents that provide support for victims. The current paper takes a complementary approach, which attempts to minimise the amount of occurrences of cyber bullying in the first place. The approach consists of a system of normative agents, which are physically present in a virtual society. The agents, which reason based on a BDI-model, use a number of techniques to detect various norm violations, including insulting and following. By using rewards and punishments, they try to reinforce the desired behaviour of the users. The system has been implemented and tested within a virtual environment for children between 6 and 12 years old, called Club Time Machine. In a real world experiment, the behaviour of the users of the virtual environment has been logged and analysed by means of a logic-based checking tool. The results show that the normative agents have the potential to reduce the amount of norm violations on the long term.",
    "paper_id": "1c1911d551553aa17c048ec1fecd1cd0f66edac3"
  },
  "1c409c370b95ce770eccda97563d72d304f46a6c": {
    "title": "Design of a Passively-Powered, Programmable Sensing Platform for UHF RFID Systems",
    "abstract": "This paper presents a wireless, battery-free, platform for sensing and computation that is powered and read by a standards compliant ultra-high frequency (UHF) radio frequency identification (RFID) reader. The WISP (wireless identification and sensing platform) includes a fully-programmable 16 bit microcontroller with analog-to-digital converter. The microcontroller firmware implements portions of the electronic product code (EPC) class 1 generation 1 protocol. When queried, the platform communicates arbitrary sensor data by emulating an EPC tag whose ID encodes the desired sensor data; the required 16-bit CRC is computed dynamically by the microcontroller. The RFID reader reports the received tag ID to application software which can interpret the information contained in the tag ID. The programmability of the WISP along with its implementation as a PCB allows for flexible integration of arbitrary low-power sensors. Furthermore, sensors are also exclusively powered from the RFID reader resulting in a completely battery free device. Sensors integrated into the WISP platform so far include light, temperature, and rectified voltage, and are shown experimentally to have an operating range of up to 4.5m. To the authors' knowledge, WISP is the first fully programmable computing platform that can operate using power transmitted from a long-range (UHF) RFID reader and communicate arbitrary, multi-bit data in a single response packet.",
    "paper_id": "1c409c370b95ce770eccda97563d72d304f46a6c"
  },
  "1ccef9fa75e519daa10618fe9f2d7a46a34a7040": {
    "title": "The Bitcoin Backbone Protocol: Analysis and Applications",
    "abstract": "Bitcoin is the first and most popular decentralized cryptocurrency to date. In this work, we extract and analyze the core of the Bitcoin protocol, which we term the Bitcoin backbone, and prove two of its fundamental properties which we call common prefix and chain quality in the static setting where the number of players remains fixed. Our proofs hinge on appropriate and novel assumptions on the \u201chashing power\u201d of the adversary relative to network synchronicity; we show our results to be tight under high synchronization. Next, we propose and analyze applications that can be built \u201con top\u201d of the backbone protocol, specifically focusing on Byzantine agreement (BA) and on the notion of a public transaction ledger. Regarding BA, we observe that Nakamoto\u2019s suggestion falls short of solving it, and present a simple alternative which works assuming that the adversary\u2019s hashing power is bounded by 1/3. The public transaction ledger captures the essence of Bitcoin\u2019s operation as a cryptocurrency, in the sense that it guarantees the liveness and persistence of committed transactions. Based on this notion we describe and analyze the Bitcoin system as well as a more elaborate BA protocol, proving them secure assuming high network synchronicity and that the adversary\u2019s hashing power is strictly less than 1/2, while the adversarial bound needed for security decreases as the network desynchronizes. Finally, we show that our analysis of the Bitcoin backbone protocol for synchronous networks extends with relative ease to the recently considered \u201cpartially synchronous\u201d model, where there is an upper bound in the delay of messages that is unknown to the honest parties.",
    "paper_id": "1ccef9fa75e519daa10618fe9f2d7a46a34a7040"
  },
  "1e54e8c5ed809b2b50da37a4c9b93a4aa87f5805": {
    "title": "The spread of obesity in a large social network over 32 years.",
    "abstract": "BACKGROUND\nThe prevalence of obesity has increased substantially over the past 30 years. We performed a quantitative analysis of the nature and extent of the person-to-person spread of obesity as a possible factor contributing to the obesity epidemic.\n\n\nMETHODS\nWe evaluated a densely interconnected social network of 12,067 people assessed repeatedly from 1971 to 2003 as part of the Framingham Heart Study. The body-mass index was available for all subjects. We used longitudinal statistical models to examine whether weight gain in one person was associated with weight gain in his or her friends, siblings, spouse, and neighbors.\n\n\nRESULTS\nDiscernible clusters of obese persons (body-mass index [the weight in kilograms divided by the square of the height in meters], > or =30) were present in the network at all time points, and the clusters extended to three degrees of separation. These clusters did not appear to be solely attributable to the selective formation of social ties among obese persons. A person's chances of becoming obese increased by 57% (95% confidence interval [CI], 6 to 123) if he or she had a friend who became obese in a given interval. Among pairs of adult siblings, if one sibling became obese, the chance that the other would become obese increased by 40% (95% CI, 21 to 60). If one spouse became obese, the likelihood that the other spouse would become obese increased by 37% (95% CI, 7 to 73). These effects were not seen among neighbors in the immediate geographic location. Persons of the same sex had relatively greater influence on each other than those of the opposite sex. The spread of smoking cessation did not account for the spread of obesity in the network.\n\n\nCONCLUSIONS\nNetwork phenomena appear to be relevant to the biologic and behavioral trait of obesity, and obesity appears to spread through social ties. These findings have implications for clinical and public health interventions.",
    "paper_id": "1e54e8c5ed809b2b50da37a4c9b93a4aa87f5805"
  },
  "1f2470a26638449afcab5311c6af5e5edad9e65c": {
    "title": "Recovering the spatial layout of cluttered rooms",
    "abstract": "In this paper, we consider the problem of recovering the spatial layout of indoor scenes from monocular images. The presence of clutter is a major problem for existing single-view 3D reconstruction algorithms, most of which rely on finding the ground-wall boundary. In most rooms, this boundary is partially or entirely occluded. We gain robustness to clutter by modeling the global room space with a parameteric 3D \u201cbox\u201d and by iteratively localizing clutter and refitting the box. To fit the box, we introduce a structured learning algorithm that chooses the set of parameters to minimize error, based on global perspective cues. On a dataset of 308 images, we demonstrate the ability of our algorithm to recover spatial layout in cluttered rooms and show several examples of estimated free space.",
    "paper_id": "1f2470a26638449afcab5311c6af5e5edad9e65c"
  },
  "1f4789a2effea966c8fd10491fe859cfc7607137": {
    "title": "A Dynamic Bayesian Network Model for Autonomous 3D Reconstruction from a Single Indoor Image",
    "abstract": "When we look at a picture, our prior knowledge about the world allows us to resolve some of the ambiguities that are inherent to monocular vision, and thereby infer 3d information about the scene. We also recognize different objects, decide on their orientations, and identify how they are connected to their environment. Focusing on the problem of autonomous 3d reconstruction of indoor scenes, in this paper we present a dynamic Bayesian network model capable of resolving some of these ambiguities and recovering 3d information for many images. Our model assumes a \"floorwall\" geometry on the scene and is trained to recognize the floor-wall boundary in each column of the image. When the image is produced under perspective geometry, we show that this model can be used for 3d reconstruction from a single image. To our knowledge, this was the first monocular approach to automatically recover 3d reconstructions from single indoor images.",
    "paper_id": "1f4789a2effea966c8fd10491fe859cfc7607137"
  },
  "1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f": {
    "title": "Identity Mappings in Deep Residual Networks",
    "abstract": "Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.",
    "paper_id": "1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f"
  },
  "20100323ec5c32ae91add8e866d891a78f1a2bbe": {
    "title": "Unsupervised Object Discovery and Tracking in Video Collections",
    "abstract": "This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates similar object regions within the same video. Interestingly, our algorithm also discovers the implicit topology of frames associated with instances of the same object class across different videos, a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle video collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision.",
    "paper_id": "20100323ec5c32ae91add8e866d891a78f1a2bbe"
  },
  "2071f3ee9ec4d17250b00626d55e47bf75ae2726": {
    "title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition",
    "abstract": "where fk is the current approximation, and Rk f the current residual (error). Using initial values of Ro f = f , fo = 0, and k = 1 , the M P algorithm is comprised of the following steps, In this paper we describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries (I) Compute the inner-products {(Rkf, Zn)},. of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the Matching Pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual convergence. We refer to this modified algorithm as Orthogonal Matching Pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively. (11) Find nktl such that (error) at every step and thereby leads to improved I ( R k f , 'nk+I) l 2 asYp I(Rkf, zj)I 1",
    "paper_id": "2071f3ee9ec4d17250b00626d55e47bf75ae2726"
  },
  "20f90d4c4974692092c3ba78c4957422abea8ac2": {
    "title": "A feature-integration theory of attention",
    "abstract": "A new hypothesis about the role of focused attention is proposed. The feature-integration theory of attention suggests that attention must be directed serially to each stimulus in a display whenever conjunctions of more than one separable feature are needed to characterize or distinguish the possible objects presented. A number of predictions were tested in a variety of paradigms including visual search, texture segregation, identification and localization, and using both separable dimensions (shape and color) and local elements or parts of figures (lines, curves, etc. in letters) as the features to be integrated into complex wholes. The results were in general consistent with the hypothesis. They offer a new set of criteria for distinguishing separable from integral features and a new rationale for predicting which tasks will show attention limits and which will not.",
    "paper_id": "20f90d4c4974692092c3ba78c4957422abea8ac2"
  },
  "2116b2eaaece4af9c28c32af2728f3d49b792cf9": {
    "title": "Improving neural networks by preventing co-adaptation of feature detectors",
    "abstract": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This overfitting is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \u201cdropout\u201d gives big improvements on many benchmark tasks and sets new records for speech and object recognition. A feedforward, artificial neural network uses layers of non-linear hidden units between its inputs and its outputs. By adapting the weights on the incoming connections of these hidden units it learns feature detectors that enable it to predict the correct output when given an input vector [15]. If the relationship between the input and the correct output is complicated and the network has enough hidden units to model it accurately, there will typically be many different settings of the weights that can model the training set almost perfectly, especially if there is only a limited amount of labeled training data. Each of these weight vectors will make different predictions on held-out test data and almost all of them will do worse on the test data than on the training data because the feature detectors have been tuned to work well together on the training data but not on the test data. Overfitting can be reduced by using \u201cdropout\u201d to prevent complex co-adaptations on the training data. On each presentation of each training case, each hidden unit is randomly omitted from the network with a probability of 0.5, so a hidden unit cannot rely on other hidden units being present. Another way to view the dropout procedure is as a very efficient way of performing model averaging with neural networks. A good way to reduce the error on the test set is to average the predictions produced by a very large number of different networks. The standard way to do this is to train many separate networks and then to apply each of these networks to the test data, but this is computationally expensive during both training and testing. Random dropout makes it possible to train a huge number of different networks in a reasonable time. There is almost certainly a different network for each presentation of each training case but all of these networks share the same weights for the hidden units that are present. We use the standard, stochastic gradient descent procedure for training the dropout neural networks on mini-batches of training cases, but we modify the penalty term that is normally used to prevent the weights from growing too large. Instead of penalizing the squared length (L2 norm) of the whole weight vector, we set an upper bound on the L2 norm of the incoming weight vector for each individual hidden unit. If a weight-update violates this constraint, we renormalize the weights of the hidden unit by division. Using a constraint rather than a penalty prevents weights from growing very large no matter how large the proposed weight-update is. This makes it possible to start with a",
    "paper_id": "2116b2eaaece4af9c28c32af2728f3d49b792cf9"
  },
  "214f552070a7eb5ef5efe0d6ffeaaa594a3c3535": {
    "title": "Learning Everything about Anything: Webly-Supervised Visual Concept Learning",
    "abstract": "Recognition is graduating from labs to real-world applications. While it is encouraging to see its potential being tapped, it brings forth a fundamental challenge to the vision researcher: scalability. How can we learn a model for any concept that exhaustively covers all its appearance variations, while requiring minimal or no human supervision for compiling the vocabulary of visual variance, gathering the training images and annotations, and learning the models? In this paper, we introduce a fully-automated approach for learning extensive models for a wide range of variations (e.g. actions, interactions, attributes and beyond) within any concept. Our approach leverages vast resources of online books to discover the vocabulary of variance, and intertwines the data collection and modeling steps to alleviate the need for explicit human supervision in training the models. Our approach organizes the visual knowledge about a concept in a convenient and useful way, enabling a variety of applications across vision and NLP. Our online system has been queried by users to learn models for several interesting concepts including breakfast, Gandhi, beautiful, etc. To date, our system has models available for over 50, 000 variations within 150 concepts, and has annotated more than 10 million images with bounding boxes.",
    "paper_id": "214f552070a7eb5ef5efe0d6ffeaaa594a3c3535"
  },
  "21a1654b856cf0c64e60e58258669b374cb05539": {
    "title": "You Only Look Once: Unified, Real-Time Object Detection",
    "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.",
    "paper_id": "21a1654b856cf0c64e60e58258669b374cb05539"
  },
  "21d4258394a9c8f0ea15f0792d67f7e645720ff6": {
    "title": "Multiscale Combinatorial Grouping",
    "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.",
    "paper_id": "21d4258394a9c8f0ea15f0792d67f7e645720ff6"
  },
  "21e7790abacfee3f2f1901f0f61d9e250e9076a5": {
    "title": "Graph-based submodular selection for extractive summarization",
    "abstract": "We propose a novel approach for unsupervised extractive summarization. Our approach builds a semantic graph for the document to be summarized. Summary extraction is then formulated as optimizing submodular functions defined on the semantic graph. The optimization is theoretically guaranteed to be near-optimal under the framework of submodularity. Extensive experiments on the ICSI meeting summarization task on both human transcripts and automatic speech recognition (ASR) outputs show that the graph-based submodular selection approach consistently outperforms the maximum marginal relevance (MMR) approach, a concept-based approach using integer linear programming (ILP), and a recursive graph-based ranking algorithm using Google's PageRank.",
    "paper_id": "21e7790abacfee3f2f1901f0f61d9e250e9076a5"
  },
  "21eabd8d1f6fcf27bcfa9e08e40384600ca4507f": {
    "title": "Genones: generalized mixture tying in continuous hidden Markov model-based speech recognizers",
    "abstract": "An algorithm is proposed that achieves a good trade-oo between modeling resolution and robustness by using a new, general scheme for tying of mixture components in continuous mixture-density hidden Markov model (HMM)-based speech recognizers. The sets of HMM states that share the same mixture components are determined automatically using agglomerative clustering techniques. Experimental results on ARPA's Wall-Street Journal corpus show that this scheme reduces errors by 25% over typical tied-mixture systems. New fast algorithms for computing Gaussian likelihoods{the most time-consuming aspect of continuous-density HMM systems{are also presented. These new algorithms signiicantly reduce the number of Gaussian densities that are evaluated with little or no impact on speech recognition accuracy.",
    "paper_id": "21eabd8d1f6fcf27bcfa9e08e40384600ca4507f"
  },
  "225409e428b1bee7660f6700277c00c8ba9bd9b0": {
    "title": "MEAD - A Platform for Multidocument Multilingual Text Summarization",
    "abstract": "Abstract This paper describes the functionality of MEAD, a comprehensive, public domain, open source, multidocument multilingual summarization environment that has been thus far downloaded by more than 500 organizations. MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection.",
    "paper_id": "225409e428b1bee7660f6700277c00c8ba9bd9b0"
  },
  "235911a6d8949fcc8edc95f0be10d1ffa00844b4": {
    "title": "High-Efficiency Harmonically Terminated Diode and Transistor Rectifiers",
    "abstract": "This paper presents a theoretical analysis of harmonically terminated high-efficiency power rectifiers and experimental validation on a class-C single Schottky-diode rectifier and a class- F-1 GaN transistor rectifier. The theory is based on a Fourier analysis of current and voltage waveforms, which arise across the rectifying element when different harmonic terminations are presented at its terminals. An analogy to harmonically terminated power amplifier (PA) theory is discussed. From the analysis, one can obtain an optimal value for the dc load given the RF circuit design. An upper limit on rectifier efficiency is derived for each case as a function of the device on-resistance. Measured results from fundamental frequency source-pull measurement of a Schottky diode rectifier with short-circuit terminations at the second and third harmonics are presented. A maximal device rectification efficiency of 72.8% at 2.45 GHz matches the theoretical prediction. A 2.14-GHz GaN HEMT rectifier is designed based on a class-F-1 PA. The gate of the transistor is terminated in an optimal impedance for self-synchronous rectification. Measurements of conversion efficiency and output dc voltage for varying gate RF impedance, dc load, and gate bias are shown with varying input RF power at the drain. The rectifier demonstrates an efficiency of 85% for a 10-W input RF power at the transistor drain with a dc voltage of 30 V across a 98-\u03a9 resistor.",
    "paper_id": "235911a6d8949fcc8edc95f0be10d1ffa00844b4"
  },
  "2446e8f2012f23176ff602be633c0ed2b956d66c": {
    "title": "Large vocabulary continuous speech recognition with context-dependent DBN-HMMS",
    "abstract": "The context-independent deep belief network (DBN) hidden Markov model (HMM) hybrid architecture has recently achieved promising results for phone recognition. In this work, we propose a context-dependent DBN-HMM system that dramatically outperforms strong Gaussian mixture model (GMM)-HMM baselines on a challenging, large vocabulary, spontaneous speech recognition dataset from the Bing mobile voice search task. Our system achieves absolute sentence accuracy improvements of 5.8% and 9.2% over GMM-HMMs trained using the minimum phone error rate (MPE) and maximum likelihood (ML) criteria, respectively, which translate to relative error reductions of 16.0% and 23.2%.",
    "paper_id": "2446e8f2012f23176ff602be633c0ed2b956d66c"
  },
  "2518ece8eeb2fecc3ed0f69adaba98ff83c026e7": {
    "title": "A General Language Model for Information Retrieval",
    "abstract": "Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.",
    "paper_id": "2518ece8eeb2fecc3ed0f69adaba98ff83c026e7"
  },
  "255e97d82f528b613dbe8883727abfd14f3f9f39": {
    "title": "ROUGE: A Package For Automatic Evaluation Of Summaries",
    "abstract": "ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",
    "paper_id": "255e97d82f528b613dbe8883727abfd14f3f9f39"
  },
  "25783dde78a862e735df8f008d7e3bc2f154adde": {
    "title": "A new, fast, and efficient image codec based on set partitioning in hierarchical trees",
    "abstract": "Embedded zerotree wavelet EZW coding introduced by J M Shapiro is a very e ective and computationally simple technique for image compression Here we o er an alternative explanation of the principles of its operation so that the reasons for its excellent performance can be better understood These principles are partial ordering by magnitude with a set partitioning sorting algorithm ordered bit plane transmission and exploitation of self similarity across di erent scales of an image wavelet transform Moreover we present a new and di erent implementation based on set partitioning in hierarchical trees SPIHT which provides even better performance than our previosly reported extension of the EZW that surpassed the performance of the original EZW The image coding results calculated from actual le sizes and images reconstructed by the decoding algorithm are either compara ble to or surpass previous results obtained through much more sophisticated and computationally complex methods In addition the new coding and decoding pro cedures are extremely fast and they can be made even faster with only small loss in performance by omitting entropy coding of the bit stream by arithmetic code",
    "paper_id": "25783dde78a862e735df8f008d7e3bc2f154adde"
  },
  "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a": {
    "title": "Selective Search for Object Recognition",
    "abstract": "This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99\u00a0% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/~uijlings/SelectiveSearch.html ).",
    "paper_id": "25d7da85858a4d89b7de84fd94f0c0a51a9fc67a"
  },
  "2628a4ab8be64bdc71d6cf9d537586d28053db86": {
    "title": "Emergence of Scaling in Random Networks",
    "abstract": "www.sciencemag.org (this information is current as of September 11, 2008 ): The following resources related to this article are available online at http://www.sciencemag.org/cgi/content/full/286/5439/509 version of this article at: including high-resolution figures, can be found in the online Updated information and services, found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/286/5439/509#related-content http://www.sciencemag.org/cgi/content/full/286/5439/509#otherarticles , 3 of which can be accessed for free: cites 11 articles This article 2718 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/286/5439/509#otherarticles 95 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/cgi/collection/physics Physics : subject collections This article appears in the following http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining",
    "paper_id": "2628a4ab8be64bdc71d6cf9d537586d28053db86"
  },
  "2640913656089380ffdd697b141b838a8f214909": {
    "title": "Named Entity Recognition: A Maximum Entropy Approach Using Global Information",
    "abstract": "This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentencebased classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.",
    "paper_id": "2640913656089380ffdd697b141b838a8f214909"
  },
  "26aad3afefe147c2b30ba410b09de47da938f512": {
    "title": "Social phishing",
    "abstract": "Sometimes a \"friendly\" email message tempts recipients to reveal more online than they otherwise would, playing right into the sender's hand.",
    "paper_id": "26aad3afefe147c2b30ba410b09de47da938f512"
  },
  "26adb749fc5d80502a6d889966e50b31391560d3": {
    "title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    "abstract": "Parameter set learned using all WMT12 data (Callison-Burch et al., 2012): \u2022 100,000 binary rankings covering 8 language directions. \u2022Restrict scoring for all languages to exact and paraphrase matching. Parameters encode human preferences that generalize across languages: \u2022Prefer recall over precision. \u2022Prefer word choice over word order. \u2022Prefer correct translations of content words over function words. \u2022Prefer exact matches over paraphrase matches, while still giving significant credit to paraphrases. Visualization",
    "paper_id": "26adb749fc5d80502a6d889966e50b31391560d3"
  },
  "272216c1f097706721096669d85b2843c23fa77d": {
    "title": "Adam: A Method for Stochastic Optimization",
    "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",
    "paper_id": "272216c1f097706721096669d85b2843c23fa77d"
  },
  "280f9cc6ee7679d02a7b8b58d08173628057f3ea": {
    "title": "Evolutionary timeline summarization: a balanced optimization framework via iterative substitution",
    "abstract": "Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.",
    "paper_id": "280f9cc6ee7679d02a7b8b58d08173628057f3ea"
  },
  "28312c3a47c1be3a67365700744d3d6665b86f22": {
    "title": "Face recognition: A literature survey",
    "abstract": "As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.",
    "paper_id": "28312c3a47c1be3a67365700744d3d6665b86f22"
  },
  "2c03df8b48bf3fa39054345bafabfeff15bfd11d": {
    "title": "Deep Residual Learning for Image Recognition",
    "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
    "paper_id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d"
  },
  "2c0a226fef7ac4c8346b586559cf97240d3a32b5": {
    "title": "Network Externalities , Competition , and Compatibility",
    "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at http://www.jstor.org/journals/aea.html. Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed page of such transmission.",
    "paper_id": "2c0a226fef7ac4c8346b586559cf97240d3a32b5"
  },
  "2d83f7bca2d6f8aa4d2fc8a95489a1e1dc8884f1": {
    "title": "Highway long short-term memory RNNS for distant speech recognition",
    "abstract": "In this paper, we extend the deep long short-term memory (DL-STM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains 43.9/47.7% WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with 15.7% and 5.3% relative improvement respectively.",
    "paper_id": "2d83f7bca2d6f8aa4d2fc8a95489a1e1dc8884f1"
  },
  "2dc9b005e936c9c303386caacc8d41cabdb1a0a1": {
    "title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets",
    "abstract": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.",
    "paper_id": "2dc9b005e936c9c303386caacc8d41cabdb1a0a1"
  },
  "2e36ea91a3c8fbff92be2989325531b4002e2afc": {
    "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
    "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
    "paper_id": "2e36ea91a3c8fbff92be2989325531b4002e2afc"
  },
  "2eb6da071a1858f4381577d41664c8715dcd60fa": {
    "title": "t-Closeness: Privacy Beyond k-Anonymity and l-Diversity",
    "abstract": "The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain \"identifying\" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.",
    "paper_id": "2eb6da071a1858f4381577d41664c8715dcd60fa"
  },
  "2f7ad26514bce4df6c8ebc42c90383ef3a974df4": {
    "title": "Pylearn2: a machine learning research library",
    "abstract": "Pylearn2 is a machine learning research library. This does n t just mean that it is a collection of machine learning algorithms that share a comm n API; it means that it has been designed for flexibility and extensibility in ord e to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summar y of the library\u2019s architecture, and a description of how the Pylearn2 communi ty functions socially.",
    "paper_id": "2f7ad26514bce4df6c8ebc42c90383ef3a974df4"
  },
  "3081f5d97c9c9b4917aed99d19fb5cf1cde0b0d1": {
    "title": "An Introduction to MCMC for Machine Learning",
    "abstract": "This purpose of this introductory paper is threefold. First, it introduces the Monte Carlo method with emphasis on probabilistic machine learning. Second, it reviews the main building blocks of modern Markov chain Monte Carlo simulation, thereby providing and introduction to the remaining papers of this special issue. Lastly, it discusses new interesting research horizons.",
    "paper_id": "3081f5d97c9c9b4917aed99d19fb5cf1cde0b0d1"
  },
  "30b339de5b12fe08418b67d532a8b43840355344": {
    "title": "A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations",
    "abstract": "Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards -100% sensitivity at the cost of high FP levels (-40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work.",
    "paper_id": "30b339de5b12fe08418b67d532a8b43840355344"
  },
  "30ec6ebe977de36c2848da0f6e191d4fb18ccb69": {
    "title": "Virtualizing Deep Neural Networks for Memory-Efficient Neural Network Design",
    "abstract": "The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher\u2019s flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average memory usage of AlexNet by 61% and OverFeat by 83%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA K40 GPU card containing 12 GB of memory, with 22% performance loss compared to a hypothetical GPU with enough memory to hold the entire DNN.",
    "paper_id": "30ec6ebe977de36c2848da0f6e191d4fb18ccb69"
  },
  "30efd4205912408f5efcbebb491d9fc1d83d062f": {
    "title": "The Anatomy of a Context-Aware Application",
    "abstract": "We describe a sensor-driven, or sentient, platform for context-aware computing that enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user\u2019s current working desktop to follow them as they move around the environment.",
    "paper_id": "30efd4205912408f5efcbebb491d9fc1d83d062f"
  },
  "31f82b6d86c6cbea7732af58ff8c23e2a2c3b002": {
    "title": "LANDMARC: indoor location sensing using active RFID",
    "abstract": "Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of \"context-aware\" applications, where location is the most essential context. We present LANDMARC, a location sensing prototype system that uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, we demonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market.",
    "paper_id": "31f82b6d86c6cbea7732af58ff8c23e2a2c3b002"
  },
  "33da83b54410af11d0cd18fd07c74e1a99f67e84": {
    "title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition",
    "abstract": "We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be repurposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.",
    "paper_id": "33da83b54410af11d0cd18fd07c74e1a99f67e84"
  },
  "3461f06617b42dadd1ce240a93ffe420513b3399": {
    "title": "Globally Trained Handwritten Word Recognizer Using Spatial Representation, Convolutional Neural Networks, and Hidden Markov Models",
    "abstract": "Yann Le Cun AT&T Bell Labs Holmdel NJ 07733 We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors.",
    "paper_id": "3461f06617b42dadd1ce240a93ffe420513b3399"
  },
  "34760b63a2ae964a0b04d1850dc57002f561ddcb": {
    "title": "Decoding by linear programming",
    "abstract": "This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/|x/sub i/|) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:=|{i:e/sub i/ /spl ne/ 0}|/spl les//spl rho//spl middot/m for some /spl rho/>0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.",
    "paper_id": "34760b63a2ae964a0b04d1850dc57002f561ddcb"
  },
  "3556c846890dc0dbf6cd15ebdcd8932f1fdef6a2": {
    "title": "Inferring activities from interactions with objects",
    "abstract": "A key aspect of pervasive computing is using computers and sensor networks to effectively and unobtrusively infer users' behavior in their environment. This includes inferring which activity users are performing, how they're performing it, and its current stage. Recognizing and recording activities of daily living is a significant problem in elder care. A new paradigm for ADL inferencing leverages radio-frequency-identification technology, data mining, and a probabilistic inference engine to recognize ADLs, based on the objects people use. We propose an approach that addresses these challenges and shows promise in automating some types of ADL monitoring. Our key observation is that the sequence of objects a person uses while performing an ADL robustly characterizes both the ADL's identity and the quality of its execution. So, we have developed Proactive Activity Toolkit (PROACT).",
    "paper_id": "3556c846890dc0dbf6cd15ebdcd8932f1fdef6a2"
  },
  "355de7460120ddc1150d9ce3756f9848983f7ff4": {
    "title": "Midge: Generating Image Descriptions From Computer Vision Detections",
    "abstract": "This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.",
    "paper_id": "355de7460120ddc1150d9ce3756f9848983f7ff4"
  },
  "363b56f85e12389017ba8894056a1b309e46a5f7": {
    "title": "Multiscale conditional random fields for image labeling",
    "abstract": "We propose an approach to include contextual features for labeling images, in which each pixel is assigned to one of a finite set of labels. The features are incorporated into a probabilistic framework, which combines the outputs of several components. Components differ in the information they encode. Some focus on the image-label mapping, while others focus solely on patterns within the label field. Components also differ in their scale, as some focus on fine-resolution patterns while others on coarser, more global structure. A supervised version of the contrastive divergence algorithm is applied to learn these features from labeled image data. We demonstrate performance on two real-world image databases and compare it to a classifier and a Markov random field.",
    "paper_id": "363b56f85e12389017ba8894056a1b309e46a5f7"
  },
  "37100857c059c190fdca074fabe6d6856b480c51": {
    "title": "Applied Text Generation",
    "abstract": "While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and a t tempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the output of the System. In Section 3, we will discuss the overall s tructure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the generation of a short text. In Section 8, we address the problem of portability, and wind up by discussing some shortcomings of Joyce in the conclusion.",
    "paper_id": "37100857c059c190fdca074fabe6d6856b480c51"
  },
  "373f76633cc1f6c7a421e31c989842021a52fca4": {
    "title": "A Fast Learning Algorithm for Deep Belief Nets",
    "abstract": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",
    "paper_id": "373f76633cc1f6c7a421e31c989842021a52fca4"
  },
  "375d7b8a70277d5d7b5e0cc999b03ba395c42901": {
    "title": "Auto-encoder bottleneck features using deep belief networks",
    "abstract": "Neural network (NN) bottleneck (BN) features are typically created by training a NN with a middle bottleneck layer. Recently, an alternative structure was proposed which trains a NN with a constant number of hidden units to predict output targets, and then reduces the dimensionality of these output probabilities through an auto-encoder, to create auto-encoder bottleneck (AE-BN) features. The benefit of placing the BN after the posterior estimation network is that it avoids the loss in frame classification accuracy incurred by networks that place the BN before the softmax. In this work, we investigate the use of pre-training when creating AE-BN features. Our experiments indicate that with the AE-BN architecture, pre-trained and deeper NNs produce better AE-BN features. On a 50-hour English Broadcast News task, the AE-BN features provide over a 1% absolute improvement compared to a state-of-the-art GMM/HMM with a WER of 18.8% and pre-trained NN hybrid system with a WER of 18.4%. In addition, on a larger 430-hour Broadcast News task, AE-BN features provide a 0.5% absolute improvement over a strong GMM/HMM baseline with a WER of 16.0%. Finally, system combination with the GMM/HMM baseline and AE-BN systems provides an additional 0.5% absolute on 430 hours over the AE-BN system alone, yielding a final WER of 15.0%.",
    "paper_id": "375d7b8a70277d5d7b5e0cc999b03ba395c42901"
  },
  "3921f459a9ee26827963abc4abf013b4cc9cbd32": {
    "title": "Crowds by Example",
    "abstract": "We present an example-based crowd simulation technique. Most crowd simulation techniques assume that the behavior exhibited by each person in the crowd can be defined by a restricted set of rules. This assumption limits the behavioral complexity of the simulated agents. By learning from real-world examples, our autonomous agents display complex natural behaviors that are often missing in crowd simulations. Examples are created from tracked video segments of real pedestrian crowds. During a simulation, autonomous agents search for examples that closely match the situation that they are facing. Trajectories taken by real people in similar situations, are copied to the simulated agents, resulting in seemingly natural behaviors.",
    "paper_id": "3921f459a9ee26827963abc4abf013b4cc9cbd32"
  },
  "39348c10c90be968357e2a6b65d5e0e479307735": {
    "title": "Friends and neighbors on the Web",
    "abstract": "The Internet has become a rich and large repository of information about us as individuals. Anything from the links and text on a user's homepage to the mailing lists the user subscribes to are reflections of social interactions a user has in the real world. In this paper we devise techniques to mine this information in order to predict relationships between individuals. Further we show that some pieces of information are better indicators of social connections than others, and that these indicators vary between user populations and provide a glimpse into the social lives of individuals in different communities. Our techniques provide potential applications in automatically inferring real-world connections and discovering, labeling, and characterizing communities.",
    "paper_id": "39348c10c90be968357e2a6b65d5e0e479307735"
  },
  "396b7932beac62a72288eaea047981cc9a21379a": {
    "title": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory",
    "abstract": "We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks\u2014 the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser\u2019s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.",
    "paper_id": "396b7932beac62a72288eaea047981cc9a21379a"
  },
  "39856a57fa0c6e7d646b7db88f48f17688693fe4": {
    "title": "The Stellar Consensus Protocol : A Federated Model for Internet-level Consensus",
    "abstract": "This paper introduces a new model for consensus called federated Byzantine agreement (FBA). FBA achieves robustness through quorum slices\u2014individual trust decisions made by each node that together determine system-level quorums. Slices bind the system together much the way individual networks\u2019 peering and transit decisions now unify the Internet. We also present the Stellar Consensus Protocol (SCP), a construction for FBA. Like all Byzantine agreement protocols, SCP makes no assumptions about the rational behavior of attackers. Unlike prior Byzantine agreement models, which presuppose a unanimously accepted membership list, SCP enjoys open membership that promotes organic network growth. Compared to decentralized proof of-work and proof-of-stake schemes, SCP has modest computing and financial requirements, lowering the barrier to entry and potentially opening up financial systems to new participants.",
    "paper_id": "39856a57fa0c6e7d646b7db88f48f17688693fe4"
  },
  "398c296d0cc7f9d180f84969f8937e6d3a413796": {
    "title": "Multi-column deep neural networks for image classification",
    "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.",
    "paper_id": "398c296d0cc7f9d180f84969f8937e6d3a413796"
  },
  "39978ba7c83333475d6825d0ff897692933895fc": {
    "title": "Conditional Random Fields as Recurrent Neural Networks",
    "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
    "paper_id": "39978ba7c83333475d6825d0ff897692933895fc"
  },
  "39d187585d7df69c330e2bdd5f9c0c41ad99eba0": {
    "title": "Single image depth estimation from predicted semantic labels",
    "abstract": "We consider the problem of estimating the depth of each pixel in a scene from a single monocular image. Unlike traditional approaches [18, 19], which attempt to map from appearance features to depth directly, we first perform a semantic segmentation of the scene and use the semantic labels to guide the 3D reconstruction. This approach provides several advantages: By knowing the semantic class of a pixel or region, depth and geometry constraints can be easily enforced (e.g., \u201csky\u201d is far away and \u201cground\u201d is horizontal). In addition, depth can be more readily predicted by measuring the difference in appearance with respect to a given semantic class. For example, a tree will have more uniform appearance in the distance than it does close up. Finally, the incorporation of semantic features allows us to achieve state-of-the-art results with a significantly simpler model than previous works.",
    "paper_id": "39d187585d7df69c330e2bdd5f9c0c41ad99eba0"
  },
  "39dba6f22d72853561a4ed684be265e179a39e4f": {
    "title": "Sequence to Sequence Learning with Neural Networks",
    "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\u201914 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u2019s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\u2019s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
    "paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f"
  },
  "39f63dbdce9207b87878290c0e3983e84cfcecd9": {
    "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition",
    "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.",
    "paper_id": "39f63dbdce9207b87878290c0e3983e84cfcecd9"
  },
  "3b3c153b09495e2f79dd973253f9d2ee763940a5": {
    "title": "Unsupervised learning of feature hierarchies",
    "abstract": "The applicability of machine learning methods is often limi ted by the amount of available labeled data, and by the ability (or inability) of the de signer to produce good internal representations and good similarity measures for the input data vectors. The aim of this thesis is to alleviate these two limitations by proposing al gorithms tolearngood internal representations, and invariant feature hierarchies from u nlabeled data. These methods go beyond traditional supervised learning algorithms, and rely on unsupervised, and semi-supervised learning. In particular, this work focuses on \u201cdeep learning\u201d methods , a et of techniques and principles to train hierarchical models. Hierarchical mod els produce feature hierarchies that can capture complex non-linear dependencies among the observed data variables in a concise and efficient manner. After training, these mode ls can be employed in real-time systems because they compute the representation by a very fast forward propagation of the input through a sequence of non-linear transf ormations. When the paucity of labeled data does not allow the use of traditional supervi s d algorithms, each layer of the hierarchy can be trained in sequence starting at the bott om by using unsupervised or semi-supervised algorithms. Once each layer has been train ed, the whole system can be fine-tuned in an end-to-end fashion. We propose several unsu pervised algorithms that can be used as building block to train such feature hierarchi es. We investigate algorithms that produce sparse overcomplete representations a nd fe tures that are invariant to known and learned transformations. These algorithms are designed using the Energy-",
    "paper_id": "3b3c153b09495e2f79dd973253f9d2ee763940a5"
  },
  "3b7fa7abb8220adc84da49313a8c72e9c12592e1": {
    "title": "A Model for Learning the Semantics of Pictures",
    "abstract": "We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.",
    "paper_id": "3b7fa7abb8220adc84da49313a8c72e9c12592e1"
  },
  "3bfa75238e15e869b902ceb62b31ffddbe8ccb0d": {
    "title": "Describing Images using Inferred Visual Dependency Representations",
    "abstract": "The Visual Dependency Representation (VDR) is an explicit model of the spatial relationships between objects in an image. In this paper we present an approach to training a VDR Parsing Model without the extensive human supervision used in previous work. Our approach is to find the objects mentioned in a given description using a state-of-the-art object detector, and to use successful detections to produce training data. The description of an unseen image is produced by first predicting its VDR over automatically detected objects, and then generating the text with a template-based generation model using the predicted VDR. The performance of our approach is comparable to a state-ofthe-art multimodal deep neural network in images depicting actions.",
    "paper_id": "3bfa75238e15e869b902ceb62b31ffddbe8ccb0d"
  },
  "3d0379688518cc0e8f896e30815d0b5e8452d4cd": {
    "title": "Autotagging Facebook: Social network context improves photo annotation",
    "abstract": "Most personal photos that are shared online are embedded in some form of social network, and these social networks are a potent source of contextual information that can be leveraged for automatic image understanding. In this paper, we investigate the utility of social network context for the task of automatic face recognition in personal photographs. We combine face recognition scores with social context in a conditional random field (CRF) model and apply this model to label faces in photos from the popular online social network Facebook, which is now the top photo-sharing site on the Web with billions of photos in total. We demonstrate that our simple method of enhancing face recognition with social network context substantially increases recognition performance beyond that of a baseline face recognition system.",
    "paper_id": "3d0379688518cc0e8f896e30815d0b5e8452d4cd"
  },
  "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87": {
    "title": "CIDEr: Consensus-based image description evaluation",
    "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.",
    "paper_id": "3d275a4e4f44d452f21e0e0ff6145a5e18e6cf87"
  },
  "3d8359c257c6c2dd5f170d4ff22d213af011940e": {
    "title": "Compressed sensing",
    "abstract": "Suppose x is an unknown vector in Ropfm (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n=O(m1/4log5/2(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an lscrp ball for 0<ples1. The N most important coefficients in that expansion allow reconstruction with lscr2 error O(N1/2-1p/). It is possible to design n=O(Nlog(m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of \"random\" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of lscrp balls in high-dimensional Euclidean space in the case 0<ples1, and give a criterion identifying near- optimal subspaces for Gel'fand n-widths. We show that \"most\" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces",
    "paper_id": "3d8359c257c6c2dd5f170d4ff22d213af011940e"
  },
  "3f5e8f884e71310d7d5571bd98e5a049b8175075": {
    "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
    "abstract": "Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method\u2019s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimizaProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s). tion process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a facematching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.",
    "paper_id": "3f5e8f884e71310d7d5571bd98e5a049b8175075"
  },
  "3f6a4556769e819242d669d073b895f1e45a706f": {
    "title": "Image Description using Visual Dependency Representations",
    "abstract": "Describing the main event of an image involves identifying the objects depicted and predicting the relationships between them. Previous approaches have represented images as unstructured bags of regions, which makes it difficult to accurately predict meaningful relationships between regions. In this paper, we introduce visual dependency representations to capture the relationships between the objects in an image, and hypothesize that this representation can improve image description. We test this hypothesis using a new data set of region-annotated images, associated with visual dependency representations and gold-standard descriptions. We describe two template-based description generation models that operate over visual dependency representations. In an image description task, we find that these models outperform approaches that rely on object proximity or corpus information to generate descriptions on both automatic measures and on human judgements.",
    "paper_id": "3f6a4556769e819242d669d073b895f1e45a706f"
  },
  "4187d2250b0d270d58710860331bc2e49610e291": {
    "title": "Greed and Grievance in Civil War",
    "abstract": "We investigate the causes of civil war, using a new data set of wars during 1960-99. Rebellion may be explained by atypically severe grievances, such as high inequality, a lack of political rights, or ethnic and religious divisions in society. Alternatively, it might be explained by atypical opportunities for building a rebel organization. Opportunity may be determined by access to finance, such as the scope for extortion of natural resources, and for donations from a diaspora population. Opportunity may also depend upon factors such as geography: mountains and forests may be needed to incubate rebellion. We test these explanations and find that opportunity provides considerably more explanatory power than grievance. Economic viability appears to be the predominant systematic explanation of rebellion. The results are robust to correction for outliers, alternative variable definition, and variations in estimation method. The findings, interpretations, and conclusions expressed in this paper are entirely those of the author. They do not necessarily represent the views of the World Bank, its Executive Directors, or the countries they represent. P ub lic D is cl os ur e A ut ho riz ed P ub lic D is cl os ur e A ut ho riz ed P ub lic D is cl os ur e A ut ho riz ed P ub lic D is cl os ur e A ut ho riz ed",
    "paper_id": "4187d2250b0d270d58710860331bc2e49610e291"
  },
  "41951953579a0e3620f0235e5fcb80b930e6eee3": {
    "title": "Deep Learning Face Representation by Joint Identification-Verification",
    "abstract": "The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 features extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 features extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset [11], 99.15% face verification accuracy is achieved. Compared with the best previous deep learning result [20] on LFW, the error rate has been significantly reduced by 67%.",
    "paper_id": "41951953579a0e3620f0235e5fcb80b930e6eee3"
  },
  "41f1abe566060e53ad93d8cfa8c39ac582256868": {
    "title": "Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial",
    "abstract": "The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models\u2014Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.",
    "paper_id": "41f1abe566060e53ad93d8cfa8c39ac582256868"
  },
  "424561d8585ff8ebce7d5d07de8dbf7aae5e7270": {
    "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
    "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet <xref ref-type=\"bibr\" rid=\"ref1\">[1]</xref> and Fast R-CNN <xref ref-type=\"bibr\" rid=\"ref2\">[2]</xref> have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a <italic>Region Proposal Network</italic> (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features\u2014using the recently popular terminology of neural networks with \u2019attention\u2019 mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model <xref ref-type=\"bibr\" rid=\"ref3\">[3]</xref> , our detection system has a frame rate of 5 fps (<italic>including all steps</italic>) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",
    "paper_id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270"
  },
  "429b8d5bb05e1a580fad0222b9e9496985465e40": {
    "title": "See No Evil, Say No Evil: Description Generation from Densely Labeled Images",
    "abstract": "This paper studies generation of descriptive sentences from densely annotated images. Previous work studied generation from automatically detected visual information but produced a limited class of sentences, hindered by currently unreliable recognition of activities and attributes. Instead, we collect human annotations of objects, parts, attributes and activities in images. These annotations allow us to build a significantly more comprehensive model of language generation and allow us to study what visual information is required to generate human-like descriptions. Experiments demonstrate high quality output and that activity annotations and relative spatial location of objects contribute most to producing high quality sentences.",
    "paper_id": "429b8d5bb05e1a580fad0222b9e9496985465e40"
  },
  "429e2f4d95fe77d8c61245265529963df171da68": {
    "title": "Working Memory Capacity as Executive Attention",
    "abstract": "Performance on measures of working memory (WM) capacity predicts performance on a wide range of real-world cognitive tasks. I review the idea that WM capacity (a) is separable from short-term memory, (b) is an important component of general fluid intelligence, and (c) represents a domainfree limitation in ability to control attention. Studies show that individual differences in WM capacity are reflected in performance on antisaccade, Stroop, and dichotic-listening tasks. WM capacity, or executive attention, is most important under conditions in which interference leads to retrieval of response tendencies that conflict with the current task.",
    "paper_id": "429e2f4d95fe77d8c61245265529963df171da68"
  },
  "42e3dac0df30d754c7c7dab9e1bb94990034a90d": {
    "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling",
    "abstract": "We propose a method for inferring human attributes (such as gender, hair style, clothes style, expression, action) from images of people under large variation of viewpoint, pose, appearance, articulation and occlusion. Convolutional Neural Nets (CNN) have been shown to perform very well on large scale object recognition problems. In the context of attribute classification, however, the signal is often subtle and it may cover only a small part of the image, while the image is dominated by the effects of pose and viewpoint. Discounting for pose variation would require training on very large labeled datasets which are not presently available. Part-based models, such as poselets [4] and DPM [12] have been shown to perform well for this problem but they are limited by shallow low-level features. We propose a new method which combines part-based models and deep learning by training pose-normalized CNNs. We show substantial improvement vs. state-of-the-art methods on challenging attribute classification tasks in unconstrained settings. Experiments confirm that our method outperforms both the best part-based methods on this problem and conventional CNNs trained on the full bounding box of the person.",
    "paper_id": "42e3dac0df30d754c7c7dab9e1bb94990034a90d"
  },
  "434a0c7b39f0f08ce0252d53dfb6d490615d0fed": {
    "title": "Automatic Vandalism Detection in Wikipedia : Towards a Machine Learning Approach",
    "abstract": "Since the end of 2006 several autonomous bots are, or have been, running on Wikipedia to keep the encyclopedia free from vandalism and other damaging edits. These expert systems, however, are far from optimal and should be improved to relieve the human editors from the burden of manually reverting such edits. We investigate the possibility of using machine learning techniques to build an autonomous system capable to distinguish vandalism from legitimate edits. We highlight the results of a small but important step in this direction by applying commonly known machine learning algorithms using a straightforward feature representation. Despite the promising results, this study reveals that elementary features, which are also used by the current approaches to fight vandalism, are not sufficient to build such a system. They will need to be accompanied by additional information which, among other things, incorporates the semantics of a",
    "paper_id": "434a0c7b39f0f08ce0252d53dfb6d490615d0fed"
  },
  "434d6726229c0f556841fad20391c18316806f73": {
    "title": "Detecting Visual Relationships with Deep Relational Networks",
    "abstract": "Relationships among objects play a crucial role in image understanding. Despite the great success of deep learning techniques in recognizing individual objects, reasoning about the relationships among objects remains a challenging task. Previous methods often treat this as a classification problem, considering each type of relationship (e.g. ride) or each distinct visual phrase (e.g. person-ride-horse) as a category. Such approaches are faced with significant difficulties caused by the high diversity of visual appearance for each kind of relationships or the large number of distinct visual phrases. We propose an integrated framework to tackle this problem. At the heart of this framework is the Deep Relational Network, a novel formulation designed specifically for exploiting the statistical dependencies between objects and their relationships. On two large data sets, the proposed method achieves substantial improvement over state-of-the-art.",
    "paper_id": "434d6726229c0f556841fad20391c18316806f73"
  },
  "439926670316aaa4236fd4677c7c82f6dc15804d": {
    "title": "Enterprise Integration Patterns",
    "abstract": "Integration of applications and business processes is a top priority for many enterprises today. Requirements for improved customer service or self-service, rapidly changing business environments and support for mergers and acquisitions are major drivers for increased integration between existing \u201cstovepipe\u201d systems. Very few new business applications are being developed or deployed without a major focus on integration, essentially making integratability a defining quality of enterprise applications.",
    "paper_id": "439926670316aaa4236fd4677c7c82f6dc15804d"
  },
  "43e48b702fbe1feba53afbf82ec322cc9a61ae6c": {
    "title": "OpenSurfaces: a richly annotated catalog of surface appearance",
    "abstract": "The appearance of surfaces in real-world scenes is determined by the materials, textures, and context in which the surfaces appear. However, the datasets we have for visualizing and modeling rich surface appearance in context, in applications such as home remodeling, are quite limited. To help address this need, we present OpenSurfaces, a rich, labeled database consisting of thousands of examples of surfaces segmented from consumer photographs of interiors, and annotated with material parameters (reflectance, material names), texture information (surface normals, rectified textures), and contextual information (scene category, and object names).\n Retrieving usable surface information from uncalibrated Internet photo collections is challenging. We use human annotations and present a new methodology for segmenting and annotating materials in Internet photo collections suitable for crowdsourcing (e.g., through Amazon's Mechanical Turk). Because of the noise and variability inherent in Internet photos and novice annotators, designing this annotation engine was a key challenge; we present a multi-stage set of annotation tasks with quality checks and validation. We demonstrate the use of this database in proof-of-concept applications including surface retexturing and material and image browsing, and discuss future uses. OpenSurfaces is a public resource available at http://opensurfaces.cs.cornell.edu/.",
    "paper_id": "43e48b702fbe1feba53afbf82ec322cc9a61ae6c"
  },
  "445da23c1b17b69f2db0c66890845b68e9eaa506": {
    "title": "Contextual Priming for Object Detection",
    "abstract": "There is general consensus that context can be a rich source of information about an object's identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.",
    "paper_id": "445da23c1b17b69f2db0c66890845b68e9eaa506"
  },
  "44fca068eecce2203d111213e3691647914a3945": {
    "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization",
    "abstract": "We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents.",
    "paper_id": "44fca068eecce2203d111213e3691647914a3945"
  },
  "461ac81b6ce10d48a6c342e64c59f86d7566fa68": {
    "title": "Social network sites: definition, history, and scholarship",
    "abstract": "This publication contains reprint articles for which IEEE does not hold copyright. Full text is not available on IEEE Xplore for these articles.",
    "paper_id": "461ac81b6ce10d48a6c342e64c59f86d7566fa68"
  },
  "46a1172c784c3741e79781ef2353209b08dbea67": {
    "title": "YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition",
    "abstract": "Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities ``in-the-wild''. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from Web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects, we also use a Web-scale language model to ``fill in'' novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.",
    "paper_id": "46a1172c784c3741e79781ef2353209b08dbea67"
  },
  "46a1dbf20cd13dc3204eb0cc35fc34aaa3e2a70c": {
    "title": "SpotON : An Indoor 3 D Location Sensing Technology Based on RF Signal Strength",
    "abstract": "Providing a reliable technology and architecture for determining the location of real world objects and people will undoubtedly enable applications, customization, and inference not currently possible. This paper documents the creation of SpotON, a new tagging technology for three dimensional location sensing based on radio signal strength analysis. Although there are many aspects to the topic of location sensing and several of them will be briefly discussed, this paper is primarily concerned with the hardware and embedded system development of such a system.",
    "paper_id": "46a1dbf20cd13dc3204eb0cc35fc34aaa3e2a70c"
  },
  "471e9d18671d713528bf2255eba68476d5acdf2f": {
    "title": "Influence and correlation in social networks",
    "abstract": "In many online social systems, social ties between users play an important role in dictating their behavior. One of the ways this can happen is through social influence, the phenomenon that the actions of a user can induce his/her friends to behave in a similar way. In systems where social influence exists, ideas, modes of behavior, or new technologies can diffuse through the network like an epidemic. Therefore, identifying and understanding social influence is of tremendous interest from both analysis and design points of view.\n This is a difficult task in general, since there are factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network. Distinguishing influence from these is essentially the problem of distinguishing correlation from causality, a notoriously hard statistical problem.\n In this paper we study this problem systematically. We define fairly general models that replicate the aforementioned sources of social correlation. We then propose two simple tests that can identify influence as a source of social correlation when the time series of user actions is available.\n We give a theoretical justification of one of the tests by proving that with high probability it succeeds in ruling out influence in a rather general model of social correlation. We also simulate our tests on a number of examples designed by randomly generating actions of nodes on a real social network (from Flickr) according to one of several models. Simulation results confirm that our test performs well on these data. Finally, we apply them to real tagging data on Flickr, exhibiting that while there is significant social correlation in tagging behavior on this system, this correlation cannot be attributed to social influence.",
    "paper_id": "471e9d18671d713528bf2255eba68476d5acdf2f"
  },
  "47598c6267a065ad0f9226c0a130728fedf18b81": {
    "title": "Gaussian Processes for Machine Learning",
    "abstract": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes.",
    "paper_id": "47598c6267a065ad0f9226c0a130728fedf18b81"
  },
  "47dd6b9d9cedbe2526ad22a01ca4fea1025e07d1": {
    "title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features",
    "abstract": "This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies why SVMs arc appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning. 1 I n t r o d u c t i o n With the rapid growth of online information, text categorization has become one of the key techniques for handling and organizing text data. Text categorization techniques are used to classify news stories, to find interesting information on the WWW, and to guide a user's search through hypertext. Since building text classifiers by hand is difficult and time-consuming, it is advantageous to learn classifiers from examples. In this paper I will explore and identify the benefits of Support Vector Machines (SVMs) for text categorization. SVMs are a new learning method introduced by V. Vapnik et al. [9] [1]. They are well-founded in terms of computational learning theory and very open to theoretical understanding and analysis. After reviewing the standard feature vector representation of text, I will identify the particular properties of text in this representation in section 4. I will argue that SVMs are very well suited for learning in this setting. The empirical results in section 5 will support this claim. Compared to state-of-the-art methods, SVMs show substantial performance gains. Moreover, in contrast to conventional text classification methods SVMs will prove to be very robust, eliminating the need for expensive parameter tuning. 2 T e x t C a t e g o r i z a t i o n The goal of text categorization is the classification of documents into a fixed number of predefined categories. Each document can be in multiple, exactly one, or no category at all. Using machine learning, the objective is to learn classifiers",
    "paper_id": "47dd6b9d9cedbe2526ad22a01ca4fea1025e07d1"
  },
  "48326c5da8fd277cc32e1440b544793c397e41d6": {
    "title": "Practical byzantine fault tolerance and proactive recovery",
    "abstract": "Our growing reliance on online services accessible on the Internet demands highly available systems that provide correct service without interruptions. Software bugs, operator mistakes, and malicious attacks are a major cause of service interruptions and they can cause arbitrary behavior, that is, Byzantine faults. This article describes a new replication algorithm, BFT, that can be used to build highly available systems that tolerate Byzantine faults. BFT can be used in practice to implement real services: it performs well, it is safe in asynchronous environments such as the Internet, it incorporates mechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. The recovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a small window of vulnerability. BFT has been implemented as a generic program library with a simple interface. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. The BFT library and BFS perform well because the library incorporates several important optimizations, the most important of which is the use of symmetric cryptography to authenticate messages. The performance results show that BFS performs 2% faster to 24% slower than production implementations of the NFS protocol that are not replicated. This supports our claim that the BFT library can be used to build practical systems that tolerate Byzantine faults.",
    "paper_id": "48326c5da8fd277cc32e1440b544793c397e41d6"
  },
  "4856e7719e566f2466369ae2031afb07c934d4d3": {
    "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
    "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.",
    "paper_id": "4856e7719e566f2466369ae2031afb07c934d4d3"
  },
  "4991785cb0e6ee3d0b7823b59e144fb80ca3a83e": {
    "title": "VQA: Visual Question Answering",
    "abstract": null,
    "paper_id": "4991785cb0e6ee3d0b7823b59e144fb80ca3a83e"
  },
  "4aeab014be438bbb68ebe4bf1ceb4cb298cc4298": {
    "title": "Extensive Imitation is Irrational and Harmful \u2217",
    "abstract": "Rationality leads people to imitate those with similar tastes but different information. But people who imitate common sources develop correlated beliefs, and rationality demands that later social learners take this redundancy into account. This implies severe limits to rational imitation. We show that (i) in most natural observation structures besides the canonical single-file case, full rationality dictates that people must \u201canti-imitate\u201d some of those they observe; and (ii) in every observation structure full rationality dictates that people who do not anti-imitate can, in essence, imitate at most one person among predecessors who share common information. We also show that in a very broad class of settings, virtually any learning rule in which people regularly do imitate more than one person without anti-imitating others will lead to a positive (and, in some environments, arbitrarily high) probability of people converging to confident and wrong long-run beliefs. When testing either the rationality or the efficiency of social learning, researchers should not focus on whether people follow others\u2019 behavior\u2014but instead whether they follow it too much. (JEL B49) \u2217We thank Paul Heidhues, seminar participants at Berkeley, DIW Berlin, Hebrew University, ITAM, LSE, Oxford, Penn, Tel Aviv University, and WZB, as well as the editor and referees, for their comments. We are grateful to the Russell Sage Foundation for its hospitality, and Katie Winograd and especially Claire Gabriel for their help with the historical literature on treating syphilis and other medical practices, and Tristan Gagnon-Bartsch and Min Zhang for excellent research assistance. \u2020Eyster: Department of Economics, LSE, Houghton Street, London WC2A 2AE, UK. Rabin: Department of Economics, UC Berkeley, 530 Evans Hall #3880, Berkeley, CA 94720-3880, USA.",
    "paper_id": "4aeab014be438bbb68ebe4bf1ceb4cb298cc4298"
  },
  "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c": {
    "title": "Recognizing Image Style",
    "abstract": "The style of an image plays a significant role in how it is viewed, but style has received little attention in computer vision research. We describe an approach to predicting style of images, and perform a thorough evaluation of different image features for these tasks. We find that features learned in a multi-layer network generally perform best \u2013 even when trained with object class (not style) labels. Our large-scale learning methods results in the best published performance on an existing dataset of aesthetic ratings and photographic style annotations. We present two novel datasets: 80K Flickr photographs annotated with 20 curated style labels, and 85K paintings annotated with 25 style/genre labels. Our approach shows excellent classification performance on both datasets. We use the learned classifiers to extend traditional tag-based image search to consider stylistic constraints, and demonstrate cross-dataset understanding of style.",
    "paper_id": "4af785bf8a5959d7e8eb37ca87c45db2ac6a544c"
  },
  "4b3c5b49fa099a77c98bad4b5299c6b4eb0a8f2e": {
    "title": "Learning Deep Features for Discriminative Localization",
    "abstract": "In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation. We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.",
    "paper_id": "4b3c5b49fa099a77c98bad4b5299c6b4eb0a8f2e"
  },
  "4d3fb523b7892e0b7a7990366e02fae46d63dcb4": {
    "title": "Fast Image Tagging",
    "abstract": "Automatic image annotation is a difficult and highly relevant machine learning task. Recent advances have significantly improved the state-of-the-art in retrieval accuracy with algorithms based on nearest neighbor classification in carefully learned metric spaces. But this comes at a price of increased computational complexity during training and testing. We propose FastTag, a novel algorithm that achieves comparable results with two simple linear mappings that are co-regularized in a joint convex loss function. The loss function can be efficiently optimized in closed form updates, which allows us to incorporate a large number of image descriptors cheaply. On several standard real-world benchmark data sets, we demonstrate that FastTag matches the current state-of-the-art in tagging quality, yet reduces the training and testing times by several orders of magnitude and has lower asymptotic complexity.",
    "paper_id": "4d3fb523b7892e0b7a7990366e02fae46d63dcb4"
  },
  "4dc1641582a60abdc66a9d818c313a9d783a74be": {
    "title": "Training Very Deep Networks",
    "abstract": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.",
    "paper_id": "4dc1641582a60abdc66a9d818c313a9d783a74be"
  },
  "4eb943bf999ce49e5ebb629d7d0ffee44becff94": {
    "title": "Finding Structure in Time",
    "abstract": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.",
    "paper_id": "4eb943bf999ce49e5ebb629d7d0ffee44becff94"
  },
  "4f410ab5c8b12b34b38421241366ee456bbebab9": {
    "title": "Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling",
    "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",
    "paper_id": "4f410ab5c8b12b34b38421241366ee456bbebab9"
  },
  "4fd7c507d3733240ab785483e790bfa6b28ff6f4": {
    "title": "Open-vocabulary Object Retrieval",
    "abstract": "In this paper, we address the problem of retrieving objects based on open-vocabulary natural language queries: Given a phrase describing a specific object, e.g., \u201cthe corn flakes box\u201d, the task is to find the best match in a set of images containing candidate objects. When naming objects, humans tend to use natural language with rich semantics, including basic-level categories, fine-grained categories, and instance-level concepts such as brand names. Existing approaches to large-scale object recognition fail in this scenario, as they expect queries that map directly to a fixed set of pre-trained visual categories, e.g. ImageNet synset tags. We address this limitation by introducing a novel object retrieval method. Given a candidate object image, we first map it to a set of words that are likely to describe it, using several learned image-to-text projections. We also propose a method for handling open-vocabularies, i.e., words not contained in the training data. We then compare the natural language query to the sets of words predicted for each candidate and select the best match. Our method can combine categoryand instance-level semantics in a common representation. We present extensive experimental results on several datasets using both instance-level and category-level matching and show that our approach can accurately retrieve objects based on extremely varied open-vocabulary queries. The source code of our approach will be publicly available together with pre-trained models at http://openvoc.berkeleyvision.org and could be directly used for robotics applications.",
    "paper_id": "4fd7c507d3733240ab785483e790bfa6b28ff6f4"
  },
  "5075636b9e0425358204211706adde1ecb5ba60b": {
    "title": "Convex Neural Networks",
    "abstract": "Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.",
    "paper_id": "5075636b9e0425358204211706adde1ecb5ba60b"
  },
  "50e983fd06143cad9d4ac75bffc2ef67024584f2": {
    "title": "LIBLINEAR: A Library for Large Linear Classification",
    "abstract": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.",
    "paper_id": "50e983fd06143cad9d4ac75bffc2ef67024584f2"
  },
  "514dd3e2e9fa732359b85ce4dd7d9437ef86cc99": {
    "title": "Webly Supervised Learning of Convolutional Networks",
    "abstract": "We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style [19] detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).",
    "paper_id": "514dd3e2e9fa732359b85ce4dd7d9437ef86cc99"
  },
  "51c765b8d872c206f6dd781ab26bd5a8c2feb81e": {
    "title": "Semantic Image Segmentation via Deep Parsing Network",
    "abstract": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.",
    "paper_id": "51c765b8d872c206f6dd781ab26bd5a8c2feb81e"
  },
  "52475db551dfb5537a2df7d3baa02d7e7caa1f24": {
    "title": "Learning to navigate through crowded environments",
    "abstract": "The goal of this research is to enable mobile robots to navigate through crowded environments such as indoor shopping malls, airports, or downtown side walks. The key research question addressed in this paper is how to learn planners that generate human-like motion behavior. Our approach uses inverse reinforcement learning (IRL) to learn human-like navigation behavior based on example paths. Since robots have only limited sensing, we extend existing IRL methods to the case of partially observable environments. We demonstrate the capabilities of our approach using a realistic crowd flow simulator in which we modeled multiple scenarios in crowded environments. We show that our planner learned to guide the robot along the flow of people when the environment is crowded, and along the shortest path if no people are around.",
    "paper_id": "52475db551dfb5537a2df7d3baa02d7e7caa1f24"
  },
  "5287d8fef49b80b8d500583c07e935c7f9798933": {
    "title": "Generative Adversarial Text to Image Synthesis",
    "abstract": "Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",
    "paper_id": "5287d8fef49b80b8d500583c07e935c7f9798933"
  },
  "528fa9bb03644ba752fb9491be49b9dd1bce1d52": {
    "title": "SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity",
    "abstract": "Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation>80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.",
    "paper_id": "528fa9bb03644ba752fb9491be49b9dd1bce1d52"
  },
  "52e1282d76d216b99340ca8d0381291654e2e98d": {
    "title": "NATURAL RESOURCE ABUNDANCE AND ECONOMIC GROWTH",
    "abstract": "One of the surprising features of modern economic growth is that economies abundant in natural resources have tended to grow slower than economies without substantial natural resources. In this paper we show that economies with a high ratio of natural resource exports to GDP in 1970 (the base year) tended to grow slowly during the subsequent 20-year period 1970-1990. This negative relationship holds true even after controlling for many variables found to be important for economic growth by previous authors. We discuss several theories and present additional evidence to understand the source of this negative association.",
    "paper_id": "52e1282d76d216b99340ca8d0381291654e2e98d"
  },
  "5330c869c67e39b7d2d61e555f69807a49064de4": {
    "title": "PageRank on Semantic Networks, with Application to Word Sense Disambiguation",
    "abstract": "This paper presents a new open text word sense disambiguation method that combines the use of logical inferences with PageRank-style algorithms applied on graphs extracted from natural language documents. We evaluate the accuracy of the proposed algorithm on several senseannotated texts, and show that it consistently outperforms the accuracy of other previously proposed knowledge-based word sense disambiguation methods. We also explore and evaluate methods that combine several open-text word sense disambiguation algorithms.",
    "paper_id": "5330c869c67e39b7d2d61e555f69807a49064de4"
  },
  "53698b91709112e5bb71eeeae94607db2aefc57c": {
    "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
    "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to incorporate into the network design aspects of the best performing hand-crafted features. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multitask learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it matches the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.",
    "paper_id": "53698b91709112e5bb71eeeae94607db2aefc57c"
  },
  "54dd77bd7b904a6a69609c9f3af11b42f654ab5d": {
    "title": "ImageNet: A large-scale hierarchical image database",
    "abstract": null,
    "paper_id": "54dd77bd7b904a6a69609c9f3af11b42f654ab5d"
  },
  "55230857760eda5b741c020aea01e7d8ea338667": {
    "title": "Query based event extraction along a timeline",
    "abstract": "In this paper, we present a framework and a system that extracts events relevant to a query from a collection C of documents, and places such events along a timeline. Each event is represented by a sentence extracted from C, based on the assumption that \"important\" events are widely cited in many documents for a period of time within which these events are of interest. In our experiments, we used queries that are event types (\"earthquake\") and person names (e.g. \"George Bush\"). Evaluation was performed using G8 leader names as queries: comparison made by human evaluators between manually and system generated timelines showed that although manually generated timelines are on average more preferable, system generated timelines are sometimes judged to be better than manually constructed ones.",
    "paper_id": "55230857760eda5b741c020aea01e7d8ea338667"
  },
  "55b81991fbb025038d98e8c71acf7dc2b78ee5e9": {
    "title": "Practical recommendations for gradient-based training of deep architectures",
    "abstract": "Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyperparameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on backpropagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.",
    "paper_id": "55b81991fbb025038d98e8c71acf7dc2b78ee5e9"
  },
  "56d778de2f20bc5288efbb53c6f2eabc9139afc3": {
    "title": "A Planar 4 . 5-GHz DC \u2013 DC Power Converter",
    "abstract": "In this paper, we present two dc\u2013dc converters that operate at a microwave frequency. The first converter consists of a Class-E switched-mode microwave amplifier, which performs the dc\u2013ac conversion, and two half-wave diode rectifier outputs. The Class-E MESFET amplifier has a maximum power-added efficiency of 86%, corresponding drain efficiency of 95%, and 120 mW of output power at 4.5 GHz. The diode rectifier has a maximum conversion efficiency of 98% and an overall efficiency of 83%. The second converter consists of a high-efficiency Class-E oscillator and a diode rectifier. The Class-E oscillator has a maximum efficiency of 57% and maximum output power of 725 mW. The dc\u2013dc converter is planar and compact, with no magnetic components, and with a maximum overall dc\u2013dc conversion efficiency of 64% for a dc input of 3 V, and the output voltage across a 87load of 2.15 V.",
    "paper_id": "56d778de2f20bc5288efbb53c6f2eabc9139afc3"
  },
  "56faf7e024ac63838a6a9f8e71b42980fbfb28e3": {
    "title": "Automatic scoring of pronunciation quality",
    "abstract": "We present a paradigm for the automatic assessment of pronunciation quality by machine. In this scoring paradigm, both native and nonnative speech data is collected and a database of human-expert ratings is created to enable the development of a variety of machine scores. We \u00aerst discuss issues related to the design of speech databases and the reliability of human ratings. We then address pronunciation evaluation as a prediction problem, trying to predict the grade a human expert would assign to a particular skill. Using the speech and the expert-ratings databases, we build statistical models and introduce di\u0080erent machine scores that can be used as predictor variables. We validate these machine scores on the Voice Interactive Language Training System (VILTS) corpus, evaluating the pronunciation of American speakers speaking French and we show that certain machine scores, like the log-posterior and the normalized duration, achieve a correlation with the targeted human grades that is comparable to the human-to-human correlation when a su\u0081cient amount of speech data is available. \u00d3 2000 Elsevier Science B.V. All rights reserved.",
    "paper_id": "56faf7e024ac63838a6a9f8e71b42980fbfb28e3"
  },
  "56ffece2817a0363f551210733a611830ba1155d": {
    "title": "Aligning where to see and what to tell: image caption with region-based attention and scene factorization",
    "abstract": "Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of \u201cabstract meaning\u201d, encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.",
    "paper_id": "56ffece2817a0363f551210733a611830ba1155d"
  },
  "57bbbfea63019a57ef658a27622c357978400a50": {
    "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
    "abstract": null,
    "paper_id": "57bbbfea63019a57ef658a27622c357978400a50"
  },
  "582ea307db25c5764e7d2ed82c4846757f4e95d7": {
    "title": "Greedy Function Approximation : A Gradient Boosting Machine",
    "abstract": "Function approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest{descent minimization. A general gradient{descent \\boosting\" paradigm is developed for additive expansions based on any tting criterion. Speci c algorithms are presented for least{squares, least{absolute{deviation, and Huber{M loss functions for regression, and multi{class logistic likelihood for classi cation. Special enhancements are derived for the particular case where the individual additive components are decision trees, and tools for interpreting such \\TreeBoost\" models are presented. Gradient boosting of decision trees produces competitive, highly robust, interpretable procedures for regression and classi cation, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire 1996, and Friedman, Hastie, and Tibshirani 1998 are discussed. 1 Function estimation In the function estimation problem one has a system consisting of a random \\output\" or \\response\" variable y and a set of random \\input\" or \\explanatory\" variables x = fx1; ; xng. Given a \\training\" sample fyi;xig N 1 of known (y;x){values, the goal is to nd a function F (x) that maps x to y, such that over the joint distribution of all (y;x){values, the expected value of some speci ed loss function (y; F (x)) is minimized F (x) = argmin F (x) Ey;x (y; F (x)) = argmin F (x) Ex [Ey( (y; F (x)) jx] : (1) Frequently employed loss functions (y; F ) include squared{error (y F ) and absolute error jy F j for y 2 R (regression), and negative binomial log{likelihood, log(1 + e 2yF ), when y 2 f 1; 1g (classi cation). A common procedure is to take F (x) to be a member of a parameterized class of functions F (x;P), where P = fP1; P2; g is a set of parameters. In this paper we focus on \\additive\" expansions of the form",
    "paper_id": "582ea307db25c5764e7d2ed82c4846757f4e95d7"
  },
  "5859e8293f8b69e9a80905c6514dabe4064024ed": {
    "title": "Failure to Launch : Critical Mass in Platform Businesses",
    "abstract": "Platform businesses add value by facilitating interactions between customers who are attracted in part by network externalities. Two-sided platform businesses with low costs of reversing participation status have become more important with the rise of the internet. This essay is concerned with new businesses of this sort and with the initial critical mass hurdle that they generally seem to face. In a very general model, we show how this hurdle depends on the nature of network effects, the dynamics of customer behavior, and the distribution of customer tastes. Weak, plausible assumptions about adjustment processes imply that platforms must get a sufficient number of members of both sides on board to launch successfully.",
    "paper_id": "5859e8293f8b69e9a80905c6514dabe4064024ed"
  },
  "586efcb57828d1d68c3d75a5598fa902d3efcea1": {
    "title": "Multi-document Summarization via Budgeted Maximization of Submodular Functions",
    "abstract": "We treat the text summarization problem as maximizing a submodular function under a budget constraint. We show, both theoretically and empirically, a modified greedy algorithm can efficiently solve the budgeted submodular maximization problem near-optimally, and we derive new approximation bounds in doing so. Experiments on DUC\u201904 task show that our approach is superior to the bestperforming method from the DUC\u201904 evaluation on ROUGE-1 scores.",
    "paper_id": "586efcb57828d1d68c3d75a5598fa902d3efcea1"
  },
  "589b8659007e1124f765a5d1bd940b2bf4d79054": {
    "title": "Projection Pursuit Regression",
    "abstract": "Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use.",
    "paper_id": "589b8659007e1124f765a5d1bd940b2bf4d79054"
  },
  "59c51fddb09f899f298d45a32cccb9760b8465c1": {
    "title": "Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition",
    "abstract": "The use of Deep Belief Networks (DBN) to pretrain Neural Networks has recently led to a resurgence in the use of Artificial Neural Network Hidden Markov Model (ANN/HMM) hybrid systems for Automatic Speech Recognition (ASR). In this paper we report results of a DBN-pretrained context-dependent ANN/HMM system trained on two datasets that are much larger than any reported previously with DBN-pretrained ANN/HMM systems 5870 hours of Voice Search and 1400 hours of YouTube data. On the first dataset, the pretrained ANN/HMM system outperforms the best Gaussian Mixture Model Hidden Markov Model (GMM/HMM) baseline, built with a much larger dataset by 3.7% absolute WER, while on the second dataset, it outperforms the GMM/HMM baseline by 4.7% absolute. Maximum Mutual Information (MMI) fine tuning and model combination using Segmental Conditional Random Fields (SCARF) give additional gains of 0.1% and 0.4% on the first dataset and 0.5% and 0.9% absolute on the second dataset.",
    "paper_id": "59c51fddb09f899f298d45a32cccb9760b8465c1"
  },
  "5a3800bee147ad58ab7d6c55d8a2be484c17a511": {
    "title": "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge",
    "abstract": "In this paper we propose the construction of linguistic descriptions of images. This is achieved through the extraction of scene description graphs (SDGs) from visual scenes using an automatically constructed knowledge base. SDGs are constructed using both vision and reasoning. Specifically, commonsense reasoning1 is applied on (a) detections obtained from existing perception methods on given images, (b) a \u201ccommonsense\u201d knowledge base constructed using natural language processing of image annotations and (c) lexical ontological knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most cases, sentences auto-constructed from SDGs obtained by our method give a more relevant and thorough description of an image than a recent state-of-the-art image caption based approach. Our Image-Sentence Alignment Evaluation results are also comparable to that of the recent state-of-the art approaches.",
    "paper_id": "5a3800bee147ad58ab7d6c55d8a2be484c17a511"
  },
  "5c15b11610d7c3ee8d6d99846c276795c072eec3": {
    "title": "Generalizing Data to Provide Anonymity when Disclosing Information (Abstract)",
    "abstract": "The proliferation of information on the Internet and access to fast computers with large storage capacities has increased the volume of information collected and disseminated about individuals. The existence os these other data sources makes it much easier to re-identify individuals whose private information is released in data believed to be anonymous. At the same time, increasing demands are made on organizations to release individualized data rather than aggregate statistical information. Even when explicit identi ers, such as name and phone number, are removed or encrypted when releasing individualized data, other characteristic data, which we term quasi-identi ers, can exist which allow the data recipient to re-identify individuals to whom the data refer. In this paper, we provide a computational disclosure technique for releasing information from a private table such that the identity of any individual to whom the released data refer cannot be de nitively recognized. Our approach protects against linking to other data. It is based on the concepts of generalization, by which stored values can be replaced with semantically consistent and truthful but less precise alternatives, and of k-anonymity . A table is said to provide k-anonymity when the contained data do not allow the recipient to associate the released information to a set of individuals smaller than k. We introduce the notions of generalized table and of minimal generalization of a table with respect to a k-anonymity requirement. As an optimization problem, the objective is to minimally distort the data while providing adequate protection. We describe an algorithm that, given a table, e ciently computes a preferred minimal generalization to provide anonymity.",
    "paper_id": "5c15b11610d7c3ee8d6d99846c276795c072eec3"
  },
  "5c63065770a03ebaf37385586579402343408f0a": {
    "title": "Direct Optimization of Ranking Measures",
    "abstract": "Web page ranking and collaborative filtering require the optimization of sophisticated performance measures. Current Support Vector approaches are unable to optimize them directly and focus on pairwise comparisons instead. We present a new approach which allows direct optimization of the relevant loss functions. This is achieved via structured estimation in Hilbert spaces. It is most related to MaxMargin-Markov networks optimization of multivariate performance measures. Key to our approach is that during training the ranking problem can be viewed as a linear assignment problem, which can be solved by the Hungarian Marriage algorithm. At test time, a sort operation is sufficient, as our algorithm assigns a relevance score to every (document, query) pair. Experiments show that the our algorithm is fast and that it works very well.",
    "paper_id": "5c63065770a03ebaf37385586579402343408f0a"
  },
  "5ca4abab527f6b0270e50548f0dea30638c9b86e": {
    "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
    "abstract": "Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. Deep learning methods have realized impressive performance in a range of applications, from visual object classification [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have been achieved despite the noted difficulty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many explanations for the difficulty of deep learning have been advanced in the literature, including the presence of many local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay of back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed 1 ar X iv :1 31 2. 61 20 v3 [ cs .N E ] 1 9 Fe b 20 14 strikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by almost stage-like transitions to better performance. However, a quantitative, analytical understanding of the rich dynamics of deep learning remains elusive. For example, what determines the time scales over which deep learning unfolds? How does training speed retard with depth? Under what conditions will greedy unsupervised pretraining speed up learning? And how do the final learned internal representations depend on the statistical regularities inherent in the training data? Here we provide an exact analytical theory of learning in deep linear neural networks that quantitatively answers these questions for this restricted setting. Because of its linearity, the input-output map of a deep linear network can always be rewritten as a shallow network. In this sense, a linear network does not gain expressive power from depth, and hence will underfit and perform poorly on complex real world problems. But while it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless exhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the training error, as a function of the network weights, is non-convex, and gradient descent dynamics on this non-convex error surface exhibits a subtle interplay between different weights across multiple layers of the network. Hence deep linear networks provide an important starting point for understanding deep learning dynamics. To answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing learning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We find exact time-dependent solutions to these nonlinear equations, as well as find conserved quantities in the weight dynamics arising from symmetries in the error function. These solutions provide intuition into how a deep network successively builds up information about the statistical structure of the training data and embeds this information into its weights and internal representations. Moreover, we compare our analytical solutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in deep non-linear networks, and find that our analytical solutions provide a reasonable approximation. Our solutions also reflect nonlinear phenomena seen in simulations, including alternating plateaus and sharp periods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics in deep linear networks is sufficient to qualitatively capture aspects of the progressive, hierarchical differentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate the commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover conditions under which such pretraining speeds learning. We show that these conditions are approximately satisfied for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advantage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial conditions on weights that, in linear networks, provide depth independent learning times, and we show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic gains are tuned so that linear amplification due to propagation of neural activity through weight matrices exactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in nonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error signals act like near isometries. 1 General learning dynamics of gradient descent W 21 W 32 x \u2208 R1 h \u2208 R2 y \u2208 R3 Figure 1: The three layer network analyzed in this section. We begin by analyzing learning in a three layer network (input, hidden, and output) with linear activation functions (Fig 1). We letNi be the number of neurons in layer i. The inputoutput map of the network is y = W W x. We wish to train the network to learn a particular input-output map from",
    "paper_id": "5ca4abab527f6b0270e50548f0dea30638c9b86e"
  },
  "5d2d797ee4053dada784639d7462abbfb2220031": {
    "title": "Guided Open Vocabulary Image Captioning with Constrained Beam Search",
    "abstract": "Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.",
    "paper_id": "5d2d797ee4053dada784639d7462abbfb2220031"
  },
  "5e0f8c355a37a5a89351c02f174e7a5ddcb98683": {
    "title": "Microsoft COCO: Common Objects in Context",
    "abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
    "paper_id": "5e0f8c355a37a5a89351c02f174e7a5ddcb98683"
  },
  "5e3d252e0ead27ace37868184eed2cb2d48b0c20": {
    "title": "Can the Market Multiply and Divide ? Non-Proportional Thinking in Financial Markets",
    "abstract": "When pricing nancial assets, rational agents should think in terms of proportional price changes, i.e., returns. However, stock price movements are often reported and discussed in dollar rather than percentage units, which may cause investors to think that news should correspond to a dollar change in price rather than a percentage change in price. Non-proportional thinking in nancial markets can lead to return underreaction for high-priced stocks and overreaction for low-priced stocks. Consistent with a simple model of non-proportional thinking, we nd that total volatility, idiosyncratic volatility, and market beta are signi cantly higher for stocks with low share prices, controlling for size. To identify a causal e ect of price, we show that volatility increases sharply following stock splits and drops following reverse stock splits. The economic magnitudes are large: non-proportional thinking can explain the leverage e ect puzzle, in which volatility is negatively related to past returns, as well as the volatility-size and beta-size relations in the data. We also show that low-priced stocks drive the long-run reversal phenomenon in asset pricing, and the magnitude of reversals can be sorted by price, holding past returns and size constant. Finally, we show that non-proportional thinking biases reactions to news that is itself reported in nominal-per-share units rather than the appropriate scaled units. Investors react to nominal earnings per share surprises, after controlling for the earnings surprise scaled by share price. The reaction to the nominal earnings surprise reverses in the long run, consistent with correction of mispricing. \u2217Kelly Shue: Yale University and NBER, kelly.shue@yale.edu. Richard Townsend: University of California San Diego, rrtownsend@ucsd.edu. We thank Huijun Sun, Kaushik Vasudevan, and Tianhao Wu for excellent research assistance and the International Center for Finance at the Yale School of Management for their support. We thank John Campbell, James Choi, Sam Hartzmark, Bryan Kelly, Toby Moskowitz, Andrei Shleifer, and Stefano Giglio for helpful comments.",
    "paper_id": "5e3d252e0ead27ace37868184eed2cb2d48b0c20"
  },
  "5eae830442c129596ab7d9f5316f1aad2e8c178f": {
    "title": "Platform Competition in Two-Sided Markets",
    "abstract": "Many if not most markets with network externalities are two-sided. To succeed, platforms in industries such as software, portals and media, payment systems and the Internet, must \u201cget both sides of the market on board \u201d. Accordingly, platforms devote much attention to their business model, that is to how they court each side while making money overall. The paper builds a model of platform competition with two-sided markets. It unveils the determinants of price allocation and enduser surplus for different governance structures (profit-maximizing platforms and not-for-profit joint undertakings), and compares the outcomes with those under an integrated monopolist and a Ramsey planner.",
    "paper_id": "5eae830442c129596ab7d9f5316f1aad2e8c178f"
  },
  "6074c1108997e0c1f97dc3c199323a162ffe978d": {
    "title": "Torch7: A Matlab-like Environment for Machine Learning",
    "abstract": "Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua\u2019s light interface.",
    "paper_id": "6074c1108997e0c1f97dc3c199323a162ffe978d"
  },
  "60f4f98ff57be60a786803a88f5e7e970b35c79e": {
    "title": "Re-evaluation the Role of Bleu in Machine Translation Research",
    "abstract": "We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric. We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu\u2019s correlation with human judgments of quality. This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.",
    "paper_id": "60f4f98ff57be60a786803a88f5e7e970b35c79e"
  },
  "61e9706e1915e1bac8e9c5283ccaab9fb2cc54d0": {
    "title": "Explain Images with Multimodal Recurrent Neural Networks",
    "abstract": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12 [9], Flickr 8K [31], Flickr 30K [14] and MS COCO [23]). Our model outperforms the state-of-the-art generative method. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.",
    "paper_id": "61e9706e1915e1bac8e9c5283ccaab9fb2cc54d0"
  },
  "62bbdc47b7f87eecf2e1b1ddf1d980f7cb27c4c6": {
    "title": "Grit: perseverance and passion for long-term goals.",
    "abstract": "The importance of intellectual talent to achievement in all professional domains is well established, but less is known about other individual differences that predict success. The authors tested the importance of 1 noncognitive trait: grit. Defined as perseverance and passion for long-term goals, grit accounted for an average of 4% of the variance in success outcomes, including educational attainment among 2 samples of adults (N=1,545 and N=690), grade point average among Ivy League undergraduates (N=138), retention in 2 classes of United States Military Academy, West Point, cadets (N=1,218 and N=1,308), and ranking in the National Spelling Bee (N=175). Grit did not relate positively to IQ but was highly correlated with Big Five Conscientiousness. Grit nonetheless demonstrated incremental predictive validity of success measures over and beyond IQ and conscientiousness. Collectively, these findings suggest that the achievement of difficult goals entails not only talent but also the sustained and focused application of talent over time.",
    "paper_id": "62bbdc47b7f87eecf2e1b1ddf1d980f7cb27c4c6"
  },
  "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7": {
    "title": "The Active Badge Location System",
    "abstract": "A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed.",
    "paper_id": "62edb6639dc857ad0f33e5d8ef97af89be7a3bc7"
  },
  "64b3435826a94ddd269b330e6254579f3244f214": {
    "title": "Matrix computations (3. ed.)",
    "abstract": null,
    "paper_id": "64b3435826a94ddd269b330e6254579f3244f214"
  },
  "64da1980714cfc130632c5b92b9d98c2f6763de6": {
    "title": "On rectified linear units for speech processing",
    "abstract": "Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",
    "paper_id": "64da1980714cfc130632c5b92b9d98c2f6763de6"
  },
  "659fc2a483a97dafb8fb110d08369652bbb759f9": {
    "title": "Improving the Fisher Kernel for Large-Scale Image Classification",
    "abstract": "The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9% to 58.3%. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained using only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant resources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.",
    "paper_id": "659fc2a483a97dafb8fb110d08369652bbb759f9"
  },
  "6772164c3dd4ff6e71ba58c5c4c22fa092b9fe55": {
    "title": "Recent advances in deep learning for speech research at Microsoft",
    "abstract": "Deep learning is becoming a mainstream technology for speech recognition at industrial scale. In this paper, we provide an overview of the work by Microsoft speech researchers since 2009 in this area, focusing on more recent advances which shed light to the basic capabilities and limitations of the current deep learning technology. We organize this overview along the feature-domain and model-domain dimensions according to the conventional approach to analyzing speech systems. Selected experimental results, including speech recognition and related applications such as spoken dialogue and language modeling, are presented to demonstrate and analyze the strengths and weaknesses of the techniques described in the paper. Potential improvement of these techniques and future research directions are discussed.",
    "paper_id": "6772164c3dd4ff6e71ba58c5c4c22fa092b9fe55"
  },
  "67ad45d20bd17c5f1be45f9f2029f815c9873379": {
    "title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
    "abstract": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already stateof-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.",
    "paper_id": "67ad45d20bd17c5f1be45f9f2029f815c9873379"
  },
  "67d9fe18e62d5a5df1414375577721557febc0dc": {
    "title": "High-efficiency harmonically-terminated rectifier for wireless powering applications",
    "abstract": "In wireless powering, the rectifier efficiency has a large effect on overall system efficiency. This paper presents an approach to high-efficiency microwave rectifier design based on reduced conduction angle power amplifier theory. The analysis for an ideal rectifying device is derived to predict efficiency dependence on optimal dc load. A class-C 2.45 GHz Schottky-diode rectifier with short-circuit 2nd and 3rd harmonic terminations is designed using source-pull measurements, and demonstrates a maximum RF-DC conversion efficiency of 72.8% when matched to 50\u2126. The approach is applied to integration of a rectifier with a dual-polarization patch antenna in a non 50\u2126 environment and free-space measurements demonstrate a lower bound on efficiency of 56% at 150 \u00b5W\\cm2 power density which includes matching circuit and mismatch losses.",
    "paper_id": "67d9fe18e62d5a5df1414375577721557febc0dc"
  },
  "6816c447cc4d3d945e0452564ff5d3220e1fdcab": {
    "title": "Viewstamped Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems",
    "abstract": "One of the potential benefits of distributed systems is their use in providing highly-available services that are likely to be usable when needed. Availabilay is achieved through replication. By having inore than one copy of information, a service continues to be usable even when some copies are inaccessible, for example, because of a crash of the computer where a copy was stored. This paper presents a new replication algorithm that has desirable performance properties. Our approach is based on the primary copy technique. Computations run at a primary. which notifies its backups of what it has done. If the primary crashes, the backups are reorganized, and one of the backups becomes the new primary. Our method works in a general network with both node crashes and partitions. Replication causes little delay in user computations and little information is lost in a reorganization; we use a special kind of timestamp called a viewstamp to detect lost information.",
    "paper_id": "6816c447cc4d3d945e0452564ff5d3220e1fdcab"
  },
  "6834913a76b686957c0b8c755d1ca6ef3bd76914": {
    "title": "Data privacy through optimal k-anonymization",
    "abstract": "Data de-identification reconciles the demand for release of data for research purposes and the demand for privacy from individuals. This paper proposes and evaluates an optimization algorithm for the powerful de-identification procedure known as k-anonymization. A k-anonymized dataset has the property that each record is indistinguishable from at least k - 1 others. Even simple restrictions of optimized k-anonymity are NP-hard, leading to significant computational challenges. We present a new approach to exploring the space of possible anonymizations that tames the combinatorics of the problem, and develop data-management strategies to reduce reliance on expensive operations such as sorting. Through experiments on real census data, we show the resulting algorithm can find optimal k-anonymizations under two representative cost measures and a wide range of k. We also show that the algorithm can produce good anonymizations in circumstances where the input data or input parameters preclude finding an optimal solution in reasonable time. Finally, we use the algorithm to explore the effects of different coding approaches and problem variations on anonymization quality and performance. To our knowledge, this is the first result demonstrating optimal k-anonymization of a non-trivial dataset under a general model of the problem.",
    "paper_id": "6834913a76b686957c0b8c755d1ca6ef3bd76914"
  },
  "696ca58d93f6404fea0fc75c62d1d7b378f47628": {
    "title": "Microsoft COCO Captions: Data Collection and Evaluation Server",
    "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.",
    "paper_id": "696ca58d93f6404fea0fc75c62d1d7b378f47628"
  },
  "6b142540de804f216b6a9a7c1cb4625de4f360f8": {
    "title": "Predicting relevant news events for timeline summaries",
    "abstract": "This paper presents a framework for automatically constructing timeline summaries from collections of web news articles. We also evaluate our solution against manually created timelines and in comparison with related work.",
    "paper_id": "6b142540de804f216b6a9a7c1cb4625de4f360f8"
  },
  "6b204e084cfb74d9ef4bb504aa4133807a198613": {
    "title": "TextRank: Bringing Order Into Texts",
    "abstract": "In this paper, we introduce TextRank \u2013 a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",
    "paper_id": "6b204e084cfb74d9ef4bb504aa4133807a198613"
  },
  "6b2b7c5efae0bd7be3156323cad75e10fe83ed66": {
    "title": "Cascaded Classification Models: Combining Models for Holistic Scene Understanding",
    "abstract": "One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difficult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classification Models ( CCM), where repeated instantiations of these classifiers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited \u201cblack box\u201d interface with the models, allowing us to use very sophisticated, state-of-the-art classifiers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction.",
    "paper_id": "6b2b7c5efae0bd7be3156323cad75e10fe83ed66"
  },
  "6b40f545d3d3f18b62112ba8d0e51feaad12d16a": {
    "title": "Multimodal Neural Language Models",
    "abstract": "We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. A multimodal neural language model can be used to retrieve images given complex description queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike most existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.",
    "paper_id": "6b40f545d3d3f18b62112ba8d0e51feaad12d16a"
  },
  "6b570069f14c7588e066f7138e1f21af59d62e61": {
    "title": "Theano: A Python framework for fast computation of mathematical expressions",
    "abstract": "Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.",
    "paper_id": "6b570069f14c7588e066f7138e1f21af59d62e61"
  },
  "6b6ae4ff053bcee2834b5e7718810cb5bc15c36c": {
    "title": "Random graph models of social networks.",
    "abstract": "We describe some new exactly solvable models of the structure of social networks, based on random graphs with arbitrary degree distributions. We give models both for simple unipartite networks, such as acquaintance networks, and bipartite networks, such as affiliation networks. We compare the predictions of our models to data for a number of real-world social networks and find that in some cases, the models are in remarkable agreement with the data, whereas in others the agreement is poorer, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.",
    "paper_id": "6b6ae4ff053bcee2834b5e7718810cb5bc15c36c"
  },
  "6bba847f25124aea6e14231f13f63a3f0a4f9c33": {
    "title": "The battle against phishing: Dynamic Security Skins",
    "abstract": "Phishing is a model problem for illustrating usability concerns of privacy and security because both system designers and attackers battle using user interfaces to guide (or misguide) users.We propose a new scheme, Dynamic Security Skins, that allows a remote web server to prove its identity in a way that is easy for a human user to verify and hard for an attacker to spoof. We describe the design of an extension to the Mozilla Firefox browser that implements this scheme.We present two novel interaction techniques to prevent spoofing. First, our browser extension provides a trusted window in the browser dedicated to username and password entry. We use a photographic image to create a trusted path between the user and this window to prevent spoofing of the window and of the text entry fields.Second, our scheme allows the remote server to generate a unique abstract image for each user and each transaction. This image creates a \"skin\" that automatically customizes the browser window or the user interface elements in the content of a remote web page. Our extension allows the user's browser to independently compute the image that it expects to receive from the server. To authenticate content from the server, the user can visually verify that the images match.We contrast our work with existing anti-phishing proposals. In contrast to other proposals, our scheme places a very low burden on the user in terms of effort, memory and time. To authenticate himself, the user has to recognize only one image and remember one low entropy password, no matter how many servers he wishes to interact with. To authenticate content from an authenticated server, the user only needs to perform one visual matching operation to compare two images. Furthermore, it places a high burden of effort on an attacker to spoof customized security indicators.",
    "paper_id": "6bba847f25124aea6e14231f13f63a3f0a4f9c33"
  },
  "6c5337a1deea7a8588493f1ebfa01e00107ce037": {
    "title": "Cyberbullying: its nature and impact in secondary school pupils.",
    "abstract": "BACKGROUND\nCyberbullying describes bullying using mobile phones and the internet. Most previous studies have focused on the prevalence of text message and email bullying.\n\n\nMETHODS\nTwo surveys with pupils aged 11-16 years: (1) 92 pupils from 14 schools, supplemented by focus groups; (2) 533 pupils from 5 schools, to assess the generalisability of findings from the first study, and investigate relationships of cyberbullying to general internet use. Both studies differentiated cyberbullying inside and outside of school, and 7 media of cyberbullying.\n\n\nRESULTS\nBoth studies found cyberbullying less frequent than traditional bullying, but appreciable, and reported more outside of school than inside. Phone call and text message bullying were most prevalent, with instant messaging bullying in the second study; their impact was perceived as comparable to traditional bullying. Mobile phone/video clip bullying, while rarer, was perceived to have more negative impact. Age and gender differences varied between the two studies. Study 1 found that most cyberbullying was done by one or a few students, usually from the same year group. It often just lasted about a week, but sometimes much longer. The second study found that being a cybervictim, but not a cyberbully, correlated with internet use; many cybervictims were traditional 'bully-victims'. Pupils recommended blocking/avoiding messages, and telling someone, as the best coping strategies; but many cybervictims had told nobody about it.\n\n\nCONCLUSIONS\nCyberbullying is an important new kind of bullying, with some different characteristics from traditional bullying. Much happens outside school. Implications for research and practical action are discussed.",
    "paper_id": "6c5337a1deea7a8588493f1ebfa01e00107ce037"
  },
  "6d8c9fcce8177d6f8d122d653c7d32d7624d6714": {
    "title": "The WEKA data mining software: an update",
    "abstract": "More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.",
    "paper_id": "6d8c9fcce8177d6f8d122d653c7d32d7624d6714"
  },
  "6dc61f37ecc552413606d8c89ffbc46ec98ed887": {
    "title": "ACCELERATION OF STOCHASTIC APPROXIMATION BY AVERAGING",
    "abstract": null,
    "paper_id": "6dc61f37ecc552413606d8c89ffbc46ec98ed887"
  },
  "6e6f47c4b2109e7824cd475336c3676faf9b113e": {
    "title": "Baby Talk : Understanding and Generating Image Descriptions",
    "abstract": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.",
    "paper_id": "6e6f47c4b2109e7824cd475336c3676faf9b113e"
  },
  "6ef3da9d9c5a7e2f96a1470bad413c857f64aa95": {
    "title": "A Pattern Language - Towns, Buildings, Construction",
    "abstract": "When there are many people who don't need to expect something more than the benefits to take, we will suggest you to have willing to reach all benefits. Be sure and surely do to take this a pattern language towns buildings construction that gives the best reasons to read. When you really need to get the reason why, this a pattern language towns buildings construction book will probably make you feel curious.",
    "paper_id": "6ef3da9d9c5a7e2f96a1470bad413c857f64aa95"
  },
  "6f20506ce955b7f82f587a14301213c08e79463b": {
    "title": "Algorithms for Inverse Reinforcement Learning",
    "abstract": null,
    "paper_id": "6f20506ce955b7f82f587a14301213c08e79463b"
  },
  "6f3ffb1a7b6cb168caeb81a23b68bbf99fdab052": {
    "title": "A circularly polarized short backfire antenna excited by an unbalance-fed cross aperture",
    "abstract": "An unbalance-fed cross aperture is developed to excite a short backfire antenna (SBA) for circular polarization. The cross aperture consists of two orthogonal H-shaped slots with a pair of capacitive stubs and is fed by a single probe that forms an unbalanced feed with a shorting pin. It is demonstrated that the cross-aperture-excited SBA can achieve an axial ratio (les 3 dB) bandwidth of 4.2% with a voltage standing wave ratio (VSWR) bandwidth of 6.5% (VSWR<1.2) and a gain of 14 dBi. The antenna structure is described and the simulation and experimental results are presented. The mechanisms for impedance matching and circular-polarization production are analyzed",
    "paper_id": "6f3ffb1a7b6cb168caeb81a23b68bbf99fdab052"
  },
  "6f568d757d2c1ab42f2006faa25690b74c3d2d44": {
    "title": "The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization",
    "abstract": "While vector quantization (VQ) has been applied widely to generate features for visual recognition problems, much recent work has focused on more powerful methods. In particular, sparse coding has emerged as a strong alternative to traditional VQ approaches and has been shown to achieve consistently higher performance on benchmark datasets. Both approaches can be split into a training phase, where the system learns a dictionary of basis functions, and an encoding phase, where the dictionary is used to extract features from new inputs. In this work, we investigate the reasons for the success of sparse coding over VQ by decoupling these phases, allowing us to separate out the contributions of training and encoding in a controlled way. Through extensive experiments on CIFAR, NORB and Caltech 101 datasets, we compare several training and encoding schemes, including sparse coding and a form of VQ with a soft threshold activation function. Our results show not only that we can use fast VQ algorithms for training, but that we can just as well use randomly chosen exemplars from the training set. Rather than spend resources on training, we find it is more important to choose a good encoder\u2014which can often be a simple feed forward non-linearity. Our results include state-of-the-art performance on both CIFAR and NORB.",
    "paper_id": "6f568d757d2c1ab42f2006faa25690b74c3d2d44"
  },
  "70cb232a6e391bfa49b0441a9956820c52ec32f2": {
    "title": "Evaluating Content Selection in Summarization: The Pyramid Method",
    "abstract": "We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.",
    "paper_id": "70cb232a6e391bfa49b0441a9956820c52ec32f2"
  },
  "722fcc35def20cfcca3ada76c8dd7a585d6de386": {
    "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
    "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.\n Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",
    "paper_id": "722fcc35def20cfcca3ada76c8dd7a585d6de386"
  },
  "72e08cf12730135c5ccd7234036e04536218b6c1": {
    "title": "An extended set of Haar-like features for rapid object detection",
    "abstract": "Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10% lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3% on the CMU face set [7].",
    "paper_id": "72e08cf12730135c5ccd7234036e04536218b6c1"
  },
  "7309a56c14f9f2737dbc7d45e106a65c205b3dcb": {
    "title": "Fast High-Dimensional Filtering Using the Permutohedral Lattice",
    "abstract": "Many useful algorithms for processing images and geometry fall under the general framework of high-dimensional Gaussian filtering. This family of algorithms includes bilateral filtering and non-local means. We propose a new way to perform such filters using the permutohedral lattice, which tessellates high-dimensional space with uniform simplices. Our algorithm is the first implementation of a high-dimensional Gaussian filter that is both linear in input size and polynomial in dimensionality. Furthermore it is parameter-free, apart from the filter size, and achieves a consistently high accuracy relative to ground truth (> 45 dB). We use this to demonstrate a number of interactive-rate applications of filters in as high as eight dimensions.",
    "paper_id": "7309a56c14f9f2737dbc7d45e106a65c205b3dcb"
  },
  "735d4220d5579cc6afe956d9f6ea501a96ae99e2": {
    "title": "On the momentum term in gradient descent learning algorithms",
    "abstract": "A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.",
    "paper_id": "735d4220d5579cc6afe956d9f6ea501a96ae99e2"
  },
  "743c961f5e529917d532798ca910508588c513e6": {
    "title": "A feasibility study on a voltage-doubler-type rectenna",
    "abstract": "We propose a voltage-doubler-type rectenna as a simple, compact and lightweight rectenna. As a feasibility study, we evaluate a 2.45 GHz voltage-doubler-type rectenna, in which Schottky diodes are mounted both in series and in shunt at the feed point of a half-wave dipole antenna, from measurements and circuit simulations. The measured rf-dc conversion efficiency of the prototype rectenna was 47.4 % at the output load of 1.5 k\u03a9 and the receiving power density of 1 mW/cm2. From circuit simulations, the maximum efficiency of 63.5 % was obtained at 600 \u03a9 under the assumed receiving power density of 1.6 mW/cm2.",
    "paper_id": "743c961f5e529917d532798ca910508588c513e6"
  },
  "74511d4c8ceba09193e87102b824126ec4783f6c": {
    "title": "Practical Byzantine Fault Tolerance",
    "abstract": "Our growing reliance on online services accessible on the Internet demands highly-available systems that provide correct service without interruptions. Byzantine faults such as software bugs, operator mistakes, and malicious attacks are the major cause of service interruptions. This thesis describes a new replication algorithm, BFT, that can be used to build highly-available systems that tolerate Byzantine faults. It shows, for the first time, how to build Byzantine-fault-tolerant systems that can be used in practice to implement real services because they do not rely on unrealistic assumptions and they perform well. BFT works in asynchronous environments like the Internet, it incorporates mechanisms to defend against Byzantine-faulty clients, and it recovers replicas proactively. The recovery mechanism allows the algorithm to tolerate any number of faults over the lifetime of the system provided fewer than 1/3 of the replicas become faulty within a small window of vulnerability. The window may increase under a denial-of-service attack but the algorithm can detect and respond to such attacks and it can also detect when the state of a replica is corrupted by an attacker. BFT has been implemented as a generic program library with a simple interface. The BFT library provides a complete solution to the problem of building real services that tolerate Byzantine faults. We used the library to implement the first Byzantine-fault-tolerant NFS file system, BFS. The BFT library and BFS perform well because the library incorporates several important optimizations. The most important optimization is the use of symmetric cryptography to authenticate messages. Public-key cryptography, which was the major bottleneck in previous systems, is used only to exchange the symmetric keys. The performance results show that BFS performs 2% faster to 24% slower than production implementations of the NFS protocol that are not replicated. Therefore, we believe that the BFT library can be used to build practical systems that tolerate Byzantine faults. Thesis Supervisor: Barbara H. Liskov Title: Ford Professor of Engineering",
    "paper_id": "74511d4c8ceba09193e87102b824126ec4783f6c"
  },
  "74875368649f52f74bfc4355689b85a724c3db47": {
    "title": "Object detection by labeling superpixels",
    "abstract": "Object detection is often conducted by object proposal generation and classification sequentially. This paper handles object detection in a superpixel oriented manner instead of the proposal oriented. Specially, this paper takes object detection as a multi-label superpixel labeling problem by minimizing an energy function. It uses the data cost term to capture the appearance, smooth cost term to encode the spatial context and label cost term to favor compact detection. The data cost is learned through a convolutional neural network and the parameters in the labeling model are learned through a structural SVM. Compared with proposal generation and classification based methods, the proposed superpixel labeling method can naturally detect objects missed by proposal generation step and capture the global image context to infer the overlapping objects. The proposed method shows its advantage in Pascal VOC and ImageNet. Notably, it performs better than the ImageNet ILSVRC2014 winner GoogLeNet (45.0% V.S. 43.9% in mAP) with much shallower and fewer CNNs.",
    "paper_id": "74875368649f52f74bfc4355689b85a724c3db47"
  },
  "74b57a54fb755d13082c1598756db7cec9866d8b": {
    "title": "Relaxed online SVMs for spam filtering",
    "abstract": "Spam is a key problem in electronic communication, including large-scale email systems and the growing number of blogs. Content-based filtering is one reliable method of combating this threat in its various forms, but some academic researchers and industrial practitioners disagree on how best to filter spam. The former have advocated the use of Support Vector Machines (SVMs) for content-based filtering, as this machine learning methodology gives state-of-the-art performance for text classification. However, similar performance gains have yet to be demonstrated for online spam filtering. Additionally, practitioners cite the high cost of SVMs as reason to prefer faster (if less statistically robust) Bayesian methods. In this paper, we offer a resolution to this controversy. First, we show that online SVMs indeed give state-of-the-art classification performance on online spam filtering on large benchmark data sets. Second, we show that nearly equivalent performance may be achieved by a Relaxed Online SVM (ROSVM) at greatly reduced computational cost. Our results are experimentally verified on email spam, blog spam, and splog detection tasks.",
    "paper_id": "74b57a54fb755d13082c1598756db7cec9866d8b"
  },
  "75290cede0e14fb0f92cafd04d5a211f05d7fde1": {
    "title": "Hidden Topic Markov Models",
    "abstract": "Algorithms such as Latent Dirichlet Allocation (LDA) have achieved significant progress in modeling word document relationships. These algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document. Given these parameters, the topics of all words in the same document are assumed to be independent. In this paper, we propose modeling the topics of words in the document as a Markov chain. Specifically, we assume that all words in the same sentence have the same topic, and successive sentences are more likely to have the same topics. Since the topics are hidden, this leads to using the well-known tools of Hidden Markov Models for learning and inference. We show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics. Quantitatively, we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity.",
    "paper_id": "75290cede0e14fb0f92cafd04d5a211f05d7fde1"
  },
  "756a3901c302ddd1a25a8c76c691396784257add": {
    "title": "A Universal UHF RFID Reader Antenna",
    "abstract": "A broadband circularly polarized patch antenna is proposed for universal ultra-high-frequency (UHF) RF identification (RFID) applications. The antenna is composed of two corner-truncated patches and a suspended microstrip line with open-circuited termination. The main patch is fed by four probes which are sequentially connected to the suspended microstrip feed line. The measurement shows that the antenna achieves a return loss of -15 dB, gain of 8.3 dBic, axial ratio (AR) of 3 dB, and 3-dB AR beamwidth of 75deg over the UHF band of 818-964 MHz or 16.4%. Therefore, the proposed antenna is universal for UHF RFID applications worldwide at the UHF band of 840-960 MHz. In addition, a parametric study is conducted to facilitate the design and optimization processes for engineers.",
    "paper_id": "756a3901c302ddd1a25a8c76c691396784257add"
  },
  "75791dc73b874a7c8fe43c76867c11a76744d394": {
    "title": "Anonymizing Social Networks",
    "abstract": "Advances in technology have made it possible to collect data about individuals and the connections between them, such as email correspondence and friendships. Agencies and researchers who have collected such social network data often have a compelling interest in allowing others to analyze the data. However, in many cases the data describes relationships that are private (e.g., email correspondence) and sharing the data in full can result in unacceptable disclosures. In this paper, we present a framework for assessing the privacy risk of sharing anonymized network data. This includes a model of adversary knowledge, for which we consider several variants and make connections to known graph theoretical results. On several real-world social networks, we show that simple anonymization techniques are inadequate, resulting in substantial breaches of privacy for even modestly informed adversaries. We propose a novel anonymization technique based on perturbing the network and demonstrate empirically that it leads to substantial reduction of the privacy threat. We also analyze the effect that anonymizing the network has on the utility of the data for social network analysis.",
    "paper_id": "75791dc73b874a7c8fe43c76867c11a76744d394"
  },
  "763542e63ab6720759d0b9e78fb17416d25efa06": {
    "title": "ELIZA - a computer program for the study of natural language communication between man and machine",
    "abstract": "ELIZA is a program operating within the MAC time-sharing system at MIT which makes certain kinds of natural language conversation between man and computer possible. Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text. Responses are generated by reassembly rules associated with selected decomposition rules. The fundamental technical problems with which ELIZA is concerned are: (1) the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA \"scripts\". A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper.",
    "paper_id": "763542e63ab6720759d0b9e78fb17416d25efa06"
  },
  "774c6a9405b6d52d5d30a15e82601ca4635369c6": {
    "title": "Some statistical issues in the comparison of speech recognition algorithms",
    "abstract": "In the development of speech recognition algorithms, it is important to know whether any apparent difference in performance of algorithms is statistically significant, yet this issue is almost always overlooked. We present two simple tests for deciding whether the difference in error-rates between two algorithms tested on the same data set is statistically significant. The first (McNemar\u2019s test) requires the errors made by an algorithm to be independent events and is most appropriate for isolated word algorithms. The second (a matched-pairs test) can be used even when errors are not independent events and is more appropriate for connected speech.",
    "paper_id": "774c6a9405b6d52d5d30a15e82601ca4635369c6"
  },
  "79dc84a3bf76f1cb983902e2591d913cee5bdb0e": {
    "title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences",
    "abstract": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.",
    "paper_id": "79dc84a3bf76f1cb983902e2591d913cee5bdb0e"
  },
  "7b7b0b0239072d442e0620a2801c47036ae05251": {
    "title": "Public Goods and Ethnic Divisions",
    "abstract": "We present a model that links heterogeneity of preferences across ethnic groups in a city to the amount and type of public good the city supplies. We test the implications of the model with three related data sets: U. S. cities, U. S. metropolitan areas, and U. S. urban counties. Results show that the shares of spending on productive public goods -education, roads, sewers and trash pickup -in U. S. cities (metro areas/urban counties) are inversely related to the city\u2019s (metro area\u2019s/county\u2019s) ethnic fragmentation, even after controlling for other socioeconomic and demographic determinants. We conclude that ethnic conflict is an important determinant of local public finances.",
    "paper_id": "7b7b0b0239072d442e0620a2801c47036ae05251"
  },
  "7bcc53f1baf3358517a602d856192faea9442c91": {
    "title": "Introduction to Algorithms, Second Edition",
    "abstract": "problems To understand the class of polynomial-time solvable problems, we must first have a formal notion of what a \"problem\" is. We define an abstract problem Q to be a binary relation on a set I of problem instances and a set S of problem solutions. For example, an instance for SHORTEST-PATH is a triple consisting of a graph and two vertices. A solution is a sequence of vertices in the graph, with perhaps the empty sequence denoting that no path exists. The problem SHORTEST-PATH itself is the relation that associates each instance of a graph and two vertices with a shortest path in the graph that connects the two vertices. Since shortest paths are not necessarily unique, a given problem instance may have more than one solution. This formulation of an abstract problem is more general than is required for our purposes. As we saw above, the theory of NP-completeness restricts attention to decision problems: those having a yes/no solution. In this case, we can view an abstract decision problem as a function that maps the instance set I to the solution set {0, 1}. For example, a decision problem related to SHORTEST-PATH is the problem PATH that we saw earlier. If i = G, u, v, k is an instance of the decision problem PATH, then PATH(i) = 1 (yes) if a shortest path from u to v has at most k edges, and PATH(i) = 0 (no) otherwise. Many abstract problems are not decision problems, but rather optimization problems, in which some value must be minimized or maximized. As we saw above, however, it is usually a simple matter to recast an optimization problem as a decision problem that is no harder. Encodings If a computer program is to solve an abstract problem, problem instances must be represented in a way that the program understands. An encoding of a set S of abstract objects is a mapping e from S to the set of binary strings. For example, we are all familiar with encoding the natural numbers N = {0, 1, 2, 3, 4,...} as the strings {0, 1, 10, 11, 100,...}. Using this encoding, e(17) = 10001. Anyone who has looked at computer representations of keyboard characters is familiar with either the ASCII or EBCDIC codes. In the ASCII code, the encoding of A is 1000001. Even a compound object can be encoded as a binary string by combining the representations of its constituent parts. Polygons, graphs, functions, ordered pairs, programs-all can be encoded as binary strings. Thus, a computer algorithm that \"solves\" some abstract decision problem actually takes an encoding of a problem instance as input. We call a problem whose instance set is the set of binary strings a concrete problem. We say that an algorithm solves a concrete problem in time O(T (n)) if, when it is provided a problem instance i of length n = |i|, the algorithm can produce the solution in O(T (n)) time. A concrete problem is polynomial-time solvable, therefore, if there exists an algorithm to solve it in time O(n) for some constant k. We can now formally define the complexity class P as the set of concrete decision problems that are polynomial-time solvable. We can use encodings to map abstract problems to concrete problems. Given an abstract decision problem Q mapping an instance set I to {0, 1}, an encoding e : I \u2192 {0, 1}* can be used to induce a related concrete decision problem, which we denote by e(Q). If the solution to an abstract-problem instance i I is Q(i) {0, 1}, then the solution to the concreteproblem instance e(i) {0, 1}* is also Q(i). As a technicality, there may be some binary strings that represent no meaningful abstract-problem instance. For convenience, we shall assume that any such string is mapped arbitrarily to 0. Thus, the concrete problem produces the same solutions as the abstract problem on binary-string instances that represent the encodings of abstract-problem instances. We would like to extend the definition of polynomial-time solvability from concrete problems to abstract problems by using encodings as the bridge, but we would like the definition to be independent of any particular encoding. That is, the efficiency of solving a problem should not depend on how the problem is encoded. Unfortunately, it depends quite heavily on the encoding. For example, suppose that an integer k is to be provided as the sole input to an algorithm, and suppose that the running time of the algorithm is \u0398(k). If the integer k is provided in unary-a string of k 1's-then the running time of the algorithm is O(n) on length-n inputs, which is polynomial time. If we use the more natural binary representation of the integer k, however, then the input length is n = \u230alg k\u230b + 1. In this case, the running time of the algorithm is \u0398 (k) = \u0398(2), which is exponential in the size of the input. Thus, depending on the encoding, the algorithm runs in either polynomial or superpolynomial time. The encoding of an abstract problem is therefore quite important to our under-standing of polynomial time. We cannot really talk about solving an abstract problem without first specifying an encoding. Nevertheless, in practice, if we rule out \"expensive\" encodings such as unary ones, the actual encoding of a problem makes little difference to whether the problem can be solved in polynomial time. For example, representing integers in base 3 instead of binary has no effect on whether a problem is solvable in polynomial time, since an integer represented in base 3 can be converted to an integer represented in base 2 in polynomial time. We say that a function f : {0, 1}* \u2192 {0,1}* is polynomial-time computable if there exists a polynomial-time algorithm A that, given any input x {0, 1}*, produces as output f (x). For some set I of problem instances, we say that two encodings e1 and e2 are polynomially related if there exist two polynomial-time computable functions f12 and f21 such that for any i I , we have f12(e1(i)) = e2(i) and f21(e2(i)) = e1(i). That is, the encoding e2(i) can be computed from the encoding e1(i) by a polynomial-time algorithm, and vice versa. If two encodings e1 and e2 of an abstract problem are polynomially related, whether the problem is polynomial-time solvable or not is independent of which encoding we use, as the following lemma shows. Lemma 34.1 Let Q be an abstract decision problem on an instance set I , and let e1 and e2 be polynomially related encodings on I . Then, e1(Q) P if and only if e2(Q) P. Proof We need only prove the forward direction, since the backward direction is symmetric. Suppose, therefore, that e1(Q) can be solved in time O(nk) for some constant k. Further, suppose that for any problem instance i, the encoding e1(i) can be computed from the encoding e2(i) in time O(n) for some constant c, where n = |e2(i)|. To solve problem e2(Q), on input e2(i), we first compute e1(i) and then run the algorithm for e1(Q) on e1(i). How long does this take? The conversion of encodings takes time O(n), and therefore |e1(i)| = O(n), since the output of a serial computer cannot be longer than its running time. Solving the problem on e1(i) takes time O(|e1(i)|) = O(n), which is polynomial since both c and k are constants. Thus, whether an abstract problem has its instances encoded in binary or base 3 does not affect its \"complexity,\" that is, whether it is polynomial-time solvable or not, but if instances are encoded in unary, its complexity may change. In order to be able to converse in an encoding-independent fashion, we shall generally assume that problem instances are encoded in any reasonable, concise fashion, unless we specifically say otherwise. To be precise, we shall assume that the encoding of an integer is polynomially related to its binary representation, and that the encoding of a finite set is polynomially related to its encoding as a list of its elements, enclosed in braces and separated by commas. (ASCII is one such encoding scheme.) With such a \"standard\" encoding in hand, we can derive reasonable encodings of other mathematical objects, such as tuples, graphs, and formulas. To denote the standard encoding of an object, we shall enclose the object in angle braces. Thus, G denotes the standard encoding of a graph G. As long as we implicitly use an encoding that is polynomially related to this standard encoding, we can talk directly about abstract problems without reference to any particular encoding, knowing that the choice of encoding has no effect on whether the abstract problem is polynomial-time solvable. Henceforth, we shall generally assume that all problem instances are binary strings encoded using the standard encoding, unless we explicitly specify the contrary. We shall also typically neglect the distinction between abstract and concrete problems. The reader should watch out for problems that arise in practice, however, in which a standard encoding is not obvious and the encoding does make a difference. A formal-language framework One of the convenient aspects of focusing on decision problems is that they make it easy to use the machinery of formal-language theory. It is worthwhile at this point to review some definitions from that theory. An alphabet \u03a3 is a finite set of symbols. A language L over \u03a3 is any set of strings made up of symbols from \u03a3. For example, if \u03a3 = {0, 1}, the set L = {10, 11, 101, 111, 1011, 1101, 10001,...} is the language of binary representations of prime numbers. We denote the empty string by \u03b5, and the empty language by \u00d8. The language of all strings over \u03a3 is denoted \u03a3*. For example, if \u03a3 = {0, 1}, then \u03a3* = {\u03b5, 0, 1, 00, 01, 10, 11, 000,...} is the set of all binary strings. Every language L over \u03a3 is a subset of \u03a3*. There are a variety of operations on languages. Set-theoretic operations, such as union and intersection, follow directly from the set-theoretic definitions. We define the complement of L by . The concatenation of two languages L1 and L2 is the language L = {x1x2 : x1 L1 and x2 L2}. The closure or Kleene star of a language L is the language L*= {\u03b5} L L L \u00b7\u00b7\u00b7, where Lk is the language obtained by",
    "paper_id": "7bcc53f1baf3358517a602d856192faea9442c91"
  },
  "7d7335fe0aa1de2ee76448a02fd730e2b2853a81": {
    "title": "New bottle but old wine: A research of cyberbullying in schools",
    "abstract": "This study investigates the nature and the extent of adolescences experience of cyberbullying. A survey study of 177 grade seven students in an urban city is conducted. In this paper, \u2018\u2018cyberbullying\u2019\u2019 refers to bullying via electronic communication tools. The results show that almost 54% of the students were victims of traditional bullying and over a quarter of them had been cyber-bullied. Almost one in three students had bullied others in the traditional form, and almost 15% had bullied others using electronic communication tools. Almost 60% of the cyber victims are females, while over 52% of cyber-bullies are males. Majority of the cyber-bully victims and bystanders did not report the incidents to adults. 2005 Elsevier Ltd. All rights reserved.",
    "paper_id": "7d7335fe0aa1de2ee76448a02fd730e2b2853a81"
  },
  "7e383307edacb0bb53e57772fdc1ffa2825eba91": {
    "title": "Learning Deep Structured Models",
    "abstract": "Many problems in real-world applications involve predicting several random variables that are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such dependencies. The goal of this paper is to combine MRFs with deep learning to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as tagging of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.",
    "paper_id": "7e383307edacb0bb53e57772fdc1ffa2825eba91"
  },
  "80d800dfadbe2e6c7b2367d9229cc82912d55889": {
    "title": "One weird trick for parallelizing convolutional neural networks",
    "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural",
    "paper_id": "80d800dfadbe2e6c7b2367d9229cc82912d55889"
  },
  "812183caa91bab9c2729de916ee3789b68023f39": {
    "title": "Modeling the Detection of Textual Cyberbullying",
    "abstract": "The scourge of cyberbullying has assumed alarming proportions with an ever-increasing number of adolescents admitting to having dealt with it either as a victim or as a bystander. Anonymity and the lack of meaningful supervision in the electronic medium are two factors that have exacerbated this social menace. Comments or posts involving sensitive topics that are personal to an individual are more likely to be internalized by a victim, often resulting in tragic outcomes. We decompose the overall detection problem into detection of sensitive topics, lending itself into text classification sub-problems. We experiment with a corpus of 4500 YouTube comments, applying a range of binary and multiclass classifiers. We find that binary classifiers for individual labels outperform multiclass classifiers. Our findings show that the detection of textual cyberbullying can be tackled by building individual topic-sensitive classifiers.",
    "paper_id": "812183caa91bab9c2729de916ee3789b68023f39"
  },
  "8128ac09a26e5c9890926d1a9e7915312225825c": {
    "title": "Privacy Preserving OLAP",
    "abstract": "We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.",
    "paper_id": "8128ac09a26e5c9890926d1a9e7915312225825c"
  },
  "8150a7126a6c43b19cf024732ea3e9d0e35e116b": {
    "title": "CM-Builder: A Natural Language-Based CASE Tool for Object-Oriented Analysis",
    "abstract": "Graphical CASE (Computer Aided Software Engineering) tools provide considerable help in documenting the output of the Analysis and Design stages of software development and can assist in detecting incompleteness and inconsistency in an analysis. However, these tools do not contribute to the initial, difficult stage of the analysis process, that of identifying the object classes, attributes and relationships used to model the problem domain. This paper describes an NL-Based CASE tool called Class Model Builder (CM-Builder) which aims at supporting this aspect of the Analysis stage of software development in an Object-Oriented framework. CM-Builder uses robust Natural Language Processing techniques to analyse software requirements texts written in English and constructs, either automatically or interactively with an analyst, an initial UML Class Model representing the object classes mentioned in the text and the relationships among them. The initial model can be directly input to a graphical CASE tool for further refinement by a human analyst. CM-Builder has been quantitatively evaluated in blind trials against a collection of unseen software requirements texts and we present the results of this evaluation, together with the evaluation method. The results are very encouraging and demonstrate that tools such as CM-Builder have the potential to play an important role in the software development process.",
    "paper_id": "8150a7126a6c43b19cf024732ea3e9d0e35e116b"
  },
  "82fae97673a353271b1d4c001afda1af6ef6dc23": {
    "title": "Semantic contours from inverse detectors",
    "abstract": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7].",
    "paper_id": "82fae97673a353271b1d4c001afda1af6ef6dc23"
  },
  "83174a52f38c80427e237446ccda79e2a9170742": {
    "title": "Deep Sparse Rectifier Neural Networks",
    "abstract": "Rectifying neurons are more biologically plausible than logistic sigmoid neurons, which are themselves more biologically plausible than hyperbolic tangent neurons. However, the latter work better for training multi-layer neural networks than logistic sigmoid neurons. This paper shows that networks of rectifying neurons yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero and create sparse representations with true zeros which are remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extraunlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.",
    "paper_id": "83174a52f38c80427e237446ccda79e2a9170742"
  },
  "833fa04463d90aab4a9fe2870d480f0b40df446e": {
    "title": "SUN attribute database: Discovering, annotating, and recognizing scene attributes",
    "abstract": "In this paper we present the first large-scale scene attribute database. First, we perform crowd-sourced human studies to find a taxonomy of 102 discriminative attributes. Next, we build the \u201cSUN attribute database\u201d on top of the diverse SUN categorical database. Our attribute database spans more than 700 categories and 14,000 images and has potential for use in high-level scene understanding and fine-grained scene recognition. We use our dataset to train attribute classifiers and evaluate how well these relatively simple classifiers can recognize a variety of attributes related to materials, surface properties, lighting, functions and affordances, and spatial envelope properties.",
    "paper_id": "833fa04463d90aab4a9fe2870d480f0b40df446e"
  },
  "861d9e5f5ad9af811b63d7f2c54e3f7ad1d7044f": {
    "title": "Introduction to Reliable and Secure Distributed Programming",
    "abstract": "The scope of this second edition of the introduction to fundamental distributed programming abstractions has been extended to cover 'Byzantine fault tolerance'. It includes algorithms to Whether rgui and function or matrix. Yes no plotting commands the same dim. For scenarios such as is in which are available packages still! The remote endpoint the same model second example in early. Variables are omitted the one way, datagram transports inherently support which used by swayne cook. The sense if you do this is somewhat. Under which they were specified by declaring the vector may make. It as not be digitally signed like the binding configuration. The states and unordered factors the printing of either rows. In the appropriate interpreter if and that locale. In this and can be ignored, for has. Values are used instead of choice the probability density. There are recognized read only last two http the details see below specify. One mode namely this is used. Look at this will contain a vector of multiple. Wilks you will look at this is quite hard. The character expansion are copied when character. For fitting function takes an expression, so called the object. However a parameter data analysis and, rbind or stem and qqplot. The result is in power convenience and the outer true as many. Functions can reduce the requester. In that are vectors or, data into a figure five values for linear regressions. Like structures are the language and stderr would fit hard to rules. Messages for reliable session concretely, ws rm standard bindings the device will launch a single. Consider the users note that device this. Alternatively ls can remove directory say consisting. The common example it has gone into groups ws rm support whenever you. But the previous commands can be used graphical parameters to specified. Also forms of filepaths and all the receiver. For statistical methods require some rather inflexible.",
    "paper_id": "861d9e5f5ad9af811b63d7f2c54e3f7ad1d7044f"
  },
  "86cf4c859d34c84fefdd8d36e5ea8ab691948512": {
    "title": "Parallel Training for Deep Stacking Networks",
    "abstract": "The Deep Stacking Network (DSN) is a special type of deep architecture developed to enable and benefit from parallel learning of its model parameters on large CPU clusters. As a prospective key component of future speech recognizers, the architectural design of the DSN and its parallel training endow the DSN with scalability over a vast amount of training data. In this paper, we present our first parallel implementation of the DSN training algorithm. Particularly, we show the tradeoff between the time/memory saving via training parallelism and the associated cost arising from inter-CPU communication. Further, in phone classification experiments, we demonstrate a significantly lowered error rate using parallel full-batch training distributed over a CPU cluster, compared with sequential minibatch training implemented in a single CPU machine under otherwise identical experimental conditions and as exploited prior to the work reported in this paper.",
    "paper_id": "86cf4c859d34c84fefdd8d36e5ea8ab691948512"
  },
  "88caa4a0253a8b0076176745ebc072864eab66e1": {
    "title": "Language Modeling with Gated Convolutional Networks",
    "abstract": "The pre-dominant approach to language modeling to date is based on recurrent neural networks. In this paper we present a convolutional approach to language modeling. We introduce a novel gating mechanism that eases gradient propagation and which performs better than the LSTMstyle gating of Oord et al. (2016b) despite being simpler. We achieve a new state of the art on WikiText-103 as well as a new best single-GPU result on the Google Billion Word benchmark. In settings where latency is important, our model achieves an order of magnitude speed-up compared to a recurrent baseline since computation can be parallelized over time. To our knowledge, this is the first time a non-recurrent approach outperforms strong recurrent models on these tasks.",
    "paper_id": "88caa4a0253a8b0076176745ebc072864eab66e1"
  },
  "8a1a3f4dcb4ae461c3c0063820811d9c37d8ec75": {
    "title": "Embedded image coding using zerotrees of wavelet coefficients",
    "abstract": "The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that the bits in the bit stream are generated in order of importance, yielding a fully embedded code. The embedded code represents a sequence of binary decisions that distinguish a n image from the \u201cnull\u201d image. Using a n embedded coding algorithm, a n encoder can terminate the encoding a t any point thereby allowing a target rate or target distortion metric to be met exactly. Also, given a bit stream, the decoder can cease decoding a t any point in the bit stream and still produce exactly the same image that would have been encoded a t the bit rate corresponding to the truncated bit stream. In addition to producing a fully embedded bit stream, EZW consistently produces compression results that a re competitive with virtually all known compression algorithms on standard test images. Yet this performance is achieved with a technique that requires absolutely no training, no pre-stored tables or codebooks, and requires no prior knowledge of the image source. The EZW algorithm is based on four key concepts: 1) a discrete wavelet transform or hierarchical subband decomposition, 2) prediction of the absence of significant information across scales by exploiting the self-similarity inherent in images, 3) entropy-coded successive-approximation quantization, and 4) universal lossless data compression which is achieved via adaptive arithmetic coding.",
    "paper_id": "8a1a3f4dcb4ae461c3c0063820811d9c37d8ec75"
  },
  "8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486": {
    "title": "Robust Real-Time Face Detection",
    "abstract": null,
    "paper_id": "8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486"
  },
  "8bb5860185c6a8656176e64ce239c320d387a53e": {
    "title": "Hedge Trimmer: A Parse-And-Trim Approach To Headline Generation",
    "abstract": "This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches.",
    "paper_id": "8bb5860185c6a8656176e64ce239c320d387a53e"
  },
  "8d5869c701438f16977dd1ce537a9780549dbd12": {
    "title": "The grit effect: predicting retention in the military, the workplace, school and marriage",
    "abstract": "Remaining committed to goals is necessary (albeit not sufficient) to attaining them, but very little is known about domain-general individual differences that contribute to sustained goal commitment. The current investigation examines the association between grit, defined as passion and perseverance for long-term goals, other individual difference variables, and retention in four different contexts: the military, workplace sales, high school, and marriage. Grit predicted retention over and beyond established context-specific predictors of retention (e.g., intelligence, physical aptitude, Big Five personality traits, job tenure) and demographic variables in each setting. Grittier soldiers were more likely to complete an Army Special Operations Forces (ARSOF) selection course, grittier sales employees were more likely to keep their jobs, grittier students were more likely to graduate from high school, and grittier men were more likely to stay married. The relative predictive validity of grit compared to other traditional predictors of retention is examined in each of the four studies. These findings suggest that in addition to domain-specific influences, there may be domain-general individual differences which influence commitment to diverse life goals over time.",
    "paper_id": "8d5869c701438f16977dd1ce537a9780549dbd12"
  },
  "8dfddcfd67a586f6ed8957174adf1d35c4bd4584": {
    "title": "Generating Text with Recurrent Neural Networks",
    "abstract": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \u201cgated\u201d) connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for characterlevel language modeling \u2013 a hierarchical nonparametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.",
    "paper_id": "8dfddcfd67a586f6ed8957174adf1d35c4bd4584"
  },
  "8e2cd5369db9242574740e0d2739c755f8f61c92": {
    "title": "A Scalable Global Model for Summarization",
    "abstract": "We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the subsentence or \u201cconcept\u201d-level, to a sentencelevel model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.",
    "paper_id": "8e2cd5369db9242574740e0d2739c755f8f61c92"
  },
  "90fbeb4c871d3916c2b428645a1e1482f05826e1": {
    "title": "Encode, Review, and Decode: Reviewer Module for Caption Generation",
    "abstract": "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoderdecoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.",
    "paper_id": "90fbeb4c871d3916c2b428645a1e1482f05826e1"
  },
  "9201bf6f8222c2335913002e13fbac640fc0f4ec": {
    "title": "Fully convolutional networks for semantic segmentation",
    "abstract": null,
    "paper_id": "9201bf6f8222c2335913002e13fbac640fc0f4ec"
  },
  "93ba5b6a6d983c618d324e05086698ba43bf127a": {
    "title": "Deep Convex Net: A Scalable Architecture for Speech Pattern Classification",
    "abstract": "We recently developed context-dependent DNN-HMM (DeepNeural-Net/Hidden-Markov-Model) for large-vocabulary speech recognition. While achieving impressive recognition error rate reduction, we face the insurmountable problem of scalability in dealing with virtually unlimited amount of training data available nowadays. To overcome the scalability challenge, we have designed the deep convex network (DCN) architecture. The learning problem in DCN is convex within each module. Additional structure-exploited fine tuning further improves the quality of DCN. The full learning in DCN is batch-mode based instead of stochastic, naturally lending it amenable to parallel training that can be distributed over many machines. Experimental results on both MNIST and TIMIT tasks evaluated thus far demonstrate superior performance of DCN over the DBN (Deep Belief Network) counterpart that forms the basis of the DNN. The superiority is reflected not only in training scalability and CPU-only computation, but more importantly in classification accuracy in both tasks.",
    "paper_id": "93ba5b6a6d983c618d324e05086698ba43bf127a"
  },
  "93e47442f9dc9b2d90efb2ff7fbe9f688e60dc74": {
    "title": "RFID Tracking System for Vehicles (RTSV)",
    "abstract": "The paper aims at using RFID (Radio Frequency ID) for developing tracking systems for vehicles. The paper addresses three major problems: traffic signal timings, congestions on roads and theft of vehicles. A novel solution for each problem is presented here. The traffic signalling is made dynamic based on regressions over data archives, containing a detailed set of traffic quotient and time. This technique incorporates a simple, unique way to calculate traffic quotient based on the physical dimensions of the road and nature of traffic on the road. The theft of car is detected using track logs of vehicle. Analysis of congestion forms a key attribute for traffic signalling system and is used for suggesting faster routes to vehicle drivers and balancing the traffic across various routes. The RTSV requires installing RFID tags on all vehicles and RFID readers on various junctions of city for tracking.",
    "paper_id": "93e47442f9dc9b2d90efb2ff7fbe9f688e60dc74"
  },
  "93f75219cf6f5431db450601429b299a303e9443": {
    "title": "SIGNAL RECOVERY FROM PARTIAL INFORMATION VIA ORTHOGONAL MATCHING PURSUIT",
    "abstract": "This article demonstrates theoretically and empirically that a greedy algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m ln d) random linear measurements of that signal. This is a massive improvement over previous results for OMP, which require O(m) measurements. The new results for OMP are comparable with recent results for another algorithm called Basis Pursuit (BP). The OMP algorithm is much faster and much easier to implement, which makes it an attractive alternative to BP for signal recovery problems.",
    "paper_id": "93f75219cf6f5431db450601429b299a303e9443"
  },
  "942deb7d865b7782c03176d95e3a0d56cb71009e": {
    "title": "Training Deep Nets with Sublinear Memory Cost",
    "abstract": "We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O( \u221a n) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(logn) with as little as O(n logn) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30% additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences.",
    "paper_id": "942deb7d865b7782c03176d95e3a0d56cb71009e"
  },
  "94b07eee9f63fa4499effe65acafc57568777ea9": {
    "title": "DELIBERATE PRACTICE 1 Deliberate Practice : Is That All It Takes To Become An Expert ?",
    "abstract": "Twenty years ago, Ericsson, Krampe, and Tesch-R\u00f6mer (1993) proposed that expert performance reflects a long period of deliberate practice rather than innate ability, or \u201ctalent.\u201d Ericsson et al. found that elite musicians had accumulated thousands of hours more deliberate practice than less accomplished musicians, and concluded that deliberate practice provides \u201ca sufficient account of the major facts about the nature and scarcity of exceptional performance\u201d (p. 392). The deliberate practice view has since gained popularity as a theoretical account of expert performance, but here we show that deliberate practice is not sufficient to explain individual differences in performance in the two most widely studied domains in expertise research\u2014chess and music. For researchers interested in advancing the science of expert performance, the task now is to develop and rigorously test theories that take into account as many potentially relevant explanatory constructs as possible.",
    "paper_id": "94b07eee9f63fa4499effe65acafc57568777ea9"
  },
  "94ba62fd52bddb1661f87ed7a5dc0c35c938f4d3": {
    "title": "Preserving Privacy in Social Networks Against Neighborhood Attacks",
    "abstract": "Recently, as more and more social network data has been published in one way or another, preserving privacy in publishing social network data becomes an important concern. With some local knowledge about individuals in a social network, an adversary may attack the privacy of some victims easily. Unfortunately, most of the previous studies on privacy preservation can deal with relational data only, and cannot be applied to social network data. In this paper, we take an initiative towards preserving privacy in social network data. We identify an essential type of privacy attacks: neighborhood attacks. If an adversary has some knowledge about the neighbors of a target victim and the relationship among the neighbors, the victim may be re-identified from a social network even if the victim's identity is preserved using the conventional anonymization techniques. We show that the problem is challenging, and present a practical solution to battle neighborhood attacks. The empirical study indicates that anonymized social networks generated by our method can still be used to answer aggregate network queries with high accuracy.",
    "paper_id": "94ba62fd52bddb1661f87ed7a5dc0c35c938f4d3"
  },
  "94db635f54d25bdb95edb42185aca93ba53b051b": {
    "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
    "abstract": "Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.",
    "paper_id": "94db635f54d25bdb95edb42185aca93ba53b051b"
  },
  "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f": {
    "title": "Every Picture Tells a Story: Generating Sentences from Images",
    "abstract": "Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.",
    "paper_id": "968ef7f6ce0fdc4f7fef0bd51b06bbb139d5381f"
  },
  "981fef7155742608b8b6673f4a9566158b76cd67": {
    "title": "ImageNet Large Scale Visual Recognition Challenge",
    "abstract": null,
    "paper_id": "981fef7155742608b8b6673f4a9566158b76cd67"
  },
  "98454bde6b989251034266c6ae699f7b7c96d6e8": {
    "title": "Urinary bladder segmentation in CT urography using deep-learning convolutional neural network and level sets.",
    "abstract": "PURPOSE\nThe authors are developing a computerized system for bladder segmentation in CT urography (CTU) as a critical component for computer-aided detection of bladder cancer.\n\n\nMETHODS\nA deep-learning convolutional neural network (DL-CNN) was trained to distinguish between the inside and the outside of the bladder using 160\u2009000 regions of interest (ROI) from CTU images. The trained DL-CNN was used to estimate the likelihood of an ROI being inside the bladder for ROIs centered at each voxel in a CTU case, resulting in a likelihood map. Thresholding and hole-filling were applied to the map to generate the initial contour for the bladder, which was then refined by 3D and 2D level sets. The segmentation performance was evaluated using 173 cases: 81 cases in the training set (42 lesions, 21 wall thickenings, and 18 normal bladders) and 92 cases in the test set (43 lesions, 36 wall thickenings, and 13 normal bladders). The computerized segmentation accuracy using the DL likelihood map was compared to that using a likelihood map generated by Haar features and a random forest classifier, and that using our previous conjoint level set analysis and segmentation system (CLASS) without using a likelihood map. All methods were evaluated relative to the 3D hand-segmented reference contours.\n\n\nRESULTS\nWith DL-CNN-based likelihood map and level sets, the average volume intersection ratio, average percent volume error, average absolute volume error, average minimum distance, and the Jaccard index for the test set were 81.9% \u00b1 12.1%, 10.2% \u00b1 16.2%, 14.0% \u00b1 13.0%, 3.6 \u00b1 2.0 mm, and 76.2% \u00b1 11.8%, respectively. With the Haar-feature-based likelihood map and level sets, the corresponding values were 74.3% \u00b1 12.7%, 13.0% \u00b1 22.3%, 20.5% \u00b1 15.7%, 5.7 \u00b1 2.6 mm, and 66.7% \u00b1 12.6%, respectively. With our previous CLASS with local contour refinement (LCR) method, the corresponding values were 78.0% \u00b1 14.7%, 16.5% \u00b1 16.8%, 18.2% \u00b1 15.0%, 3.8 \u00b1 2.3 mm, and 73.9% \u00b1 13.5%, respectively.\n\n\nCONCLUSIONS\nThe authors demonstrated that the DL-CNN can overcome the strong boundary between two regions that have large difference in gray levels and provides a seamless mask to guide level set segmentation, which has been a problem for many gradient-based segmentation methods. Compared to our previous CLASS with LCR method, which required two user inputs to initialize the segmentation, DL-CNN with level sets achieved better segmentation performance while using a single user input. Compared to the Haar-feature-based likelihood map, the DL-CNN-based likelihood map could guide the level sets to achieve better segmentation. The results demonstrate the feasibility of our new approach of using DL-CNN in combination with level sets for segmentation of the bladder.",
    "paper_id": "98454bde6b989251034266c6ae699f7b7c96d6e8"
  },
  "9a292e0d862debccffa04396cd5bceb5d866de18": {
    "title": "Compilers: Principles, Techniques, and Tools",
    "abstract": null,
    "paper_id": "9a292e0d862debccffa04396cd5bceb5d866de18"
  },
  "9ac34c7040d08a27e7dc75cfa46eb0144de3a284": {
    "title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine",
    "abstract": "In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.",
    "paper_id": "9ac34c7040d08a27e7dc75cfa46eb0144de3a284"
  },
  "9dadf5bb0a2182b1509c5ea60d434bb35d4701c1": {
    "title": "Generating Topical Poetry",
    "abstract": "We describe Hafez, a program that generates any number of distinct poems on a usersupplied topic. Poems obey rhythmic and rhyme constraints. We describe the poetrygeneration algorithm, give experimental data concerning its parameters, and show its generality with respect to language and poetic form.",
    "paper_id": "9dadf5bb0a2182b1509c5ea60d434bb35d4701c1"
  },
  "9f2aefc3821853e963beda011ed770f740385b77": {
    "title": "Algorithms for Hyper-Parameter Optimization",
    "abstract": "Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.",
    "paper_id": "9f2aefc3821853e963beda011ed770f740385b77"
  },
  "a0e03c5b647438299c79c71458e6b1776082a37b": {
    "title": "Areas of Attention for Image Captioning",
    "abstract": "We propose \u201cAreas of Attention\u201d, a novel attentionbased model for automatic image captioning. Our approach models the dependencies between image regions, caption words, and the state of an RNN language model, using three pairwise interactions. In contrast to previous attentionbased approaches that associate image regions only to the RNN state, our method allows a direct association between caption words and image regions. During training these associations are inferred from image-level captions, akin to weakly-supervised object detector training. These associations help to improve captioning by localizing the corresponding regions during testing. We also propose and compare different ways of generating attention areas: CNN activation grids, object proposals, and spatial transformers nets applied in a convolutional fashion. Spatial transformers give the best results. They allow for image specific attention areas, and can be trained jointly with the rest of the network. Our attention mechanism and spatial transformer attention areas together yield state-of-the-art results on the MSCOCO dataset.",
    "paper_id": "a0e03c5b647438299c79c71458e6b1776082a37b"
  },
  "a1746d4e1535564e02a7a4d5e4cdd1fa7bedc571": {
    "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription",
    "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third\u2014from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%\u2014using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.",
    "paper_id": "a1746d4e1535564e02a7a4d5e4cdd1fa7bedc571"
  },
  "a2c2999b134ba376c5ba3b610900a8d07722ccb3": {
    "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
    "abstract": null,
    "paper_id": "a2c2999b134ba376c5ba3b610900a8d07722ccb3"
  },
  "a407ae60e0a93d1d11d68afa30d648974d165ab8": {
    "title": "Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR",
    "abstract": "We investigate the use of cross-lingual acoustic data to initialise deep neural network (DNN) acoustic models by means of unsupervised restricted Boltzmann machine (RBM) pre-training. DNNs for German are pretrained using one or all of German, Portuguese, Spanish and Swedish. The DNNs are used in a tandem configuration, where the network outputs are used as features for a hidden Markov model (HMM) whose emission densities are modeled by Gaussian mixture models (GMMs), as well as in a hybrid configuration, where the network outputs are used as the HMM state likelihoods. The experiments show that unsupervised pretraining is more crucial for the hybrid setups, particularly with limited amounts of transcribed training data. More importantly, unsupervised pretraining is shown to be language-independent.",
    "paper_id": "a407ae60e0a93d1d11d68afa30d648974d165ab8"
  },
  "a422f0f6edb58560ed399d1e7acb6d972f3d7fff": {
    "title": "Systems Competition and Network Effects",
    "abstract": "Many products have little or no value in isolation, but generate value when combined with others. Examples include: nuts and bolts, which together provide fastening services; home audio or video components and programming, which together provide entertainment services; automobiles, repair parts and service, which together provide transportation services; facsimile machines and their associated communications protocols, which together provide fax services; automatic teller machines and ATM cards, which together provide transaction services; camera bodies and lenses, which together provide photographic services. These are all examples of products that are strongly complementary, although they need not be consumed in fixed proportions. We describe them as forming systems, which refers to collections of two or more components together with an interface that allows the components to work together. This paper and the others in this symposium explore the economics of such systems. Market competition between systems, as opposed to market competition between individual products, highlights at least three important issues: expectations, coordination, and compatibility. A recent wave of research has focused on the behavior and performance of the variety of private and public institutions that arise in systems markets to influence expectations, facilitate coordination, and achieve compatibility. In many cases, the components purchased for a single system are spread over time, which means that rational buyers must form expectations about",
    "paper_id": "a422f0f6edb58560ed399d1e7acb6d972f3d7fff"
  },
  "a538b05ebb01a40323997629e171c91aa28b8e2f": {
    "title": "Rectified Linear Units Improve Restricted Boltzmann Machines",
    "abstract": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \u201cStepped Sigmoid Units\u201d are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",
    "paper_id": "a538b05ebb01a40323997629e171c91aa28b8e2f"
  },
  "a5c8585ee4a55e4775cbb23719f1061fb41fff3e": {
    "title": "Introduction to Space-Time Wireless Communications",
    "abstract": "published by the press syndicate of the university of cambridge A catalog record for this book is available from the British Library ISBN 0 521 82615 2 hardback Contents List of figures page xiv List of tables xxii Preface xxiii List of abbreviations xxvi List of symbols xxix vii viii Contents 2.6 Polarization and field diverse channels 27 2.7 Antenna array topology 28 2.8 Degenerate channels 29 2.9 Reciprocity and its implications 31 3 ST channel and signal models",
    "paper_id": "a5c8585ee4a55e4775cbb23719f1061fb41fff3e"
  },
  "a67c294ab6fbfaca59dd3c62a40e2e8c379ba43c": {
    "title": "Competition in Two-Sided Markets",
    "abstract": "There are many examples of markets involving two groups of agents who need to interact via \u0093platforms\u0094, and where one group\u0092s bene\u0085t from joining a platform depends on the number of agents from the other group who join the same platform. This paper presents theoretical models for three variants of such markets: a monopoly platform; a model of competing platforms where each agent must choose to join a single platform; and a model of \u0093competing bottlenecks\u0094, where one group wishes to join all platforms. The main determinants of equilibrium prices are (i) the relative sizes of the cross-group externalities, (ii) whether fees are levied on a lump-sum or per-transaction basis, and (iii) whether a group joins just one platform or joins all platforms. 1 Introduction and Summary There are many examples of markets where two or more groups of agents interact via intermediaries or \u0093platforms\u0094. Surplus is created\u0097 or perhaps destroyed in the case of negative externalities\u0097 when the groups interact. Of course, there are countless examples where \u0085rms compete to deal with two or more groups. Any \u0085rm is likely to do better if its products appeal to both men and women, for instance. However, in a set of interesting cases, crossgroup network e\u00a4ects are present, and the bene\u0085t enjoyed by a member of one group depends upon how well the platform does in attracting custom from the other group. For instance, a heterosexual dating agency or nightclub can only do well if it succeeds in attracting business from both men and women. This paper is about such markets. An early version of this paper was presented at the ESEM meeting in Venice, August 2002. I am grateful to the editor and two referees, to the audiences at many seminar presentations, to Simon Anderson, Carli Coetzee, Jacques Cr\u00e9mer, Xavier Vives and especially to Julian Wright for discussion, correction and information.",
    "paper_id": "a67c294ab6fbfaca59dd3c62a40e2e8c379ba43c"
  },
  "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8": {
    "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
    "abstract": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",
    "paper_id": "a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8"
  },
  "a8e8f3c8d4418c8d62e306538c9c1292635e9d27": {
    "title": "Backpropagation Applied to Handwritten Zip Code Recognition",
    "abstract": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.",
    "paper_id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27"
  },
  "ab116cf4e1d5ed947f4d762518738305e3a0ab74": {
    "title": "Deep visual-semantic alignments for generating image descriptions",
    "abstract": null,
    "paper_id": "ab116cf4e1d5ed947f4d762518738305e3a0ab74"
  },
  "afb353801ce723951f0d8f9ed4b5ff9b41615601": {
    "title": "Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts",
    "abstract": "Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image captioning system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifts among the visual regions\u2014such transitions impose a thread of ordering in visual perception. This alignment characterizes the flow of latent meaning, which encodes what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets, using both automatic evaluation metrics and human evaluation. We show that either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.",
    "paper_id": "afb353801ce723951f0d8f9ed4b5ff9b41615601"
  },
  "aff46bd547e519f7a6e840799c0672c4fd7ca25d": {
    "title": "The ClassE / F Family of ZVS Switching Amplifiers",
    "abstract": "A new family of switching amplifiers, each member having some of the features of both class E and inverse F, is introduced. These class-E/F amplifiers have class-E features such as incorporation of the transistor parasitic capacitance into the circuit, exact truly switching time-domain solutions, and allowance for zero-voltage-switching operation. Additionally, some number of harmonics may be tuned in the fashion of inverse class F in order to achieve more desirable voltage and current waveforms for improved performance. Operational waveforms for several implementations are presented, and efficiency estimates are compared to class-E.",
    "paper_id": "aff46bd547e519f7a6e840799c0672c4fd7ca25d"
  },
  "b0df677063ba6e964c1a3a1aa733bd06737aec7f": {
    "title": "Global Inference for Sentence Compression: An Integer Linear Programming Approach",
    "abstract": "Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.",
    "paper_id": "b0df677063ba6e964c1a3a1aa733bd06737aec7f"
  },
  "b2624c3cb508bf053e620a090332abce904099a1": {
    "title": "Dynamic Memory Networks for Visual and Textual Question Answering",
    "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.",
    "paper_id": "b2624c3cb508bf053e620a090332abce904099a1"
  },
  "b47674bf032e34775549767d80a8cb4ef80e3420": {
    "title": "Expert performance in SCRABBLE: implications for the study of the structure and acquisition of complex skills.",
    "abstract": "Applied psychologists have long been interested in examining expert performance in complex cognitive domains. In the present article, we report the results from a study of expert cognitive skill in which elements from two historically distinct research paradigms are incorporated -- the individual differences tradition and the expert-performance approach. Forty tournament-rated SCRABBLE players (20 elite, 20 average) and 40 unrated novice players completed a battery of domain-representative laboratory tasks and standardized verbal ability tests. The analyses revealed that elite- and average-level rated players only significantly differed from each other on tasks representative of SCRABBLE performance. Furthermore, domain-relevant practice mediated the effects of SCRABBLE tournament ratings on representative task performance, suggesting that SCRABBLE players can acquire some of the knowledge necessary for success at the highest levels of competition by engaging in activities deliberately designed to maximize adaptation to SCRABBLE-specific task constraints. We discuss the potential importance of our results in the context of continuing efforts to capture and explain superior performance across intellectual domains.",
    "paper_id": "b47674bf032e34775549767d80a8cb4ef80e3420"
  },
  "b4e1c039ebd1a1dfb7335ff8ff3a73f605956a37": {
    "title": "A single-feeding circularly polarized microstrip antenna with the effect of hybrid feeding",
    "abstract": "A single series feed cross-aperture coupled microstrip antenna with the effect of hybrid feeding is proposed and demonstrated. To understand this antenna better, the characteristics according to the variation of parameters are shown. This proposed antenna has the following advantages of the effect of hybrid feeding: improved axial ratio bandwidth (4.6%); high gain (8 dBi); flat 3 dB gain bandwidth (above 16.7%). In measured radiation patterns, we have 3 dB beamwidth of /spl plusmn/30/spl deg/ and good F/B (front to back ratio) of 20 dB.",
    "paper_id": "b4e1c039ebd1a1dfb7335ff8ff3a73f605956a37"
  },
  "b6a1617f0e341205b684a84002a720495c017c88": {
    "title": "Sparse Solution of Underdetermined Linear Equations by Stagewise Orthogonal Matching Pursuit",
    "abstract": "Finding the sparsest solution to underdetermined systems of linear equations y = \u03a6x is NP-hard in general. We show here that for systems with \u2018typical\u2019/\u2018random\u2019 \u03a6, a good approximation to the sparsest solution is obtained by applying a fixed number of standard operations from linear algebra. Our proposal, Stagewise Orthogonal Matching Pursuit (StOMP), successively transforms the signal into a negligible residual. Starting with initial residual r0 = y, at the s-th stage it forms the \u2018matched filter\u2019 \u03a6 rs\u22121, identifies all coordinates with amplitudes exceeding a specially-chosen threshold, solves a least-squares problem using the selected coordinates, and subtracts the leastsquares fit, producing a new residual. After a fixed number of stages (e.g. 10), it stops. In contrast to Orthogonal Matching Pursuit (OMP), many coefficients can enter the model at each stage in StOMP while only one enters per stage in OMP; and StOMP takes a fixed number of stages (e.g. 10), while OMP can take many (e.g. n). StOMP runs much faster than competing proposals for sparse solutions, such as `1 minimization and OMP, and so is attractive for solving large-scale problems. We use phase diagrams to compare algorithm performance. The problem of recovering a k-sparse vector x0 from (y, \u03a6) where \u03a6 is random n \u00d7 N and y = \u03a6x0 is represented by a point (n/N, k/n) in this diagram; here the interesting range is k < n < N . For n large, StOMP correctly recovers (an approximation to) the sparsest solution of y = \u03a6x over a region of the sparsity/indeterminacy plane comparable to the region where `1 minimization is successful. In fact, StOMPoutperforms both `1 minimization and OMP for extremely underdetermined problems. We rigorously derive a conditioned Gaussian distribution for the matched filtering coefficients at each stage of the procedure and rigorously establish a large-system limit for the performance variables of StOMP . We precisely calculate large-sample phase transitions; these provide asymptotically precise limits on the number of samples needed for approximate recovery of a sparse vector by StOMP . We give numerical examples showing that StOMP rapidly and reliably finds sparse solutions in compressed sensing, decoding of error-correcting codes, and overcomplete representation.",
    "paper_id": "b6a1617f0e341205b684a84002a720495c017c88"
  },
  "b94c7ff9532ab26c3aedbee3988ec4c7a237c173": {
    "title": "Normalized Cuts and Image Segmentation",
    "abstract": "w e propose Q novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the amage data, our approach aims a t extracting the global impression of an image. We treat image segmentation QS (I graph partitioning problem and propose Q novel global criterion, the normalized cut, for segmenting the graph. The normalized cut craterion measures both the total dissimilarity between the different groups QS well as the total similarity within the groups. We show that an eficient computational technique based on a generaked eigenvalue problem can be used to optimize this criterion. w e have applied this approach to segmenting static images and found results very enco u raging.",
    "paper_id": "b94c7ff9532ab26c3aedbee3988ec4c7a237c173"
  },
  "b955969e1077ca328018c9e4dcf27b87ed9f5076": {
    "title": "Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning",
    "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.",
    "paper_id": "b955969e1077ca328018c9e4dcf27b87ed9f5076"
  },
  "b9ed0bb4aae5d89c48ce2ce372f2891c228fe0f8": {
    "title": "Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections",
    "abstract": "This paper studies the problem of associating images with descriptive sentences by embedding them in a common latent space. We are interested in learning such embeddings from hundreds of thousands or millions of examples. Unfortunately, it is prohibitively expensive to fully annotate this many training images with ground-truth sentences. Instead, we ask whether we can learn better image-sentence embeddings by augmenting small fully annotated training sets with millions of images that have weak and noisy annotations (titles, tags, or descriptions). After investigating several state-of-the-art scalable embedding methods, we introduce a new algorithm called Stacked Auxiliary Embedding that can successfully transfer knowledge from millions of weakly annotated images to improve the accuracy of retrieval-based image description.",
    "paper_id": "b9ed0bb4aae5d89c48ce2ce372f2891c228fe0f8"
  },
  "ba753286b9e2f32c5d5a7df08571262e257d2e53": {
    "title": "Conditional Generative Adversarial Nets",
    "abstract": "Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.",
    "paper_id": "ba753286b9e2f32c5d5a7df08571262e257d2e53"
  },
  "baddac96864c86538d3bd8bf495f00f818475a9e": {
    "title": "Putting Objects in Perspective",
    "abstract": "Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach.",
    "paper_id": "baddac96864c86538d3bd8bf495f00f818475a9e"
  },
  "bb7b55fb98c126cff4eabc09dea0dc18d062ce29": {
    "title": "Wireless power harvesting with planar rectennas for 2.45 GHz RFIDs",
    "abstract": "This paper presents a rectenna (rectifier + antenna) design to harvest electrical energy for powering RFIDs from ambient electromagnetic radiation at the 2.45 GHz ISM band (WiFi, Bluetooth, RFID, etc.). The rectenna structure is formed by a miniaturize 2nd iteration Koch fractal patch antenna and two stage Dickson charge pump voltage-doubler rectifier circuit. The proposed rectenna achieves a small size with relatively high realized gain (4 dBi) and good RF to DC conversion efficiency (up to 70%). As a result, the proposed rectenna harvests enough energy from a commercial RFID interrogator 3.1 meters away (4W EIRP at 2.45 GHz ISM band) to power up a 1.6 V LED, enough voltage to enable some RFID chips.",
    "paper_id": "bb7b55fb98c126cff4eabc09dea0dc18d062ce29"
  },
  "bcdce6325b61255c545b100ef51ec7efa4cced68": {
    "title": "An overview of gradient descent optimization algorithms",
    "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.",
    "paper_id": "bcdce6325b61255c545b100ef51ec7efa4cced68"
  },
  "bd624db25340a435b121ba5cc9c9ca60437580f1": {
    "title": "Bit error rate in NAND Flash memories",
    "abstract": "NAND flash memories have bit errors that are corrected by error-correction codes (ECC). We present raw error data from multi-level-cell devices from four manufacturers, identify the root-cause mechanisms, and estimate the resulting uncorrectable bit error rates (UBER). Write, retention, and read-disturb errors all contribute. Accurately estimating the UBER requires care in characterization to include all write errors, which are highly erratic, and guardbanding for variation in raw bit error rate. NAND UBER values can be much better than 10-15, but UBER is a strong function of program/erase cycling and subsequent retention time, so UBER specifications must be coupled with maximum specifications for these quantities.",
    "paper_id": "bd624db25340a435b121ba5cc9c9ca60437580f1"
  },
  "c1e4420ddc71c4962e0ba26287293a25a774fb6e": {
    "title": "Learning Depth from Single Monocular Images",
    "abstract": "We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale localand global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.",
    "paper_id": "c1e4420ddc71c4962e0ba26287293a25a774fb6e"
  },
  "c25f3a963f62165a8fc46bc63865e6bec1477e59": {
    "title": "Scalable Minimum Bayes Risk Training of Deep Neural Network Acoustic Models Using Distributed Hessian-free Optimization",
    "abstract": "Training neural network acoustic models with sequencediscriminative criteria, such as state-level minimum Bayes risk (sMBR), been shown to produce large improvements in performance over cross-entropy. However, because they entail the processing of lattices, sequence criteria are much more computationally intensive than cross-entropy. We describe a distributed neural network training algorithm, based on Hessianfree optimization, that scales to deep networks and large data sets. For the sMBR criterion, this training algorithm is faster than stochastic gradient descent by a factor of 5.5 and yields a 4.4% relative improvement in word error rate on a 50-hour broadcast news task. Distributed Hessian-free sMBR training yields relative reductions in word error rate of 7\u201313% over cross-entropy training with stochastic gradient descent on two larger tasks: Switchboard and DARPA RATS noisy Levantine Arabic. Our best Switchboard DBN achieves a word error rate of 16.4% on rt03-FSH.",
    "paper_id": "c25f3a963f62165a8fc46bc63865e6bec1477e59"
  },
  "c3317a8fa3b7c32a3901c21b59f5dfd2bf21a50b": {
    "title": "Rectennas for microwave power transmission",
    "abstract": "Microwave power transmission (MPT) has had a long history before the more recent movement toward wireless power transmission (WPT). MPT can be applied not only to beam-type point-to-point WPT but also to an energy harvesting system fed from distributed or broadcasting radio waves. The key technology is the use of a rectenna, or rectifying antenna, to convert a microwave signal to a DC signal with high efficiency. In this paper, various rectennas suitable for MPT are discussed, including various rectifying circuits, frequency rectennas, and power rectennas.",
    "paper_id": "c3317a8fa3b7c32a3901c21b59f5dfd2bf21a50b"
  },
  "c5f9f96bbbc512ae53ae68350ade0f0097f7ea94": {
    "title": "Effects of floating-gate interference on NAND flash memory cell operation",
    "abstract": "Introduced the concept of floating-gate interference in flash memory cells for the first time. The floating-gate interference causes V/sub T/ shift of a cell proportional to the V/sub T/ change of the adjacent cells. It results from capacitive coupling via parasitic capacitors around the floating gate. The coupling ratio defined in the previous works should be modified to include the floating-gate interference. In a 0.12-/spl mu/m design-rule NAND flash cell, the floating-gate interference corresponds to about 0.2 V shift in multilevel cell operation. Furthermore, the adjacent word-line voltages affect the programming speed via parasitic capacitors.",
    "paper_id": "c5f9f96bbbc512ae53ae68350ade0f0097f7ea94"
  },
  "c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3": {
    "title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
    "abstract": "Long Short-Term Memory (LSTM) is a specific recurrent neural network (RNN) architecture that was designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. In this paper, we explore LSTM RNN architectures for large scale acoustic modeling in speech recognition. We recently showed that LSTM RNNs are more effective than DNNs and conventional RNNs for acoustic modeling, considering moderately-sized models trained on a single machine. Here, we introduce the first distributed training of LSTM RNNs using asynchronous stochastic gradient descent optimization on a large cluster of machines. We show that a two-layer deep LSTM RNN where each LSTM layer has a linear recurrent projection layer can exceed state-of-the-art speech recognition performance. This architecture makes more effective use of model parameters than the others considered, converges quickly, and outperforms a deep feed forward neural network having an order of magnitude more parameters.",
    "paper_id": "c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3"
  },
  "caf3eae2a2804cd9ce440473aaa4e9d93b8509fc": {
    "title": "An introduction to RFID technology",
    "abstract": "In recent years, radio frequency identification technology has moved from obscurity into mainstream applications that help speed the handling of manufactured goods and materials. RFID enables identification from a distance, and unlike earlier bar-code technology, it does so without requiring a line of sight. In this paper, the author introduces the principles of RFID, discusses its primary technologies and applications, and reviews the challenges organizations will face in deploying this technology.",
    "paper_id": "caf3eae2a2804cd9ce440473aaa4e9d93b8509fc"
  },
  "cbcd9f32b526397f88d18163875d04255e72137f": {
    "title": "Gradient-based learning applied to document recognition",
    "abstract": null,
    "paper_id": "cbcd9f32b526397f88d18163875d04255e72137f"
  },
  "ce32df632ab7d448646e5453f68665d6fad74293": {
    "title": "A UHF class E2 DC/DC converter using GaN HEMTs",
    "abstract": "In this paper, the design of a class E2 resonant DC/DC converter, operating at UHF band, is proposed. Combining the use of GaN HEMT devices, both for the inverter and the synchronous rectifier, with high Q lumped-element multi-harmonic matching networks, a peak efficiency value of 72% has been obtained at 780 MHz with a 10.3 W output power. By means of a Pulse Width Modulation (PWM) over the gate driving envelope, the output voltage may be controlled while keeping low switching losses, with an estimated small-signal bandwidth (BW) and a slew rate of 11 MHz and 630 V/\u00b5Seg, respectively.",
    "paper_id": "ce32df632ab7d448646e5453f68665d6fad74293"
  },
  "ce6ed169d817280d02c04ee3442568cd325c1c0b": {
    "title": "Structure and evolution of online social networks",
    "abstract": "In this paper, we consider the evolution of structure within large online social networks. We present a series of measurements of two such networks, together comprising in excess of five million people and ten million friendship links, annotated with metadata capturing the time of every event in the life of the network. Our measurements expose a surprising segmentation of these networks into three regions: singletons who do not participate in the network; isolated communities which overwhelmingly display star structure; and a giant component anchored by a well-connected core region which persists even in the absence of stars.We present a simple model of network growth which captures these aspects of component structure. The model follows our experimental results, characterizing users as either passive members of the network; inviters who encourage offline friends and acquaintances to migrate online; and linkers who fully participate in the social evolution of the network.",
    "paper_id": "ce6ed169d817280d02c04ee3442568cd325c1c0b"
  },
  "d15877fa737a22c42d85b64c46d94cadbac15c24": {
    "title": "An Introduction to Unification-Based Approaches to Grammar",
    "abstract": "Well, someone can decide by themselves what they want to do and need to do but sometimes, that kind of person will need some introduction to unification based approaches to grammar references. People with open minded will always try to seek for the new things and information from many sources. On the contrary, people with closed mind will always think that they can do it by their principals. So, what kind of person are you?",
    "paper_id": "d15877fa737a22c42d85b64c46d94cadbac15c24"
  },
  "d4356931e9b9e7fb5c9496e65c0d72c3541a6b3d": {
    "title": "Unsupervised learning of visual invariance with temporal coherence",
    "abstract": "Natural scenes in a video stream contain rich collections of visual transformations. In this paper, a generic neural network is built to lea rn visual invariance from videos in an unsupervised manner. We use temporal coher enc to learn both visual transformations and features with complex invarian ces. Without fine-tuning with labels, our invariant features are superior for classi fying objects in still images. The learned features out-perform features learned wi th sparsity in vision benchmarks Caltech-101, STL-10 and COIL-100.",
    "paper_id": "d4356931e9b9e7fb5c9496e65c0d72c3541a6b3d"
  },
  "d8abf01fce0d44665949e7a73716fff7731fa6da": {
    "title": "Places: An Image Database for Deep Scene Understanding",
    "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.",
    "paper_id": "d8abf01fce0d44665949e7a73716fff7731fa6da"
  },
  "d8d2cc5bfac5a5ec19897c0e77c2f39b829eb0d3": {
    "title": "Towards Diverse and Natural Image Descriptions via a Conditional GAN",
    "abstract": "Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect. Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the \u201cground-truth\u201d captions, while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity \u2013 two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.",
    "paper_id": "d8d2cc5bfac5a5ec19897c0e77c2f39b829eb0d3"
  },
  "d9a9dad2dfd3c5980b2783f863ed0a8d5dcd306b": {
    "title": "HT06, tagging paper, taxonomy, Flickr, academic article, to read",
    "abstract": "In recent years, tagging systems have become increasingly popular. These systems enable users to add keywords (i.e., \"tags\") to Internet resources (e.g., web pages, images, videos) without relying on a controlled vocabulary. Tagging systems have the potential to improve search, spam detection, reputation systems, and personal organization while introducing new modalities of social communication and opportunities for data mining. This potential is largely due to the social structure that underlies many of the current systems.Despite the rapid expansion of applications that support tagging of resources, tagging systems are still not well studied or understood. In this paper, we provide a short description of the academic related work to date. We offer a model of tagging systems, specifically in the context of web-based systems, to help us illustrate the possible benefits of these tools. Since many such systems already exist, we provide a taxonomy of tagging systems to help inform their analysis and design, and thus enable researchers to frame and compare evidence for the sustainability of such systems. We also provide a simple taxonomy of incentives and contribution models to inform potential evaluative frameworks. While this work does not present comprehensive empirical results, we present a preliminary study of the photo-sharing and tagging system Flickr to demonstrate our model and explore some of the issues in one sample system. This analysis helps us outline and motivate possible future directions of research in tagging systems.",
    "paper_id": "d9a9dad2dfd3c5980b2783f863ed0a8d5dcd306b"
  },
  "da437db062e751d49528914971ecdc868b557648": {
    "title": "Improved Image Captioning via Policy Gradient optimization of SPIDEr",
    "abstract": "Current image captioning methods are usually trained via maximum likelihood estimation. However, the log-likelihood score of a caption does not correlate well with human assessments of quality. Standard syntactic evaluation metrics, such as BLEU, METEOR and ROUGE, are also not well correlated. The newer SPICE and CIDEr metrics are better correlated, but have traditionally been hard to optimize for. In this paper, we show how to use a policy gradient (PG) method to directly optimize a linear combination of SPICE and CIDEr (a combination we call SPIDEr): the SPICE score ensures our captions are semantically faithful to the image, while CIDEr score ensures our captions are syntactically fluent. The PG method we propose improves on the prior MIXER approach, by using Monte Carlo rollouts instead of mixing MLE training with PG. We show empirically that our algorithm leads to easier optimization and improved results compared to MIXER. Finally, we show that using our PG method we can optimize any of the metrics, including the proposed SPIDEr metric which results in image captions that are strongly preferred by human raters compared to captions generated by the same model but trained to optimize MLE or the COCO metrics.",
    "paper_id": "da437db062e751d49528914971ecdc868b557648"
  },
  "dbbee277c6c9e73b09c37b239cb4fc61a51ab355": {
    "title": "Do psychosocial and study skill factors predict college outcomes? A meta-analysis.",
    "abstract": "This study examines the relationship between psychosocial and study skill factors (PSFs) and college outcomes by meta-analyzing 109 studies. On the basis of educational persistence and motivational theory models, the PSFs were categorized into 9 broad constructs: achievement motivation, academic goals, institutional commitment, perceived social support, social involvement, academic self-efficacy, general self-concept, academic-related skills, and contextual influences. Two college outcomes were targeted: performance (cumulative grade point average; GPA) and persistence (retention). Meta-analyses indicate moderate relationships between retention and academic goals, academic self-efficacy, and academic-related skills (ps =.340,.359, and.366, respectively). The best predictors for GPA were academic self-efficacy and achievement motivation (ps =.496 and.303, respectively). Supplementary regression analyses confirmed the incremental contributions of the PSF over and above those of socioeconomic status, standardized achievement, and high school GPA in predicting college outcomes.",
    "paper_id": "dbbee277c6c9e73b09c37b239cb4fc61a51ab355"
  },
  "dd9a9175fa952a3888da3aafba13e777588dc574": {
    "title": "Self-Critical Sequence Training for Image Captioning",
    "abstract": "Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a baseline to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.",
    "paper_id": "dd9a9175fa952a3888da3aafba13e777588dc574"
  },
  "df777dea399d182c54a62a83c284c067eb76a3a2": {
    "title": "Low-Power Far-Field Wireless Powering for Wireless Sensors",
    "abstract": "This paper discusses far-field wireless powering for low-power wireless sensors, with applications to sensing in environments where it is difficult or impossible to change batteries and where the exact position of the sensors might not be known. With expected radio-frequency (RF) power densities in the 20-200- \u03bcW/cm2 range, and desired small sensor overall size, low-power nondirective wireless powering is appropriate for sensors that transmit data at low duty cycles. The sensor platform is powered through an antenna which receives incident electromagnetic waves in the gigahertz frequency range, couples the energy to a rectifier circuit which charges a storage device (e.g., thin-film battery) through an efficient power management circuit, and the entire platform, including sensors and a low-power wireless transmitter, and is controlled through a low-power microcontroller. For low incident power density levels, codesign of the RF powering and the power management circuits is required for optimal performance. Results for hybrid and monolithic implementations of the power management circuitry are presented with integrated antenna rectifiers operating in the 1.96-GHz cellular and in 2.4-GHz industrial-scientific-medical (ISM) bands.",
    "paper_id": "df777dea399d182c54a62a83c284c067eb76a3a2"
  },
  "e03bbca03dc10c4dfb10eca7439aa1a19233aa5a": {
    "title": "Semantic object classes in video: A high-definition ground truth database",
    "abstract": "Available online xxxx",
    "paper_id": "e03bbca03dc10c4dfb10eca7439aa1a19233aa5a"
  },
  "e10e532ced7393a169f862d8012c46c9e05dfa44": {
    "title": "Harnessing the Wisdom of Crowds *",
    "abstract": "When will a large group provide an accurate answer to a question involving quantity estimation? We empirically examine this question on a crowd-based corporate earnings forecast platform (Estimize.com). By tracking user activities, we monitor the amount of public information a user views before making an earnings forecast. We find that the more public information users view, the less weight they will put on their own private information. While this improves the accuracy of individual forecasts, it reduces the accuracy of the group consensus forecast, because useful private information is prevented from entering the consensus. To address endogeneity concerns related to a user\u2019s information acquisition choice, we collaborate with Estimize.com to run experiments that restrict the information available to randomly selected stocks and users. The experiments confirm that \u201cindependent\u201d forecasts result in a more accurate consensus. Estimize.com was convinced to switch to a \u201cblind\u201d platform from November 2015 on. The findings suggest that the wisdom of crowds can be better harnessed by encouraging independent voices from among group members, and that more public information disclosure may not always improve group decision making. * We thank Renee Adams, Kenneth Ahern (discussant), Qi Chen, Stefano DellaVigna, Erik Eyster (discussant), Cary Frydman, Itay Goldstein, Umit Gurun, David Hirshleifer, Harrison Hong, Byoung-Hyoun Hwang, Russell Jame (discussant), Petri Jylha (discussant), Peter Kelly, Tse-Chun Lin (discussant), Yin Luo, Davud Rostam-Afschar (discussant), Jacob Sagi (discussant), Johan Sulaeman (discussant), Adam Szeidl, Baolian Wang (discussant), Chishen Wei (discussant), Holly Yang (discussant), Liyan Yang, and Ezra Zuckerman, as well as seminar participants at the Arizona State University, Cambridge University, Emory University, HEC-Paris, London School of Economics, Shanghai Advanced Institute of Finance, Tsinghua University, the University of Arizona, the University of Edinburgh, the University of Georgia, the University of Notre Dame, Villanova University, Washington University in St. Louis, the 2016 FSU SunTrust Beach Conference, the 2016 SFS Cavalcade, the 2016 ABFER 4th Annual Conference in Singapore, the 2016 CEIBS Finance Conference, the 2016 WFA, the 2016 CICF, the 2016 Early Career Behavioral Economics Conference, the NBER PRIT workshop, the 2016 Helsinki Finance Submit, the 2016 European Finance Association meeting, the 27th Annual Conference on Financial Economics and Accounting, the 2016 CalTech Behavioral Finance Junior Faculty Conference, the 2017 Utah Winter Finance Conference, and the 2017 Finance Down Under Conference, the Second Annual News and Financial Markets Conference at Columbia Business School for their helpful comments and suggestions. We thank Leigh Drogen and Josh Dulberger of Estimize for their generous support. \u2020University of Notre Dame, Mendoza College of Business, Notre Dame, IN, 46556, USA. Email: zda@nd.edu \u2021Washington University in St. Louis, Olin Business School, Simon Hall, St. Louis, MO 63130-4899, USA. Email: xing.huang@wustl.edu \u201cThe more influence we exert on each other, the more likely it is that we will believe the same things and make the same mistakes. That means it\u2019s possible that we could become individually smarter but collectively dumber.\u201d James Surowiecki, The Wisdom of Crowds.",
    "paper_id": "e10e532ced7393a169f862d8012c46c9e05dfa44"
  },
  "e4522ce1bd91c46ddd549ac88af132fee164b077": {
    "title": "A pseudo-machine for packet monitoring and statistics",
    "abstract": "This paper concerns the design of a flexible and efficient packet monitoring program for analyzing traffic patterns and gathering statistics on a packet network. This monitor operates in real time, using an analyzer which is an interpretive pseudo-machine driving object-oriented data collection programs. The pseudo-program for the interpreter is \u201ccompiled\u201d from configuration commands written in a monitoring control language.",
    "paper_id": "e4522ce1bd91c46ddd549ac88af132fee164b077"
  },
  "e52c061e94c610424fb8639d754c8d18276403e7": {
    "title": "gSpan: Graph-Based Substructure Pattern Mining",
    "abstract": "We invesrigare new appmaches for frequent graph-based patrem mining in graph darasers andpmpose a novel ofgorirhm called gSpan (graph-based,Tubsmrure parrern mining), which discovers frequenr subsrrucrures z h o u r candidate generorion. &an builds a new lexicographic or. der among graphs, and maps each graph to a unique minimum DFS code as irs canonical label. Based on rhis lexicographic orde,: &an adopts rhe deprh-jrsr search srraregy ro mine frequenr cannecred subgraphs eflciently. Our performance study shows rhar gSpan subsianriolly outperforms previous algorithm, somerimes by an order of magnirude.",
    "paper_id": "e52c061e94c610424fb8639d754c8d18276403e7"
  },
  "e6c10634f50c1bd4ea2949385c62d9ebe1258319": {
    "title": "Using argumentation to control lexical choice: a functional unification implementation",
    "abstract": "Using Argumentation to Control Lexical Choice: A Functional Unification Implementation",
    "paper_id": "e6c10634f50c1bd4ea2949385c62d9ebe1258319"
  },
  "e7b593a7b2b6db162002d7a15659ff7d4b3eaf02": {
    "title": "Efficiency and harmonics generation in microwave to DC conversion circuits of half-wave and full-wave rectifier types",
    "abstract": "A Rectifying Antenna (Rectenna) is one of the most important components for a wireless power transmission. It has been developed for many applications such as Space Solar Power System (SSPS), and Radio Frequency Identification (RFID) etc. The Rectenna consisting of RF-DC conversion circuits and receiving antennas needs to be designed for high conversion efficiency to achieve efficient power transmission. we try to design the mw-Class RF-DC conversion circuit using half-and full-wave rectification. The measured conversion efficiencies is 59.3 % and 65.3 % athalf- and full-wave type, respectively. And the full-wave type has lower 2nd harmonic reflection coefficient.",
    "paper_id": "e7b593a7b2b6db162002d7a15659ff7d4b3eaf02"
  },
  "ea003a8e3d90616dc43e21cc5fbff430fa49a560": {
    "title": "Advances in Automatic Text Summarization",
    "abstract": "It has been said for decades (if not centuries) that more and more information is becoming available and that tools are needed to handle it. Only recently, however, does it seem that a sufficient quantity of this information is electronically available to produce a widespread need for automatic summarization. Consequently, this research area has enjoyed a resurgence of interest in the past few years, as illustrated by a 1997 ACL Workshop, a 1998 AAAI Spring Symposium and in the same year SUMMAC: a TREC-like TIPSTER-funded summarization evaluation conference. Not unexpectedly, there is now a book to add to this list: Advances in Automatic Summarization, a collection of papers edited by Inderjeet Mani and Mark T. Maybury and published by The MIT Press. Half of it is a historical record: thirteen previously published papers, including classics such as Luhn\u2019s 1958 word-counting sentence-extraction paper, Edmundson\u2019s 1969 use of cue words and phrases, and Kupiec, Pedersen, and Chen\u2019s 1995 trained summarizer. The other half of the book holds new papers, which attempt to cover current issues and point to future trends. It starts with a paper by Karen Sp\u00e4rck Jones, which acts as an overall introduction. In it, the summarization process and the uses of summaries are broken down into their constituent parts and each of these is discussed (it reminded me of a much earlier Sp\u00e4rck Jones paper on categorization [1970]). Despite its comprehensiveness and authority, I must confess to finding this opener heavy going at times. The rest of the papers are grouped into six sections, each of which is prefaced with two or three well-written pages from the editors. These introductions contain valuable commentary on the coming papers\u2014even pointing out a possible flaw in the evaluation part of one. The opening section holds three papers on so-called classical approaches. Here one finds the oft-cited papers of Luhn, Edmundson, and Pollock and Zamora. As a package, these papers provide a novice with a good idea of how basic summarization works. My only quibble was in their reproduction. In Luhn\u2019s paper, an article from Scientific American is summarized and it would have been beneficial to have this included in the book as well. Some of the figures in another paper contained very small fonts and were hard to read; fixing this for a future print run is probably worth thinking about. The next section holds papers on corpus-based approaches to summarization, starting with Kupiec et al.\u2019s paper about a summarizer trained on an existing corpus of manually abstracted documents. Two new papers building upon the Kupiec et al. work follow this. Exploiting the discourse structure of a document is the topic of the next section. Of the five papers here, I thought Daniel Marcu\u2019s was the best, nicely describing summarization work so far and then clearly explaining his system, which is based on Rhetorical Structure Theory. The following section on knowledge-rich approaches to summarization covers such things as Wendy Lehnert\u2019s work on breaking",
    "paper_id": "ea003a8e3d90616dc43e21cc5fbff430fa49a560"
  },
  "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649": {
    "title": "Understanding the difficulty of training deep feedforward neural networks",
    "abstract": "Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. 1 Deep Neural Networks Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include Appearing in Proceedings of the 13 International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&CP 9. Copyright 2010 by the authors. learning methods for a wide array of deep architectures, including neural networks with many hidden layers (Vincent et al., 2008) and graphical models with many levels of hidden variables (Hinton et al., 2006), among others (Zhu et al., 2009; Weston et al., 2008). Much attention has recently been devoted to them (see (Bengio, 2009) for a review), because of their theoretical appeal, inspiration from biology and human cognition, and because of empirical success in vision (Ranzato et al., 2007; Larochelle et al., 2007; Vincent et al., 2008) and natural language processing (NLP) (Collobert & Weston, 2008; Mnih & Hinton, 2009). Theoretical results reviewed and discussed by Bengio (2009), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Most of the recent experimental results with deep architecture are obtained with models that can be turned into deep supervised neural networks, but with initialization or training schemes different from the classical feedforward neural networks (Rumelhart et al., 1986). Why are these new algorithms working so much better than the standard random initialization and gradient-based optimization of a supervised training criterion? Part of the answer may be found in recent analyses of the effect of unsupervised pretraining (Erhan et al., 2009), showing that it acts as a regularizer that initializes the parameters in a \u201cbetter\u201d basin of attraction of the optimization procedure, corresponding to an apparent local minimum associated with better generalization. But earlier work (Bengio et al., 2007) had shown that even a purely supervised but greedy layer-wise procedure would give better results. So here instead of focusing on what unsupervised pre-training or semi-supervised criteria bring to deep architectures, we focus on analyzing what may be going wrong with good old (but deep) multilayer neural networks. Our analysis is driven by investigative experiments to monitor activations (watching for saturation of hidden units) and gradients, across layers and across training iterations. We also evaluate the effects on these of choices of activation function (with the idea that it might affect saturation) and initialization procedure (since unsupervised pretraining is a particular form of initialization and it has a drastic impact).",
    "paper_id": "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649"
  },
  "eaba75d4c2c68ee6c249798d98e405f4ce069d8d": {
    "title": "NeuCoin: the First Secure, Cost-efficient and Decentralized Cryptocurrency",
    "abstract": "NeuCoin is a decentralized peer-to-peer cryptocurrency derived from Sunny King\u2019s Peercoin, which itself was derived from Satoshi Nakamoto\u2019s Bitcoin. As with Peercoin, proof-of-stake replaces proof-of-work as NeuCoin\u2019s security model, effectively replacing the operating costs of Bitcoin miners (electricity, computers) with the capital costs of holding the currency. Proof-of-stake also avoids proof-of-work\u2019s inherent tendency towards centralization resulting from competition for coinbase rewards among miners based on lowest cost electricity and hash power. NeuCoin increases security relative to Peercoin and other existing proof-of-stake currencies in numerous ways, including: (1) incentivizing nodes to continuously stake coins over time through substantially higher mining rewards and lower minimum stake age; (2) abandoning the use of coin age in the mining formula; (3) causing the stake modifier parameter to change over time for each stake; and (4) utilizing a client that punishes nodes that attempt to mine on multiple branches with duplicate stakes. This paper demonstrates how NeuCoin\u2019s proof-of-stake implementation addresses all commonly raised \u201cnothing at stake\u201d objections to generic proof-of-stake systems. It also reviews many of the flaws of proof-of-work designs to highlight the potential for an alternate cryptocurrency that solves these flaws.",
    "paper_id": "eaba75d4c2c68ee6c249798d98e405f4ce069d8d"
  },
  "ecbd467eacde24de43f43bc703c891df447ff389": {
    "title": "Domain adaptation of natural language processing systems",
    "abstract": "DOMAIN ADAPTATION OF NATURAL LANGUAGE PROCESSING SYSTEMS John Blitzer Fernando Pereira Statistical language processing models are being applied t o an ever wider and more varied range of linguistic domains. Collecting and curating raining sets for each different domain is prohibitively expensive, and at the same time diff erences in vocabulary and writing style across domains can cause state-of-the-art su pervised models to dramatically increase in error. The first part of this thesis describes structural correspon dence learning (SCL), a method for adapting linear discriminative models from reso urce-richsourcedomains to resource-poor target domains. The key idea is the use of pivot features which occur frequently and behave similarly in both the source and target do mains. SCL builds a shared representation by searching for a low-dimensional feature s bspace that allows us to accurately predict the presence or absence of pivot features on u nlabeled data. We demonstrate SCL on two text processing problems: sentiment classificatio n of product reviews and part of speech tagging. For both tasks, SCL significantly impr oves over state of the art supervised models using only unlabeled target data. In the second part of the thesis, we develop a formal framewor k for analyzing domain adaptation tasks. We first describe a measure of divergence, theH\u2206H-divergence, that depends on the hypothesis class H from which we estimate our supervised model. We then use this measure to state an upper bound on the true targe t error of a model trained to minimize a convex combination of empirical source and targe t errors. The bound characterizes the tradeoff inherent in training on both the large q uantity of biased source data and the small quantity of unbiased target data, and we can comput e it from finite labeled and unlabeled samples of the source and target distributions un der relatively weak assumptions. Finally, we confirm experimentally that the bound cor responds well to empirical target error for the task of sentiment classification. iv COPYRIGHT John Blitzer 2007",
    "paper_id": "ecbd467eacde24de43f43bc703c891df447ff389"
  },
  "ecd4bc32bb2717c96f76dd100fcd1255a07bd656": {
    "title": "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition",
    "abstract": "Recently, deep learning techniques have been successfully applied to automatic speech recognition tasks -first to phonetic recognition with context-independent deep belief network (DBN) hidden Markov models (HMMs) and later to large vocabulary continuous speech recognition using context-dependent (CD) DBN-HMMs. In this paper, we report our most recent experiments designed to understand the roles of the two main phases of the DBN learning -pre-training and fine tuning -in the recognition performance of a CD-DBN-HMM based large-vocabulary speech recognizer. As expected, we show that pre-training can initialize weights to a point in the space where fine-tuning can be effective and thus is crucial in training deep structured models. However, a moderate increase of the amount of unlabeled pre-training data has an insignificant effect on the final recognition results as long as the original training size is sufficiently large to initialize the DBN weights. On the other hand, with additional labeled training data, the fine-tuning phase of DBN training can significantly improve the recognition accuracy.",
    "paper_id": "ecd4bc32bb2717c96f76dd100fcd1255a07bd656"
  },
  "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2": {
    "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
    "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a a c Purchase Export Previous article Next article Check if you have access through your login credentials or your institution.",
    "paper_id": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2"
  },
  "ee71ee844053f5a1d0131f477bf73859b232f8b7": {
    "title": "PERSUASION BIAS , SOCIAL INFLUENCE , AND UNIDIMENSIONAL",
    "abstract": "We propose a boundedly-rational model of opinion formation in which individuals are subject to persuasion bias; that is, they fail to account for possible repetition in the information they receive. We show that persuasion bias implies the phenomenon of social influence, whereby one\u2019s influence on group opinions depends not only on accuracy, but also on how well-connected one is in the social network that determines communication. Persuasion bias also implies the phenomenon of unidimensional opinions; that is, individuals\u2019 opinions over a multidimensional set of issues converge to a single \u201cleft-right\u201d spectrum. We explore the implications of our model in several natural settings, including political science and marketing, and we obtain a number of novel empirical implications. DeMarzo and Zwiebel: Graduate School of Business, Stanford University, Stanford CA 94305, Vayanos: MIT Sloan School of Management, 50 Memorial Drive E52-437, Cambridge MA 02142. This paper is an extensive revision of our paper, \u201cA Model of Persuasion \u2013 With Implication for Financial Markets,\u201d (first draft, May 1997). We are grateful to Nick Barberis, Gary Becker, Jonathan Bendor, Larry Blume, Simon Board, Eddie Dekel, Stefano DellaVigna, Darrell Duffie, David Easley, Glenn Ellison, Simon Gervais, Ed Glaeser, Ken Judd, David Kreps, Edward Lazear, George Loewenstein, Lee Nelson, Anthony Neuberger, Matthew Rabin, Jos\u00e9 Scheinkman, Antoinette Schoar, Peter Sorenson, Pietro Veronesi, Richard Zeckhauser, three anonymous referees, and seminar participants at the American Finance Association Annual Meetings, Boston University, Cornell, Carnegie-Mellon, ESSEC, the European Summer Symposium in Financial Markets at Gerzensee, HEC, the Hoover Institution, Insead, MIT, the NBER Asset Pricing Conference, the Northwestern Theory Summer Workshop, NYU, the Stanford Institute for Theoretical Economics, Stanford, Texas A&M, UCLA, U.C. Berkeley, Universit\u00e9 Libre de Bruxelles, University of Michigan, University of Texas at Austin, University of Tilburg, and the Utah Winter Finance Conference for helpful comments and discussions. All errors are our own.",
    "paper_id": "ee71ee844053f5a1d0131f477bf73859b232f8b7"
  },
  "ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f": {
    "title": "The link-prediction problem for social networks",
    "abstract": null,
    "paper_id": "ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f"
  },
  "efb1a85cf540fd4f901a78100a2e450d484aebac": {
    "title": "The Quest for Scalable Blockchain Fabric: Proof-of-Work vs. BFT Replication",
    "abstract": "Bitcoin cryptocurrency demonstrated the utility of global consensus across thousands of nodes, changing the world of digital transactions forever. In the early days of Bitcoin, the performance of its probabilistic proof-of-work (PoW) based consensus fabric, also known as blockchain, was not a major issue. Bitcoin became a success story, despite its consensus latencies on the order of an hour and the theoretical peak throughput of only up to 7 transactions per second. The situation today is radically different and the poor performance scalability of early PoW blockchains no longer makes sense. Specifically, the trend of modern cryptocurrency platforms, such as Ethereum, is to support execution of arbitrary distributed applications on blockchain fabric, needing much better performance. This approach, however, makes cryptocurrency platforms step away from their original purpose and enter the domain of database-replication protocols, notably, the classical state-machine replication, and in particular its Byzantine fault-tolerant (BFT) variants. In this paper, we contrast PoW-based blockchains to those based on BFT state machine replication, focusing on their scalability limits. We also discuss recent proposals to overcoming these scalability limits and outline key outstanding open problems in the quest for the \u201cultimate\u201d blockchain fabric(s).",
    "paper_id": "efb1a85cf540fd4f901a78100a2e450d484aebac"
  },
  "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97": {
    "title": "Recurrent Neural Network Regularization",
    "abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, and machine translation.",
    "paper_id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97"
  },
  "f333fbec2a0caf953ddf969184d575c27c18cdf1": {
    "title": "Spectral Efficiency of CDMA with Random Spreading",
    "abstract": "The CDMA channel with randomly and independently chosen spreading sequences accurately models the situation where pseudonoise sequences span many symbol periods. Furthermore, its analysis provides a comparison baseline for CDMA channels with deterministic signature waveforms spanning one symbol period. We analyze the spectral efficiency (total capacity per chip) as a function of the number of users, spreading gain, and signal-to-noise ratio, and we quantify the loss in efficiency relative to an optimally chosen set of signature sequences and relative to multiaccess with no spreading. White Gaussian background noise and equal-power synchronous users are assumed. The following receivers are analyzed: a) optimal joint processing, b) single-user matched filtering, c) decorrelation, and d) MMSE linear processing.",
    "paper_id": "f333fbec2a0caf953ddf969184d575c27c18cdf1"
  },
  "f5653389328cdd9a7562cedf4a4f41b209038c6a": {
    "title": "1 An Introduction to Conditional Random Fields for Relational Learning",
    "abstract": "1.1 Introduction Relational data has two characteristics: first, statistical dependencies exist between the entities we wish to model, and second, each entity often has a rich set of features that can aid classification. For example, when classifying Web documents, the page's text provides much information about the class label, but hyperlinks define a relationship between pages that can improve classification [Taskar et al., 2002]. Graphical models are a natural formalism for exploiting the dependence structure among entities. Traditionally, graphical models have been used to represent the joint probability distribution p(y, x), where the variables y represent the attributes of the entities that we wish to predict, and the input variables x represent our observed knowledge about the entities. But modeling the joint distribution can lead to difficulties when using the rich local features that can occur in relational data, because it requires modeling the distribution p(x), which can include complex dependencies. Modeling these dependencies among inputs can lead to intractable models, but ignoring them can lead to reduced performance. A solution to this problem is to directly model the conditional distribution p(y|x), which is sufficient for classification. This is the approach taken by conditional random fields [Lafferty et al., 2001]. A conditional random field is simply a conditional distribution p(y|x) with an associated graphical structure. Because the model is",
    "paper_id": "f5653389328cdd9a7562cedf4a4f41b209038c6a"
  },
  "f5a52b69dde106cb69cb7c35dd8ca23071966876": {
    "title": "Nonparametric Scene Parsing via Label Transfer",
    "abstract": "While there has been a lot of recent work on object recognition and image understanding, the focus has been on carefully establishing mathematical models for images, scenes, and objects. In this paper, we propose a novel, nonparametric approach for object recognition and scene parsing using a new technology we name label transfer. For an input image, our system first retrieves its nearest neighbors from a large database containing fully annotated images. Then, the system establishes dense correspondences between the input image and each of the nearest neighbors using the dense SIFT flow algorithm [28], which aligns two images based on local image structures. Finally, based on the dense scene correspondences obtained from SIFT flow, our system warps the existing annotations and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on challenging databases. Compared to existing object recognition approaches that require training classifiers or appearance models for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.",
    "paper_id": "f5a52b69dde106cb69cb7c35dd8ca23071966876"
  },
  "f852c5f3fe649f8a17ded391df0796677a59927f": {
    "title": "Architecture of the Hyperledger Blockchain Fabric \u2217",
    "abstract": "Overview. A blockchain is best understood in the model of state-machine replication [8], where a service maintains some state and clients invoke operations that transform the state and generate outputs. A blockchain emulates a \u201ctrusted\u201d computing service through a distributed protocol, run by nodes connected over the Internet. The service represents or creates an asset, in which all nodes have some stake. The nodes share the common goal of running the service but do not necessarily trust each other for more. In a \u201cpermissionless\u201d blockchain such as the one underlying the Bitcoin cryptocurrency, anyone can operate a node and participate through spending CPU cycles and demonstrating a \u201cproof-of-work.\u201d On the other hand, blockchains in the \u201cpermissioned\u201d model control who participates in validation and in the protocol; these nodes typically have established identities and form a consortium. A report of Swanson compares the two models [9].",
    "paper_id": "f852c5f3fe649f8a17ded391df0796677a59927f"
  },
  "fbb11a841893d4b68fa2173226285ded4f7b04d6": {
    "title": "Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement",
    "abstract": "Structured summary 2 Provide a structured summary including, as applicable: background; objectives; data sources; study eligibility criteria, participants, and interventions; study appraisal and synthesis methods; results; limitations; conclusions and implications of key findings; systematic review registration number. INTRODUCTION Rationale 3 Describe the rationale for the review in the context of what is already known. Objectives 4 Provide an explicit statement of questions being addressed with reference to participants, interventions, comparisons, outcomes, and study design (PICOS). METHODS Protocol and registration 5 Indicate if a review protocol exists, if and where it can be accessed (e.g., Web address), and, if available, provide registration information including registration number. Eligibility criteria 6 Specify study characteristics (e.g., PICOS, length of follow-up) and report characteristics (e.g., years considered, language, publication status) used as criteria for eligibility, giving rationale. Information sources 7 Describe all information sources (e.g., databases with dates of coverage, contact with study authors to identify additional studies) in the search and date last searched. Search 8 Present full electronic search strategy for at least one database, including any limits used, such that it could be repeated. Study selection 9 State the process for selecting studies (i.e., screening, eligibility, included in systematic review, and, if applicable, included in the meta-analysis). Data collection process 10 Describe method of data extraction from reports (e.g., piloted forms, independently, in duplicate) and any processes for obtaining and confirming data from investigators. Data items 11 List and define all variables for which data were sought (e.g., PICOS, funding sources) and any assumptions and simplifications made. Risk of bias in individual studies 12 Describe methods used for assessing risk of bias of individual studies (including specification of whether this was done at the study or outcome level), and how this information is to be used in any data synthesis. Summary measures 13 State the principal summary measures (e.g., risk ratio, difference in means). Synthesis of results 14 Describe the methods of handling data and combining results of studies, if done, including measures of consistency (e.g., I) for each meta-analysis. Risk of bias across studies 15 Specify any assessment of risk of bias that may affect the cumulative evidence (e.g., publication bias, selective reporting within studies). Additional analyses 16 Describe methods of additional analyses (e.g., sensitivity or subgroup analyses, meta-regression), if done, indicating which were pre-specified. RESULTS Study selection 17 Give numbers of studies screened, assessed for eligibility, and included in the review, with reasons for exclusions at each stage, ideally with a flow diagram. Study characteristics 18 For each study, present characteristics for which data were extracted (e.g., study size, PICOS, follow-up period) and provide the citations. Risk of bias within studies 19 Present data on risk of bias of each study and, if available, any outcome-level assessment (see Item 12). Results of individual studies 20 For all outcomes considered (benefits or harms), present, for each study: (a) simple summary data for each intervention group and (b) effect estimates and confidence intervals, ideally with a forest plot. Synthesis of results 21 Present results of each meta-analysis done, including confidence intervals and measures of consistency. Risk of bias across studies 22 Present results of any assessment of risk of bias across studies (see Item 15). Additional analysis 23 Give results of additional analyses, if done (e.g., sensitivity or subgroup analyses, meta-regression [see Item 16]). DISCUSSION Summary of evidence 24 Summarize the main findings including the strength of evidence for each main outcome; consider their relevance to key groups (e.g., health care providers, users, and policy makers). Limitations 25 Discuss limitations at study and outcome level (e.g., risk of bias), and at review level (e.g., incomplete retrieval of identified research, reporting bias). Conclusions 26 Provide a general interpretation of the results in the context of other evidence, and implications for future research. FUNDING Funding 27 Describe sources of funding for the systematic review and other support (e.g., supply of data); role of funders for the systematic review. Academia and Clinic The PRISMA Statement 266 18 August 2009 Annals of Internal Medicine Volume 151 \u2022 Number 4 www.annals.org Downloaded From: http://annals.org/ by Billie White on 07/30/2018",
    "paper_id": "fbb11a841893d4b68fa2173226285ded4f7b04d6"
  },
  "fbdbe5a32c3eec2097981f768617bf65f684946a": {
    "title": "Near Field Communication (NFC) in an Automotive Environment",
    "abstract": "Near Field Communication (NFC) offers intuitive interactions between humans and vehicles. In this paper we explore different NFC based use cases in an automotive context. Nearly all described use cases have been implemented in a BMW vehicle to get experiences of NFC in a real in-car environment. We describe the underlying soft- and hardware architecture and our experiences in setting up the prototype.",
    "paper_id": "fbdbe5a32c3eec2097981f768617bf65f684946a"
  }
}